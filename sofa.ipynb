{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "16736929",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>stay_id</th>\n",
       "      <th>charttime</th>\n",
       "      <th>storetime</th>\n",
       "      <th>itemid</th>\n",
       "      <th>value</th>\n",
       "      <th>valuenum</th>\n",
       "      <th>valueuom</th>\n",
       "      <th>warning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000032</td>\n",
       "      <td>29079034</td>\n",
       "      <td>39553978</td>\n",
       "      <td>2180-07-23 12:36:00</td>\n",
       "      <td>2180-07-23 14:45:00</td>\n",
       "      <td>226512</td>\n",
       "      <td>39.4</td>\n",
       "      <td>39.4</td>\n",
       "      <td>kg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject_id   hadm_id   stay_id            charttime            storetime  \\\n",
       "0    10000032  29079034  39553978  2180-07-23 12:36:00  2180-07-23 14:45:00   \n",
       "\n",
       "   itemid  value  valuenum valueuom  warning  \n",
       "0  226512   39.4      39.4       kg        0  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./data/icu/chartevents.csv', low_memory=False, nrows=1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59eec13e",
   "metadata": {},
   "source": [
    "first_day_vitalsign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19465040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading icustays …\n",
      "Scanning chartevents …\n",
      "Building wide DataFrame …\n",
      "Wrote 73,126 rows to data/icu/first_day_vitalsign.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Compute 24-hour vital sign summary per ICU stay.\n",
    "Inputs: ./data/icu/icustays.csv, ./data/icu/chartevents.csv(.gz)\n",
    "Output: ./data/icu/first_day_vitalsign.csv, one row per stay_id with\n",
    "    columns: heartrate, sysbp, diasbp, meanbp, resprate, tempc, spo2, glucose (min/max/mean)\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Paths\n",
    "BASE = Path(\"./data/icu\")\n",
    "CHART = BASE / \"chartevents.csv\"\n",
    "ICU   = BASE / \"icustays.csv\"\n",
    "\n",
    "# Vital sign mapping\n",
    "VITAL_MAP = {\n",
    "    211: 1, 220045: 1,                    # Heart rate\n",
    "    51: 2, 442: 2, 455: 2, 6701: 2, 220179: 2, 220050: 2, 225309: 2,  # Systolic BP\n",
    "    8368: 3, 8440: 3, 8441: 3, 8555: 3, 220180: 3, 220051: 3, 225310: 3, # Diastolic BP\n",
    "    456: 4, 52: 4, 6702: 4, 443: 4, 220052: 4, 220181: 4, 225312: 4,     # Mean BP\n",
    "    615: 5, 618: 5, 220210: 5, 224690: 5,       # Resp. rate\n",
    "    223761: 6, 678: 6,                         # Temp in °F\n",
    "    223762: 6, 676: 6,                         # Temp in °C\n",
    "    646: 7, 220277: 7,                         # SpO₂\n",
    "    807: 8, 811: 8, 1529: 8, 3745: 8, 3744: 8,\n",
    "    225664: 8, 220621: 8, 226537: 8,            # Glucose\n",
    "}\n",
    "TEMP_F_IDS = {223761, 678}  # IDs in Fahrenheit\n",
    "\n",
    "CAT2NAME = {\n",
    "    1: \"heartrate\", 2: \"sysbp\", 3: \"diasbp\", 4: \"meanbp\",\n",
    "    5: \"resprate\", 6: \"tempc\", 7: \"spo2\", 8: \"glucose\",\n",
    "}\n",
    "\n",
    "# Load ICU stays\n",
    "print(\"Loading icustays …\")\n",
    "icu = pd.read_csv(ICU, usecols=[\"stay_id\", \"subject_id\", \"hadm_id\", \"intime\"],\n",
    "          parse_dates=[\"intime\"])\n",
    "icu.set_index(\"stay_id\", inplace=True)\n",
    "intime_series = icu[\"intime\"]\n",
    "\n",
    "# Aggregation dictionaries\n",
    "agg_min = {}  # (stay, cat) -> min\n",
    "agg_max = {}  # (stay, cat) -> max\n",
    "agg_sum = {}  # (stay, cat) -> sum\n",
    "agg_cnt = {}  # (stay, cat) -> count\n",
    "\n",
    "def update(stay, cat, val):\n",
    "    key = (stay, cat)\n",
    "    if key not in agg_min:\n",
    "    agg_min[key] = val\n",
    "    agg_max[key] = val\n",
    "    agg_sum[key] = val\n",
    "    agg_cnt[key] = 1\n",
    "    else:\n",
    "    agg_min[key] = min(agg_min[key], val)\n",
    "    agg_max[key] = max(agg_max[key], val)\n",
    "    agg_sum[key] += val\n",
    "    agg_cnt[key] += 1\n",
    "\n",
    "# Process chartevents in chunks\n",
    "USECOLS = [\"stay_id\", \"itemid\", \"valuenum\", \"charttime\"]\n",
    "CHUNK = 1_000_000\n",
    "print(\"Scanning chartevents …\")\n",
    "for chunk in pd.read_csv(CHART, usecols=USECOLS,\n",
    "             parse_dates=[\"charttime\"],\n",
    "             chunksize=CHUNK, low_memory=False):\n",
    "    # Filter for vital signs and valid values\n",
    "    chunk = chunk[chunk[\"itemid\"].isin(VITAL_MAP)]\n",
    "    chunk = chunk[pd.notnull(chunk[\"valuenum\"])]\n",
    "\n",
    "    # Attach ICU admission time and compute hours since admit\n",
    "    chunk = chunk.join(intime_series, on=\"stay_id\", how=\"inner\")\n",
    "    delta = (chunk[\"charttime\"] - chunk[\"intime\"]).dt.total_seconds() / 3600.0\n",
    "    chunk = chunk[(delta > 0) & (delta <= 24)]\n",
    "    if chunk.empty:\n",
    "    continue\n",
    "\n",
    "    # Convert Fahrenheit to Celsius\n",
    "    f_mask = chunk[\"itemid\"].isin(TEMP_F_IDS)\n",
    "    chunk.loc[f_mask, \"valuenum\"] = (chunk.loc[f_mask, \"valuenum\"] - 32.0) / 1.8\n",
    "\n",
    "    # Map itemid to category\n",
    "    chunk[\"cat\"] = chunk[\"itemid\"].map(VITAL_MAP).astype(np.int8)\n",
    "\n",
    "    # Aggregate stats\n",
    "    for row in chunk.itertuples(index=False):\n",
    "    update(row.stay_id, row.cat, row.valuenum)\n",
    "\n",
    "# Build wide DataFrame\n",
    "print(\"Building wide DataFrame …\")\n",
    "records = []\n",
    "for (stay, cat), _ in agg_min.items():\n",
    "    prefix = CAT2NAME[cat]\n",
    "    records.append({\n",
    "    \"stay_id\": stay,\n",
    "    f\"{prefix}_min\": agg_min[(stay, cat)],\n",
    "    f\"{prefix}_max\": agg_max[(stay, cat)],\n",
    "    f\"{prefix}_mean\": agg_sum[(stay, cat)] / agg_cnt[(stay, cat)],\n",
    "    })\n",
    "wide = pd.DataFrame(records)\n",
    "\n",
    "# One row per stay\n",
    "wide = wide.groupby(\"stay_id\").first().reset_index()\n",
    "\n",
    "# Merge additional info\n",
    "wide = wide.merge(icu.reset_index()[[\"stay_id\", \"subject_id\", \"hadm_id\"]],\n",
    "          on=\"stay_id\", how=\"left\")\n",
    "\n",
    "# Reorder columns\n",
    "cols = [\"subject_id\", \"hadm_id\", \"stay_id\"]\n",
    "for name in CAT2NAME.values():\n",
    "    cols += [f\"{name}_min\", f\"{name}_max\", f\"{name}_mean\"]\n",
    "wide = wide.reindex(columns=cols)\n",
    "\n",
    "# Save output\n",
    "OUT = BASE / \"first_day_vitalsign.csv\"\n",
    "wide.to_csv(OUT, index=False)\n",
    "print(f\"Wrote {len(wide):,} rows to {OUT}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7a2e2b",
   "metadata": {},
   "source": [
    "first_day_urine_output.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cd3e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading icustays …\n",
      "Scanning outputevents …\n",
      "Building CSV …\n",
      "Wrote 69,730 rows ➜ data/icu/first_day_urine_output.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Sum urine output for the first 24h of ICU stay.\n",
    "Input  : ./data/icu/icustays.csv, ./data/icu/outputevents.csv\n",
    "Output : ./data/icu/first_day_urine_output.csv\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# File paths\n",
    "BASE = Path(\"./data/icu\")\n",
    "ICU = BASE / \"icustays.csv\"\n",
    "OE = BASE / \"outputevents.csv\"\n",
    "\n",
    "# ITEMIDs for urine output in MIMIC-IV\n",
    "URINE_IDS = {\n",
    "    # CareVue\n",
    "    40055, 43175, 40069, 40094, 40715, 40473, 40085, 40057, 40056,\n",
    "    40405, 40428, 40086, 40096, 40651,\n",
    "    # MetaVision\n",
    "    226559, 226560, 226561, 226584, 226563, 226564,\n",
    "    226565, 226567, 226557, 226558,\n",
    "    # GU irrigant (in/out)\n",
    "    227488, 227489,\n",
    "}\n",
    "IRRIGANT_IN = 227488  # special handling: negative output\n",
    "\n",
    "# Load ICU stays (intime and IDs)\n",
    "print(\"Loading icustays …\")\n",
    "icu = pd.read_csv(\n",
    "    ICU,\n",
    "    usecols=[\"stay_id\", \"subject_id\", \"hadm_id\", \"intime\"],\n",
    "    parse_dates=[\"intime\"],\n",
    ")\n",
    "icu.set_index(\"stay_id\", inplace=True)\n",
    "intime = icu[\"intime\"]\n",
    "\n",
    "# Accumulate urine output\n",
    "totals = defaultdict(float)  # stay_id -> total output\n",
    "USECOLS = [\"stay_id\", \"itemid\", \"charttime\", \"value\"]\n",
    "CHUNK = 1_000_000\n",
    "\n",
    "print(\"Scanning outputevents …\")\n",
    "for chunk in pd.read_csv(\n",
    "        OE,\n",
    "        usecols=USECOLS,\n",
    "        parse_dates=[\"charttime\"],\n",
    "        low_memory=False,\n",
    "        chunksize=CHUNK,\n",
    "):\n",
    "    # Filter for relevant ITEMIDs and non-null values\n",
    "    chunk = chunk[chunk[\"itemid\"].isin(URINE_IDS)]\n",
    "    chunk = chunk[pd.notnull(chunk[\"value\"])]\n",
    "    if chunk.empty:\n",
    "        continue\n",
    "\n",
    "    # Attach admit time and limit to first 24h\n",
    "    chunk = chunk.join(intime, on=\"stay_id\", how=\"inner\")\n",
    "    delta_hr = (chunk[\"charttime\"] - chunk[\"intime\"]).dt.total_seconds() / 3600.0\n",
    "    chunk = chunk[(delta_hr > 0) & (delta_hr <= 24)]\n",
    "    if chunk.empty:\n",
    "        continue\n",
    "\n",
    "    # Adjust GU irrigant: make value negative\n",
    "    irr_mask = chunk[\"itemid\"] == IRRIGANT_IN\n",
    "    chunk.loc[irr_mask, \"value\"] = -1.0 * chunk.loc[irr_mask, \"value\"]\n",
    "\n",
    "    # Sum values by stay_id\n",
    "    for row in chunk.itertuples(index=False):\n",
    "        totals[row.stay_id] += row.value\n",
    "\n",
    "# Create the result DataFrame\n",
    "print(\"Building CSV …\")\n",
    "res = (\n",
    "    pd.DataFrame({\"stay_id\": list(totals.keys()),\n",
    "                  \"UrineOutput\": list(totals.values())})\n",
    "      .merge(icu.reset_index()[[\"stay_id\", \"subject_id\", \"hadm_id\"]],\n",
    "             on=\"stay_id\", how=\"left\")\n",
    "      .loc[:, [\"subject_id\", \"hadm_id\", \"stay_id\", \"UrineOutput\"]]\n",
    "      .sort_values([\"subject_id\", \"hadm_id\", \"stay_id\"])\n",
    ")\n",
    "\n",
    "OUT = BASE / \"first_day_urine_output.csv\"\n",
    "res.to_csv(OUT, index=False)\n",
    "print(f\"Wrote {len(res):,} rows ➜ {OUT}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad2e1a2",
   "metadata": {},
   "source": [
    "first_day_gcs.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213bc4d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading icustays …\n",
      "Scanning chartevents …\n",
      "Building result …\n",
      "Wrote 72,538 rows ➜ data/icu/first_day_gcs.csv\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Compute the minimum GCS score in the first 24 h of ICU stays.\n",
    "Input: icustays.csv, chartevents.csv (from ./data/icu/)\n",
    "Output: first_day_gcs.csv with subject_id, hadm_id, stay_id, mingcs, gcsmotor, gcsverbal, gcseyes, endotrachflag\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# File paths and parameters\n",
    "BASE   = Path(\"./data/icu\")          \n",
    "ICU_FN = BASE / \"icustays.csv\"    \n",
    "CE_FN  = BASE / \"chartevents.csv\"\n",
    "CHUNK  = 1_000_000\n",
    "\n",
    "# Map Metavision to CareVue IDs\n",
    "MAP_IDS = {\n",
    "    223900: 723,   # Verbal\n",
    "    223901: 454,   # Motor\n",
    "    220739: 184,   # Eyes\n",
    "}\n",
    "ALL_IDS = {184, 454, 723, *MAP_IDS.keys()}\n",
    "\n",
    "# Values indicating intubation\n",
    "TEMP_INTUB_IDS = {\n",
    "    (723,    \"1.0 ET/Trach\"),\n",
    "    (223900, \"No Response-ETT\"),\n",
    "}\n",
    "\n",
    "# Load ICU stays data\n",
    "print(\"Loading icustays …\")\n",
    "icu = pd.read_csv(\n",
    "    ICU_FN,\n",
    "    usecols=[\"stay_id\", \"subject_id\", \"hadm_id\", \"intime\"],\n",
    "    parse_dates=[\"intime\"],\n",
    ")\n",
    "icu.set_index(\"stay_id\", inplace=True)\n",
    "intime = icu[\"intime\"]\n",
    "\n",
    "# Dictionaries to store minimal GCS data per stay\n",
    "min_gcs = {}\n",
    "min_components = {}\n",
    "\n",
    "# Store recent component values for each stay (≤6 h old)\n",
    "prev_val = {}\n",
    "\n",
    "def update_prev(stay, comp, val, t):\n",
    "    if stay not in prev_val:\n",
    "        prev_val[stay] = {}\n",
    "    prev_val[stay][comp] = (val, t)\n",
    "\n",
    "def get_prev(stay, comp, now):\n",
    "    if stay not in prev_val or comp not in prev_val[stay]:\n",
    "        return None\n",
    "    val, t_prev = prev_val[stay][comp]\n",
    "    if (now - t_prev).total_seconds() / 3600.0 <= 6:\n",
    "        return val\n",
    "    return None\n",
    "\n",
    "# Process chartevents in chunks\n",
    "USECOLS = [\"stay_id\", \"itemid\", \"charttime\", \"valuenum\", \"value\"]\n",
    "print(\"Scanning chartevents …\")\n",
    "for chunk in pd.read_csv(\n",
    "        CE_FN,\n",
    "        usecols=USECOLS,\n",
    "        parse_dates=[\"charttime\"],\n",
    "        chunksize=CHUNK,\n",
    "        low_memory=False):\n",
    "\n",
    "    chunk = chunk[chunk[\"itemid\"].isin(ALL_IDS)]\n",
    "    chunk = chunk.merge(intime, on=\"stay_id\", how=\"inner\")\n",
    "    # Limit to first 24 hours\n",
    "    delta = (chunk[\"charttime\"] - chunk[\"intime\"]).dt.total_seconds() / 3600.0\n",
    "    chunk = chunk[(delta > 0) & (delta <= 24)]\n",
    "    if chunk.empty:\n",
    "        continue\n",
    "\n",
    "    # Convert Metavision IDs to CareVue IDs\n",
    "    chunk[\"itemid\"] = chunk[\"itemid\"].replace(MAP_IDS)\n",
    "\n",
    "    # Determine numeric value, with special intubation handling\n",
    "    def _num(row):\n",
    "        if (row.itemid, str(row.value).strip()) in TEMP_INTUB_IDS:\n",
    "            return 0.0\n",
    "        return row.valuenum\n",
    "    chunk[\"comp_val\"] = chunk.apply(_num, axis=1)\n",
    "\n",
    "    # Remove rows without a numeric value\n",
    "    chunk = chunk[pd.notnull(chunk[\"comp_val\"])]\n",
    "\n",
    "    # Process events in chronological order by stay\n",
    "    chunk.sort_values([\"stay_id\", \"charttime\"], inplace=True)\n",
    "\n",
    "    for row in chunk.itertuples(index=False):\n",
    "        stay   = row.stay_id\n",
    "        comp   = row.itemid          # 454: motor, 723: verbal, 184: eyes\n",
    "        val    = float(row.comp_val)\n",
    "        tstamp = row.charttime\n",
    "\n",
    "        # Update recent value for this component\n",
    "        update_prev(stay, comp, val, tstamp)\n",
    "\n",
    "        # Get current values for components using new or recent values\n",
    "        motor  = val if comp == 454 else get_prev(stay, 454, tstamp)\n",
    "        verbal = val if comp == 723 else get_prev(stay, 723, tstamp)\n",
    "        eyes   = val if comp == 184 else get_prev(stay, 184, tstamp)\n",
    "\n",
    "        if motor is None and verbal is None and eyes is None:\n",
    "            continue\n",
    "\n",
    "        endotrachflag = 1 if verbal == 0 else 0\n",
    "\n",
    "        pmotor  = get_prev(stay, 454, tstamp)\n",
    "        pverbal = get_prev(stay, 723, tstamp)\n",
    "        peyes   = get_prev(stay, 184, tstamp)\n",
    "\n",
    "        # Compute GCS score using defaults if needed\n",
    "        if verbal == 0 or (verbal is None and pverbal == 0):\n",
    "            gcs = 15\n",
    "        elif pverbal == 0:\n",
    "            gcs = (\n",
    "                (motor  if motor  is not None else 6) +\n",
    "                (verbal if verbal is not None else 5) +\n",
    "                (eyes   if eyes   is not None else 4)\n",
    "            )\n",
    "        else:\n",
    "            gcs = (\n",
    "                (motor  if motor  is not None else (pmotor  if pmotor  is not None else 6)) +\n",
    "                (verbal if verbal is not None else (pverbal if pverbal is not None else 5)) +\n",
    "                (eyes   if eyes   is not None else (peyes   if peyes   is not None else 4))\n",
    "            )\n",
    "\n",
    "        # Record minimal GCS score per stay\n",
    "        if (stay not in min_gcs) or (gcs < min_gcs[stay]):\n",
    "            min_gcs[stay] = gcs\n",
    "            min_components[stay] = (\n",
    "                motor  if motor  is not None else pmotor  if pmotor  is not None else 6,\n",
    "                verbal if verbal is not None else pverbal if pverbal is not None else 5,\n",
    "                eyes   if eyes   is not None else peyes   if peyes   is not None else 4,\n",
    "                endotrachflag,\n",
    "            )\n",
    "\n",
    "# Build the result table\n",
    "print(\"Building result …\")\n",
    "records = []\n",
    "for stay, gcs in min_gcs.items():\n",
    "    motor, verbal, eyes, etflag = min_components[stay]\n",
    "    subj  = icu.at[stay, \"subject_id\"]\n",
    "    hadm  = icu.at[stay, \"hadm_id\"]\n",
    "    records.append((subj, hadm, stay, gcs, motor, verbal, eyes, etflag))\n",
    "\n",
    "res = pd.DataFrame(\n",
    "    records,\n",
    "    columns=[\"subject_id\", \"hadm_id\", \"stay_id\", \"mingcs\", \"gcsmotor\", \"gcsverbal\", \"gcseyes\", \"endotrachflag\"],\n",
    ").sort_values([\"subject_id\", \"hadm_id\", \"stay_id\"])\n",
    "\n",
    "# Save the results\n",
    "OUT = BASE / \"first_day_gcs.csv\"\n",
    "res.to_csv(OUT, index=False)\n",
    "print(f\"Wrote {len(res):,} rows ➜ {OUT}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6616e6",
   "metadata": {},
   "source": [
    "first_day_lab.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbf9e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading icustays …\n",
      "Scanning labevents …\n",
      "Building wide DataFrame …\n",
      "Wrote 71,945 rows ➜ data/icu/first_day_lab.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Build first_day_lab.csv \n",
    "\n",
    "Input  (./data/)\n",
    "    • icu/icustays.csv      \n",
    "    • hosp/labevents.csv    \n",
    "\n",
    "Output\n",
    "    • icu/first_day_lab.csv \n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Paths & parameters\n",
    "ROOT  = Path(\"./data\")\n",
    "ICU   = ROOT / \"icu\"  / \"icustays.csv\"\n",
    "LAB   = ROOT / \"hosp\" / \"labevents.csv\"\n",
    "CHUNK = 1_000_000\n",
    "\n",
    "# Mapping from itemid to lab name\n",
    "ID2LAB = {\n",
    "    50868: \"aniongap\", 50862: \"albumin\", 51144: \"bands\",\n",
    "    50882: \"bicarbonate\", 50885: \"bilirubin\", 50912: \"creatinine\",\n",
    "    50902: \"chloride\", 50806: \"chloride\", 50809: \"glucose\",\n",
    "    50931: \"glucose\", 50810: \"hematocrit\", 51221: \"hematocrit\",\n",
    "    50811: \"hemoglobin\", 51222: \"hemoglobin\", 50813: \"lactate\",\n",
    "    51265: \"platelet\", 50822: \"potassium\", 50971: \"potassium\",\n",
    "    51275: \"ptt\", 51237: \"inr\", 51274: \"pt\", 50824: \"sodium\",\n",
    "    50983: \"sodium\", 51006: \"bun\", 51301: \"wbc\", 51300: \"wbc\",\n",
    "}\n",
    "\n",
    "# Upper limits for quality control\n",
    "UPPER = {\n",
    "    50862: 10, 50868: 1e4, 51144: 100,\n",
    "    50882: 1e4, 50885: 150, 50912: 150,\n",
    "    50806: 1e4, 50902: 1e4, 50809: 1e4,\n",
    "    50931: 1e4, 50810: 100, 51221: 100,\n",
    "    50811: 50,  51222: 50,  50813: 50,\n",
    "    51265: 1e4, 50822: 30,  50971: 30,\n",
    "    51275: 150, 51237: 50,  51274: 150,\n",
    "    50824: 200, 50983: 200, 51006: 300,\n",
    "    51300: 1000, 51301: 1000,\n",
    "}\n",
    "\n",
    "KEEP_IDS = set(ID2LAB.keys())\n",
    "\n",
    "LABS = [\n",
    "    \"aniongap\",\"albumin\",\"bands\",\"bicarbonate\",\"bilirubin\",\"creatinine\",\n",
    "    \"chloride\",\"glucose\",\"hematocrit\",\"hemoglobin\",\"lactate\",\"platelet\",\n",
    "    \"potassium\",\"ptt\",\"inr\",\"pt\",\"sodium\",\"bun\",\"wbc\"\n",
    "]\n",
    "\n",
    "# Load icustays data\n",
    "print(\"Loading icustays …\")\n",
    "icu = pd.read_csv(\n",
    "    ICU,\n",
    "    usecols=[\"stay_id\", \"subject_id\", \"hadm_id\", \"intime\"],\n",
    "    parse_dates=[\"intime\"],\n",
    ")\n",
    "icu.set_index(\"stay_id\", inplace=True)\n",
    "intime = icu[\"intime\"]\n",
    "\n",
    "# Dictionaries to collect min/max lab values per stay\n",
    "lab_min = {}\n",
    "lab_max = {}\n",
    "\n",
    "def update(stay, lab, val):\n",
    "    key = (stay, lab)\n",
    "    if key not in lab_min:\n",
    "        lab_min[key] = lab_max[key] = val\n",
    "    else:\n",
    "        lab_min[key] = min(lab_min[key], val)\n",
    "        lab_max[key] = max(lab_max[key], val)\n",
    "\n",
    "# Process labevents in chunks\n",
    "print(\"Scanning labevents …\")\n",
    "USE = [\"subject_id\",\"hadm_id\",\"itemid\",\"charttime\",\"valuenum\"]\n",
    "for chunk in pd.read_csv(\n",
    "        LAB, usecols=USE, parse_dates=[\"charttime\"],\n",
    "        chunksize=CHUNK, low_memory=False):\n",
    "    \n",
    "    chunk = chunk[chunk[\"itemid\"].isin(KEEP_IDS)]\n",
    "    chunk = chunk[pd.notnull(chunk[\"valuenum\"]) & (chunk[\"valuenum\"] > 0)]\n",
    "    if chunk.empty:\n",
    "        continue\n",
    "\n",
    "    # Merge with icustays to get intime\n",
    "    chunk = chunk.merge(\n",
    "        icu.reset_index()[[\"stay_id\",\"subject_id\",\"hadm_id\",\"intime\"]],\n",
    "        on=[\"subject_id\",\"hadm_id\"], how=\"inner\",\n",
    "    )\n",
    "    if chunk.empty:\n",
    "        continue\n",
    "\n",
    "    # Time filter: within -6 to +24 hours of intime\n",
    "    dt = (chunk[\"charttime\"] - chunk[\"intime\"]).dt.total_seconds() / 3600.0\n",
    "    chunk = chunk[(dt >= -6) & (dt <= 24)]\n",
    "    if chunk.empty:\n",
    "        continue\n",
    "\n",
    "    # Quality filters\n",
    "    mask_good = np.ones(len(chunk), dtype=bool)\n",
    "    for item, lim in UPPER.items():\n",
    "        idx = (chunk[\"itemid\"] == item) & (chunk[\"valuenum\"] > lim)\n",
    "        mask_good &= ~idx\n",
    "    mask_good &= ~((chunk[\"itemid\"] == 51144) & (chunk[\"valuenum\"] < 0))\n",
    "    chunk = chunk[mask_good]\n",
    "    if chunk.empty:\n",
    "        continue\n",
    "\n",
    "    # Map itemid to lab label\n",
    "    chunk[\"lab\"] = chunk[\"itemid\"].map(ID2LAB)\n",
    "\n",
    "    # Aggregate min and max values\n",
    "    for row in chunk.itertuples(index=False):\n",
    "        update(row.stay_id, row.lab, row.valuenum)\n",
    "\n",
    "# Construct wide DataFrame\n",
    "print(\"Building wide DataFrame …\")\n",
    "records = []\n",
    "for (stay, lab) in lab_min:\n",
    "    records.append({\n",
    "        \"stay_id\": stay,\n",
    "        f\"{lab}_min\": lab_min[(stay, lab)],\n",
    "        f\"{lab}_max\": lab_max[(stay, lab)],\n",
    "    })\n",
    "wide = pd.DataFrame(records).groupby(\"stay_id\").first().reset_index()\n",
    "\n",
    "# Add identifiers\n",
    "wide = wide.merge(\n",
    "    icu.reset_index()[[\"stay_id\",\"subject_id\",\"hadm_id\"]],\n",
    "    on=\"stay_id\", how=\"left\",\n",
    ")\n",
    "\n",
    "# Set column order\n",
    "cols = [\"subject_id\",\"hadm_id\",\"stay_id\"]\n",
    "for lb in LABS:\n",
    "    cols += [f\"{lb}_min\", f\"{lb}_max\"]\n",
    "wide = wide.reindex(columns=cols)\n",
    "\n",
    "# Save output\n",
    "OUT = ROOT / \"icu\" / \"first_day_lab.csv\"\n",
    "wide.to_csv(OUT, index=False)\n",
    "print(f\"Wrote {len(wide):,} rows ➜ {OUT}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c078b247",
   "metadata": {},
   "source": [
    "first_day_blood_gas.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f23d5ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading icustays …\n",
      "Scanning labevents …\n",
      "Pivoting …\n",
      "Writing CSV …\n",
      "Wrote 222,866 rows ➜ data/icu/first_day_blood_gas.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Pivot blood-gas/chemistry results from the first 24h of each ICU stay.\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "\n",
    "ROOT   = Path(\"./data\")\n",
    "ICU_F  = ROOT / \"icu\"  / \"icustays.csv\"\n",
    "LAB_F  = ROOT / \"hosp\" / \"labevents.csv\"\n",
    "OUT_F  = ROOT / \"derived\" / \"first_day_blood_gas.csv\"\n",
    "CHUNK  = 1_000_000\n",
    "\n",
    "# Map item IDs to labels\n",
    "ID2LBL = {\n",
    "    50800: 'specimen', 50801: 'aado2', 50802: 'baseexcess', 50803: 'bicarbonate',\n",
    "    50804: 'totalco2', 50805: 'carboxyhemoglobin', 50806: 'chloride',\n",
    "    50808: 'calcium',   50809: 'glucose', 50810: 'hematocrit', 50811: 'hemoglobin',\n",
    "    50812: 'intubated', 50813: 'lactate', 50814: 'methemoglobin', 50815: 'o2flow',\n",
    "    50816: 'fio2',      50817: 'so2',      50818: 'pco2', 50819: 'peep',\n",
    "    50820: 'ph',        50821: 'po2',      50822: 'potassium', 50823: 'requiredo2',\n",
    "    50824: 'sodium',    50825: 'temperature', 50826: 'tidalvolume',\n",
    "    50827: 'ventilationrate', 50828: 'ventilator',\n",
    "}\n",
    "\n",
    "KEEP_IDS = set(ID2LBL.keys()) | {51545}  # additional ID in old view\n",
    "LABELS = list(dict.fromkeys(ID2LBL.values()))  # unique labels in order\n",
    "\n",
    "# Upper-limit filters\n",
    "UPPER = {50810: 100, 50816: 100, 50817: 100, 50815: 70, 50821: 800}\n",
    "\n",
    "# Load ICU stays\n",
    "print(\"Loading icustays …\")\n",
    "icu = pd.read_csv(ICU_F, usecols=[\"stay_id\", \"subject_id\", \"hadm_id\", \"intime\"],\n",
    "                  parse_dates=[\"intime\"])\n",
    "icu.set_index(\"stay_id\", inplace=True)\n",
    "\n",
    "# Stream and process labevents\n",
    "print(\"Scanning labevents …\")\n",
    "rows = []\n",
    "usecols = [\"subject_id\", \"hadm_id\", \"itemid\", \"charttime\", \"value\", \"valuenum\"]\n",
    "\n",
    "for chunk in pd.read_csv(LAB_F, usecols=usecols, parse_dates=[\"charttime\"],\n",
    "                         chunksize=CHUNK, low_memory=False):\n",
    "\n",
    "    chunk = chunk[chunk[\"itemid\"].isin(KEEP_IDS)]\n",
    "    chunk = chunk[pd.notnull(chunk[\"valuenum\"])]\n",
    "    if chunk.empty:\n",
    "        continue\n",
    "\n",
    "    chunk = chunk.merge(icu.reset_index(), on=[\"subject_id\", \"hadm_id\"], how=\"inner\")\n",
    "    if chunk.empty:\n",
    "        continue\n",
    "\n",
    "    # Filter by charttime relative to intime (-6h to +24h)\n",
    "    dt = (chunk[\"charttime\"] - chunk[\"intime\"]).dt.total_seconds() / 3600\n",
    "    chunk = chunk[(dt >= -6) & (dt <= 24)]\n",
    "    if chunk.empty:\n",
    "        continue\n",
    "\n",
    "    # Apply upper-limit filters\n",
    "    ok = np.ones(len(chunk), bool)\n",
    "    for iid, limit in UPPER.items():\n",
    "        ok &= ~((chunk[\"itemid\"] == iid) & (chunk[\"valuenum\"] > limit))\n",
    "    chunk = chunk[ok]\n",
    "    if chunk.empty:\n",
    "        continue\n",
    "\n",
    "    # Adjust FiO2 values (21-100)\n",
    "    mask_fio2 = chunk[\"itemid\"] == 50816\n",
    "    chunk.loc[mask_fio2 & (chunk[\"valuenum\"] < 20), \"valuenum\"] = np.nan\n",
    "    chunk.loc[mask_fio2 & (chunk[\"valuenum\"] > 100), \"valuenum\"] = np.nan\n",
    "\n",
    "    # Cap oxygen saturation at 100\n",
    "    mask_so2 = chunk[\"itemid\"] == 50817\n",
    "    chunk.loc[mask_so2 & (chunk[\"valuenum\"] > 100), \"valuenum\"] = np.nan\n",
    "\n",
    "    # Remove negative values (except baseexcess)\n",
    "    neg_mask = (chunk[\"valuenum\"] <= 0) & (chunk[\"itemid\"] != 50802)\n",
    "    chunk.loc[neg_mask, \"valuenum\"] = np.nan\n",
    "\n",
    "    # Map itemid to label\n",
    "    chunk[\"label\"] = chunk[\"itemid\"].map(ID2LBL)\n",
    "\n",
    "    # For specimen, use the text value instead of the numeric readout\n",
    "    chunk.loc[chunk[\"label\"] == \"specimen\", \"valuenum\"] = np.nan\n",
    "\n",
    "    rows.append(chunk[[\"stay_id\", \"charttime\", \"label\", \"value\", \"valuenum\"]])\n",
    "\n",
    "if not rows:\n",
    "    raise RuntimeError(\"No blood-gas rows found!\")\n",
    "\n",
    "df = pd.concat(rows, ignore_index=True)\n",
    "\n",
    "# Pivot data to wide format\n",
    "print(\"Pivoting …\")\n",
    "wide = (\n",
    "    df.pivot_table(index=[\"stay_id\", \"charttime\"],\n",
    "                   columns=\"label\",\n",
    "                   values=\"valuenum\",\n",
    "                   aggfunc=\"max\")\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "# Get specimen value\n",
    "spec = (df[df[\"label\"] == \"specimen\"]\n",
    "        .drop_duplicates(subset=[\"stay_id\", \"charttime\"])\n",
    "        .set_index([\"stay_id\", \"charttime\"])[\"value\"])\n",
    "wide[\"specimen\"] = wide.set_index([\"stay_id\", \"charttime\"]).index.map(spec)\n",
    "\n",
    "# Add subject and hospital admission details\n",
    "wide = wide.merge(icu.reset_index()[[\"stay_id\", \"subject_id\", \"hadm_id\"]],\n",
    "                  on=\"stay_id\", how=\"left\")\n",
    "\n",
    "# Organize columns: IDs, time, specimen, then labs\n",
    "cols = [\"subject_id\", \"hadm_id\", \"stay_id\", \"charttime\", \"specimen\"] + LABELS\n",
    "wide = wide.reindex(columns=cols)\n",
    "\n",
    "print(\"Writing CSV …\")\n",
    "wide.to_csv(OUT_F, index=False)\n",
    "print(f\"Wrote {len(wide):,} rows ➜ {OUT_F}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11834aed",
   "metadata": {},
   "source": [
    "first_day_bg_art.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5060defc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading chartevents …\n",
      "Loading first-day blood-gases …\n",
      "Merging SpO₂ …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cm/blwm27yd02l6xpvwrm9_l41m0000gn/T/ipykernel_36181/3513169847.py:113: UserWarning: Sortedness of columns cannot be checked when 'by' groups provided\n",
      "  bg_pl.join_asof(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5, 36)\n",
      "┌────────────┬──────────┬──────────┬────────────┬───┬────────────┬────────────┬────────────┬───────┐\n",
      "│ subject_id ┆ hadm_id  ┆ stay_id  ┆ charttime  ┆ … ┆ ventilator ┆ subject_id ┆ hadm_id_ri ┆ Spo2  │\n",
      "│ ---        ┆ ---      ┆ ---      ┆ ---        ┆   ┆ ---        ┆ _right     ┆ ght        ┆ ---   │\n",
      "│ i64        ┆ i64      ┆ i32      ┆ datetime[μ ┆   ┆ f64        ┆ ---        ┆ ---        ┆ f64   │\n",
      "│            ┆          ┆          ┆ s]         ┆   ┆            ┆ i64        ┆ i64        ┆       │\n",
      "╞════════════╪══════════╪══════════╪════════════╪═══╪════════════╪════════════╪════════════╪═══════╡\n",
      "│ 12466550   ┆ 23998182 ┆ 30000153 ┆ 2174-09-29 ┆ … ┆ null       ┆ 12466550   ┆ 23998182   ┆ 100.0 │\n",
      "│            ┆          ┆          ┆ 13:27:00   ┆   ┆            ┆            ┆            ┆       │\n",
      "│ 12466550   ┆ 23998182 ┆ 30000153 ┆ 2174-09-29 ┆ … ┆ null       ┆ 12466550   ┆ 23998182   ┆ 100.0 │\n",
      "│            ┆          ┆          ┆ 14:07:00   ┆   ┆            ┆            ┆            ┆       │\n",
      "│ 12466550   ┆ 23998182 ┆ 30000153 ┆ 2174-09-29 ┆ … ┆ null       ┆ 12466550   ┆ 23998182   ┆ 100.0 │\n",
      "│            ┆          ┆          ┆ 16:05:00   ┆   ┆            ┆            ┆            ┆       │\n",
      "│ 13180007   ┆ 27543152 ┆ 30000213 ┆ 2162-06-21 ┆ … ┆ null       ┆ 13180007   ┆ 27543152   ┆ 100.0 │\n",
      "│            ┆          ┆          ┆ 08:27:00   ┆   ┆            ┆            ┆            ┆       │\n",
      "│ 13180007   ┆ 27543152 ┆ 30000213 ┆ 2162-06-21 ┆ … ┆ null       ┆ 13180007   ┆ 27543152   ┆ 100.0 │\n",
      "│            ┆          ┆          ┆ 14:13:00   ┆   ┆            ┆            ┆            ┆       │\n",
      "└────────────┴──────────┴──────────┴────────────┴───┴────────────┴────────────┴────────────┴───────┘\n",
      "shape: (5, 37)\n",
      "┌────────────┬──────────┬──────────┬────────────┬───┬────────────┬────────────┬───────┬────────────┐\n",
      "│ subject_id ┆ hadm_id  ┆ stay_id  ┆ charttime  ┆ … ┆ subject_id ┆ hadm_id_ri ┆ Spo2  ┆ fio2_chart │\n",
      "│ ---        ┆ ---      ┆ ---      ┆ ---        ┆   ┆ _right     ┆ ght        ┆ ---   ┆ events     │\n",
      "│ i64        ┆ i64      ┆ i32      ┆ datetime[μ ┆   ┆ ---        ┆ ---        ┆ f64   ┆ ---        │\n",
      "│            ┆          ┆          ┆ s]         ┆   ┆ i64        ┆ i64        ┆       ┆ f32        │\n",
      "╞════════════╪══════════╪══════════╪════════════╪═══╪════════════╪════════════╪═══════╪════════════╡\n",
      "│ 12466550   ┆ 23998182 ┆ 30000153 ┆ 2174-09-29 ┆ … ┆ 12466550   ┆ 23998182   ┆ 100.0 ┆ 50.0       │\n",
      "│            ┆          ┆          ┆ 13:27:00   ┆   ┆            ┆            ┆       ┆            │\n",
      "│ 12466550   ┆ 23998182 ┆ 30000153 ┆ 2174-09-29 ┆ … ┆ 12466550   ┆ 23998182   ┆ 100.0 ┆ 50.0       │\n",
      "│            ┆          ┆          ┆ 14:07:00   ┆   ┆            ┆            ┆       ┆            │\n",
      "│ 12466550   ┆ 23998182 ┆ 30000153 ┆ 2174-09-29 ┆ … ┆ 12466550   ┆ 23998182   ┆ 100.0 ┆ 50.0       │\n",
      "│            ┆          ┆          ┆ 16:05:00   ┆   ┆            ┆            ┆       ┆            │\n",
      "│ 13180007   ┆ 27543152 ┆ 30000213 ┆ 2162-06-21 ┆ … ┆ 13180007   ┆ 27543152   ┆ 100.0 ┆ 50.0       │\n",
      "│            ┆          ┆          ┆ 08:27:00   ┆   ┆            ┆            ┆       ┆            │\n",
      "│ 13180007   ┆ 27543152 ┆ 30000213 ┆ 2162-06-21 ┆ … ┆ 13180007   ┆ 27543152   ┆ 100.0 ┆ 50.0       │\n",
      "│            ┆          ┆          ┆ 14:13:00   ┆   ┆            ┆            ┆       ┆            │\n",
      "└────────────┴──────────┴──────────┴────────────┴───┴────────────┴────────────┴───────┴────────────┘\n",
      "Done. 162,522 rows written to first_day_bg_art.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cm/blwm27yd02l6xpvwrm9_l41m0000gn/T/ipykernel_36181/3513169847.py:190: UserWarning: Sortedness of columns cannot be checked when 'by' groups provided\n",
      "  .join_asof(\n"
     ]
    }
   ],
   "source": [
    "import pathlib, numpy as np, pandas as pd\n",
    "import polars as pl\n",
    "import pyarrow\n",
    "from math import exp\n",
    "\n",
    "DATA_DIR = pathlib.Path(\"./data\")\n",
    "ICU_DIR  = DATA_DIR / \"icu\"\n",
    "\n",
    "# Load raw files\n",
    "print(\"Loading chartevents …\")\n",
    "ce = pd.read_csv(\n",
    "    ICU_DIR / \"chartevents.csv\",\n",
    "    usecols=[\"subject_id\", \"hadm_id\", \"stay_id\", \"charttime\", \"itemid\", \"valuenum\"],\n",
    "    dtype={\n",
    "        \"subject_id\": \"int32\",\n",
    "        \"hadm_id\":    \"Int32\",\n",
    "        \"stay_id\":    \"int32\",\n",
    "        \"itemid\":     \"int32\",\n",
    "        \"valuenum\":   \"float32\",\n",
    "    },\n",
    "    parse_dates=[\"charttime\"]\n",
    ")\n",
    "\n",
    "print(\"Loading first-day blood-gases …\")\n",
    "bg = pd.read_csv(ICU_DIR / \"first_day_blood_gas.csv\", parse_dates=[\"charttime\"])\n",
    "bg[\"stay_id\"] = bg[\"stay_id\"].astype(\"int32\")\n",
    "\n",
    "# Extract SpO₂ values\n",
    "SPO2_ITEMS = [646, 220277]\n",
    "spo2 = (\n",
    "    ce.loc[ce.itemid.isin(SPO2_ITEMS)]\n",
    "      .assign(valuenum=lambda d: d.valuenum.where((d.valuenum > 0) & (d.valuenum <= 100)))\n",
    "      .groupby([\"subject_id\", \"hadm_id\", \"stay_id\", \"charttime\"], as_index=False)\n",
    "      .agg(SpO2=(\"valuenum\", \"max\"))\n",
    ")\n",
    "\n",
    "# Extract FiO₂ values from chartevents and clean them\n",
    "FIO2_ITEMS = [3420, 3422, 190, 223835]\n",
    "\n",
    "def _clean_fio2(row):\n",
    "    v = row.valuenum\n",
    "    if row.itemid == 223835:\n",
    "        if 0 < v <= 1:   return v * 100\n",
    "        if 21 <= v <= 100: return v\n",
    "    elif row.itemid in (3420, 3422):\n",
    "        return v\n",
    "    elif row.itemid == 190:\n",
    "        if 0.20 < v < 1: return v * 100\n",
    "    return np.nan\n",
    "\n",
    "fio2_raw = ce.loc[ce.itemid.isin(FIO2_ITEMS)].copy()\n",
    "fio2_raw[\"fio2_clean\"] = fio2_raw.apply(_clean_fio2, axis=1)\n",
    "fio2 = (\n",
    "    fio2_raw\n",
    "      .groupby([\"subject_id\", \"hadm_id\", \"stay_id\", \"charttime\"], as_index=False)\n",
    "      .agg(fio2_chartevents=(\"fio2_clean\", \"max\"))\n",
    ")\n",
    "\n",
    "# Merge SpO₂ with blood-gas data (≤ 2h)\n",
    "print(\"Merging SpO₂ …\")\n",
    "bg_pl = pl.from_pandas(bg)\n",
    "spo2_pl = (\n",
    "    pl.read_csv(\n",
    "        \"./data/icu/chartevents.csv\",\n",
    "        columns=[\"subject_id\",\"hadm_id\",\"stay_id\",\"charttime\",\"itemid\",\"valuenum\"],\n",
    "        try_parse_dates=True\n",
    "    )\n",
    "    .filter(pl.col(\"itemid\").is_in([646,220277]) &\n",
    "            (pl.col(\"valuenum\") > 0) & (pl.col(\"valuenum\") <= 100))\n",
    "    .select([\n",
    "        \"subject_id\",\"hadm_id\",\"stay_id\",\n",
    "        pl.col(\"charttime\").alias(\"charttime\"),\n",
    "        pl.col(\"valuenum\").alias(\"Spo2\")\n",
    "    ])\n",
    ")\n",
    "\n",
    "bg_pl   = bg_pl.with_columns(pl.col(\"stay_id\").cast(pl.Int32))\n",
    "spo2_pl = spo2_pl.with_columns(pl.col(\"stay_id\").cast(pl.Int32))\n",
    "TIME_UNIT = \"us\"\n",
    "bg_pl   = bg_pl.with_columns(pl.col(\"charttime\").dt.cast_time_unit(TIME_UNIT))\n",
    "spo2_pl = spo2_pl.with_columns(pl.col(\"charttime\").dt.cast_time_unit(TIME_UNIT))\n",
    "result_pl = bg_pl.join_asof(spo2_pl, on=\"charttime\", by=\"stay_id\", strategy=\"backward\", tolerance=\"2h\")\n",
    "print(result_pl.head())\n",
    "\n",
    "# Attach FiO₂ (≤ 4h)\n",
    "fio2_pl = (\n",
    "    pl.read_csv(\n",
    "        \"./data/icu/chartevents.csv\",\n",
    "        columns=[\"stay_id\", \"charttime\", \"itemid\", \"valuenum\"],\n",
    "        try_parse_dates=True,\n",
    "    )\n",
    "    .filter(pl.col(\"itemid\").is_in(FIO2_ITEMS))\n",
    "    .with_columns(\n",
    "        (\n",
    "            pl.when(pl.col(\"itemid\") == 223835)\n",
    "              .then(\n",
    "                  pl.when((pl.col(\"valuenum\") > 0) & (pl.col(\"valuenum\") <= 1))\n",
    "                    .then(pl.col(\"valuenum\") * 100)\n",
    "                  .when((pl.col(\"valuenum\") >= 21) & (pl.col(\"valuenum\") <= 100))\n",
    "                    .then(pl.col(\"valuenum\"))\n",
    "                  .otherwise(None)\n",
    "              )\n",
    "            .when(pl.col(\"itemid\").is_in([3420, 3422]))\n",
    "              .then(pl.col(\"valuenum\"))\n",
    "            .when(\n",
    "                (pl.col(\"itemid\") == 190)\n",
    "                & (pl.col(\"valuenum\") > 0.20)\n",
    "                & (pl.col(\"valuenum\") < 1)\n",
    "            )\n",
    "              .then(pl.col(\"valuenum\") * 100)\n",
    "            .otherwise(None)\n",
    "            .cast(pl.Float32)\n",
    "            .alias(\"fio2_ce\")\n",
    "        )\n",
    "    )\n",
    "    .group_by([\"stay_id\", \"charttime\"])\n",
    "    .agg(pl.col(\"fio2_ce\").max())\n",
    "    .select([\"stay_id\", \"charttime\", \"fio2_ce\"])\n",
    "    .with_columns([\n",
    "        pl.col(\"stay_id\").cast(pl.Int32),\n",
    "        pl.col(\"charttime\").dt.cast_time_unit(\"us\"),\n",
    "    ])\n",
    "    .drop_nulls([\"stay_id\", \"charttime\"])\n",
    "    .sort([\"stay_id\", \"charttime\"])\n",
    ")\n",
    "\n",
    "result_pl = (\n",
    "    result_pl\n",
    "    .with_columns([\n",
    "        pl.col(\"stay_id\").cast(pl.Int32),\n",
    "        pl.col(\"charttime\").dt.cast_time_unit(\"us\"),\n",
    "    ])\n",
    "    .sort([\"stay_id\", \"charttime\"])\n",
    ")\n",
    "\n",
    "result_pl = result_pl.join_asof(\n",
    "    fio2_pl,\n",
    "    by=\"stay_id\",\n",
    "    on=\"charttime\",\n",
    "    strategy=\"backward\",\n",
    "    tolerance=\"4h\",\n",
    ").rename({\"fio2_ce\": \"fio2_chartevents\"})\n",
    "print(result_pl.head())\n",
    "\n",
    "# Compute SPECIMEN_PROB\n",
    "coef  = {\n",
    "    \"intercept\": -0.02544, \"po2\": 0.04598, \"spo2\": -0.15356, \"fio2_ce\": 0.00621,\n",
    "    \"hemoglobin\": 0.10559, \"so2\": 0.13251, \"pco2\": -0.01511, \"fio2\": 0.01480,\n",
    "    \"aado2\": -0.00200, \"bicarbonate\": -0.03220, \"totalco2\": 0.05384,\n",
    "    \"lactate\": 0.08202, \"ph\": 0.10956, \"o2flow\": 0.00848,\n",
    "}\n",
    "mean  = {\n",
    "    \"spo2\": 97.49420, \"fio2_ce\": 51.49550, \"hemoglobin\": 10.32307,\n",
    "    \"so2\": 93.66539, \"pco2\": 42.08866, \"fio2\": 63.97836, \"aado2\": 442.21186,\n",
    "    \"bicarbonate\": 22.96894, \"totalco2\": 24.72632, \"lactate\": 3.06436,\n",
    "    \"ph\": 7.36233, \"o2flow\": 7.59362,\n",
    "}\n",
    "\n",
    "def logistic_expr(expr: pl.Expr) -> pl.Expr:\n",
    "    return 1 / (1 + (-expr).exp())\n",
    "\n",
    "result_pl = (\n",
    "    result_pl\n",
    "    .with_columns([\n",
    "        pl.col(\"Spo2\").fill_null(mean[\"spo2\"]).alias(\"_spo2\"),\n",
    "        pl.col(\"fio2_chartevents\").fill_null(mean[\"fio2_ce\"]).alias(\"_fio2_ce\"),\n",
    "        pl.col(\"hemoglobin\").fill_null(mean[\"hemoglobin\"]).alias(\"_hgb\"),\n",
    "        pl.col(\"so2\").fill_null(mean[\"so2\"]).alias(\"_so2\"),\n",
    "        pl.col(\"pco2\").fill_null(mean[\"pco2\"]).alias(\"_pco2\"),\n",
    "        pl.col(\"fio2\").fill_null(mean[\"fio2\"]).alias(\"_fio2\"),\n",
    "        pl.col(\"aado2\").fill_null(mean[\"aado2\"]).alias(\"_aado2\"),\n",
    "        pl.col(\"bicarbonate\").fill_null(mean[\"bicarbonate\"]).alias(\"_bicarb\"),\n",
    "        pl.col(\"totalco2\").fill_null(mean[\"totalco2\"]).alias(\"_tco2\"),\n",
    "        pl.col(\"lactate\").fill_null(mean[\"lactate\"]).alias(\"_lact\"),\n",
    "        pl.col(\"ph\").fill_null(mean[\"ph\"]).alias(\"_ph\"),\n",
    "        pl.col(\"o2flow\").fill_null(mean[\"o2flow\"]).alias(\"_o2flow\"),\n",
    "    ])\n",
    "    .with_columns([\n",
    "        (\n",
    "            coef[\"intercept\"]\n",
    "            + coef[\"po2\"]        * pl.col(\"po2\")\n",
    "            + coef[\"spo2\"]       * pl.col(\"_spo2\")          + 0.13429\n",
    "            + coef[\"fio2_ce\"]    * pl.col(\"_fio2_ce\")       - 0.24958\n",
    "            + coef[\"hemoglobin\"] * pl.col(\"_hgb\")           + 0.05954\n",
    "            + coef[\"so2\"]        * pl.col(\"_so2\")           - 0.23172\n",
    "            + coef[\"pco2\"]       * pl.col(\"_pco2\")          - 0.01630\n",
    "            + coef[\"fio2\"]       * pl.col(\"_fio2\")          - 0.31142\n",
    "            + coef[\"aado2\"]      * pl.col(\"_aado2\")         - 0.01328\n",
    "            + coef[\"bicarbonate\"]* pl.col(\"_bicarb\")        - 0.06535\n",
    "            + coef[\"totalco2\"]   * pl.col(\"_tco2\")          - 0.01405\n",
    "            + coef[\"lactate\"]    * pl.col(\"_lact\")          + 0.06038\n",
    "            + coef[\"ph\"]         * pl.col(\"_ph\")            - 0.00617\n",
    "            + coef[\"o2flow\"]     * pl.col(\"_o2flow\")        - 0.35803\n",
    "        ).alias(\"_z\")\n",
    "    ])\n",
    "    .with_columns([\n",
    "        logistic_expr(pl.col(\"_z\")).alias(\"SPECIMEN_PROB\")\n",
    "    ])\n",
    ")\n",
    "\n",
    "# Derived metrics\n",
    "result_pl = (\n",
    "    result_pl\n",
    "    .with_columns([\n",
    "        pl.when(\n",
    "            pl.all_horizontal(\n",
    "                pl.col(\"po2\").is_not_null(),\n",
    "                pl.col(\"pco2\").is_not_null(),\n",
    "                (pl.col(\"fio2\").is_not_null() | pl.col(\"fio2_chartevents\").is_not_null())\n",
    "            )\n",
    "        ).then(\n",
    "            (pl.coalesce([pl.col(\"fio2\"), pl.col(\"fio2_chartevents\")]) / 100)\n",
    "            * (760 - 47)\n",
    "            - (pl.col(\"pco2\") / 0.8)\n",
    "            - pl.col(\"po2\")\n",
    "        ).otherwise(None).alias(\"AADO2_calc\"),\n",
    "        pl.when(\n",
    "            pl.col(\"po2\").is_not_null() &\n",
    "            (pl.col(\"fio2\").is_not_null() | pl.col(\"fio2_chartevents\").is_not_null())\n",
    "        ).then(\n",
    "            100 * pl.col(\"po2\") / pl.coalesce([pl.col(\"fio2\"), pl.col(\"fio2_chartevents\")])\n",
    "        ).otherwise(None).alias(\"PaO2FiO2\")\n",
    "    ])\n",
    ")\n",
    "\n",
    "# Select arterial samples\n",
    "result_pl = result_pl.with_columns(pl.col(\"specimen\").cast(pl.Utf8))\n",
    "result_pl = (\n",
    "    result_pl\n",
    "    .filter(\n",
    "        (pl.col(\"specimen\") == \"ART\") |\n",
    "        (pl.col(\"SPECIMEN_PROB\") > 0.75)\n",
    "    )\n",
    "    .sort([\"stay_id\", \"charttime\"])\n",
    ")\n",
    "\n",
    "# Save output\n",
    "result_pl.write_csv(\"first_day_bg_art.csv\")\n",
    "print(f\"Done. {result_pl.height:,} rows written to first_day_bg_art.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9672572e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fio2_pl schema: Schema([('stay_id', Int32), ('charttime', Datetime(time_unit='us', time_zone=None)), ('fio2_ce', Float32)])\n",
      "result_pl schema: Schema([('subject_id', Int64), ('hadm_id', Int64), ('stay_id', Int32), ('charttime', Datetime(time_unit='us', time_zone=None)), ('specimen', Float64), ('specimen.1', Float64), ('aado2', Float64), ('baseexcess', Float64), ('bicarbonate', Float64), ('totalco2', Float64), ('carboxyhemoglobin', Float64), ('chloride', Float64), ('calcium', Float64), ('glucose', Float64), ('hematocrit', Float64), ('hemoglobin', Float64), ('intubated', Float64), ('lactate', Float64), ('methemoglobin', Float64), ('o2flow', Float64), ('fio2', Float64), ('so2', Float64), ('pco2', Float64), ('peep', Float64), ('ph', Float64), ('po2', Float64), ('potassium', Float64), ('requiredo2', Float64), ('sodium', Float64), ('temperature', Float64), ('tidalvolume', Float64), ('ventilationrate', Float64), ('ventilator', Float64), ('subject_id_right', Int64), ('hadm_id_right', Int64), ('Spo2', Float64)])\n"
     ]
    }
   ],
   "source": [
    "print(\"fio2_pl schema:\", fio2_pl.schema)   # OR\n",
    "\n",
    "print(\"result_pl schema:\", result_pl.schema)  # OR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7af67bc",
   "metadata": {},
   "source": [
    "ventilator_setting.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591265e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done – 1,010,169 rows → ventilator_setting.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "CE_CSV   = Path(\"./data/icu/chartevents.csv\")   \n",
    "OUT_FILE = \"ventilator_setting.csv\"\n",
    "\n",
    "# Load selected itemids\n",
    "ITEMS = [\n",
    "    224688, 224689, 224690, 224687, 224685, 224684, 224686,\n",
    "    224696, 220339, 224700, 223835, 223849, 229314, 223848, 224691\n",
    "]\n",
    "\n",
    "usecols = [\"subject_id\", \"stay_id\", \"charttime\", \"itemid\",\n",
    "           \"value\", \"valuenum\", \"valueuom\", \"storetime\"]\n",
    "\n",
    "ce = pd.read_csv(\n",
    "    CE_CSV,\n",
    "    usecols=usecols,\n",
    "    parse_dates=[\"charttime\", \"storetime\"]\n",
    ")\n",
    "\n",
    "# Filter rows with valid values and stay_id, and select relevant items\n",
    "ce = ce[\n",
    "    ce[\"itemid\"].isin(ITEMS) &\n",
    "    ce[\"value\"].notna() &\n",
    "    ce[\"stay_id\"].notna()\n",
    "].copy()\n",
    "\n",
    "ce = ce.astype({\"stay_id\": \"Int32\", \"itemid\": \"int64\"})\n",
    "\n",
    "# Clean 'valuenum' based on item-specific rules\n",
    "def clean_valuenum(row):\n",
    "    iid, v = row.itemid, row.valuenum\n",
    "    if iid == 223835:  # FiO₂\n",
    "        if 0.20 <= v <= 1:  return v * 100\n",
    "        if 1 < v < 20:      return np.nan\n",
    "        if 20 <= v <= 100:  return v\n",
    "        return np.nan\n",
    "    if iid in (220339, 224700):  # PEEP\n",
    "        if 0 <= v <= 100: return v\n",
    "        return np.nan\n",
    "    return v\n",
    "\n",
    "ce[\"valuenum\"] = ce.apply(clean_valuenum, axis=1)\n",
    "\n",
    "# Keep the latest row per (subject_id, charttime, itemid)\n",
    "ce.sort_values(\"storetime\", ascending=False, inplace=True)\n",
    "ce = ce.drop_duplicates(\n",
    "    subset=[\"subject_id\", \"charttime\", \"itemid\"],\n",
    "    keep=\"first\"\n",
    ")\n",
    "\n",
    "# Map itemids to column names\n",
    "mapping_valnum = {\n",
    "    224688: \"respiratory_rate_set\",\n",
    "    224690: \"respiratory_rate_total\",\n",
    "    224689: \"respiratory_rate_spontaneous\",\n",
    "    224687: \"minute_volume\",\n",
    "    224684: \"tidal_volume_set\",\n",
    "    224685: \"tidal_volume_observed\",\n",
    "    224686: \"tidal_volume_spontaneous\",\n",
    "    224696: \"plateau_pressure\",\n",
    "    220339: \"peep\",\n",
    "    224700: \"peep\",\n",
    "    223835: \"fio2\",\n",
    "    224691: \"flow_rate\",\n",
    "}\n",
    "mapping_value = {\n",
    "    223849: \"ventilator_mode\",\n",
    "    229314: \"ventilator_mode_hamilton\",\n",
    "    223848: \"ventilator_type\",\n",
    "}\n",
    "\n",
    "# Split data into numeric and string frames\n",
    "num_df = ce[ce[\"itemid\"].isin(mapping_valnum)].copy()\n",
    "str_df = ce[ce[\"itemid\"].isin(mapping_value)].copy()\n",
    "num_df[\"column\"] = num_df[\"itemid\"].map(mapping_valnum)\n",
    "str_df[\"column\"] = str_df[\"itemid\"].map(mapping_value)\n",
    "\n",
    "# Pivot numeric values\n",
    "num_wide = num_df.pivot_table(\n",
    "    index=[\"subject_id\", \"charttime\"],\n",
    "    columns=\"column\",\n",
    "    values=\"valuenum\",\n",
    "    aggfunc=\"max\"\n",
    ")\n",
    "\n",
    "# Pivot string values\n",
    "str_wide = str_df.pivot_table(\n",
    "    index=[\"subject_id\", \"charttime\"],\n",
    "    columns=\"column\",\n",
    "    values=\"value\",\n",
    "    aggfunc=\"max\"\n",
    ")\n",
    "\n",
    "# Combine datasets and add max(stay_id)\n",
    "stay = ce.groupby([\"subject_id\", \"charttime\"])[\"stay_id\"].max()\n",
    "\n",
    "wide = pd.concat([stay, num_wide, str_wide], axis=1) \\\n",
    "         .reset_index() \\\n",
    "         .astype({\"stay_id\": \"Int32\"})\n",
    "\n",
    "# Reorder columns\n",
    "col_order = [\"subject_id\", \"stay_id\", \"charttime\",\n",
    "             \"respiratory_rate_set\", \"respiratory_rate_total\",\n",
    "             \"respiratory_rate_spontaneous\", \"minute_volume\",\n",
    "             \"tidal_volume_set\", \"tidal_volume_observed\",\n",
    "             \"tidal_volume_spontaneous\", \"plateau_pressure\",\n",
    "             \"peep\", \"fio2\", \"flow_rate\",\n",
    "             \"ventilator_mode\", \"ventilator_mode_hamilton\",\n",
    "             \"ventilator_type\"]\n",
    "wide = wide.reindex(columns=col_order)\n",
    "\n",
    "# Save output CSV\n",
    "wide.sort_values([\"subject_id\", \"charttime\"]).to_csv(OUT_FILE, index=False)\n",
    "print(f\"Done – {len(wide):,} rows → {OUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c537901",
   "metadata": {},
   "source": [
    "oxygen_delivery.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e40bf1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done – 1,503,989 rows → oxygen_delivery.csv\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "ICU_PATH = Path(\"./data/icu/chartevents.csv\")\n",
    "OUTFILE  = \"oxygen_delivery.csv\"\n",
    "\n",
    "# Load selected columns\n",
    "usecols = [\"subject_id\", \"stay_id\", \"charttime\", \"itemid\", \"value\", \"valuenum\", \"valueuom\", \"storetime\"]\n",
    "ce = pd.read_csv(ICU_PATH, usecols=usecols, parse_dates=[\"charttime\", \"storetime\"])\n",
    "ce = ce.astype({\"stay_id\": \"Int32\", \"itemid\": \"int64\"})\n",
    "\n",
    "# Select FLOW rows and map 227582 to 223834\n",
    "ITEMS_FLOW = [223834, 227582, 227287]\n",
    "flows = ce[ce[\"itemid\"].isin(ITEMS_FLOW) & ce[\"value\"].notna()].copy()\n",
    "flows.loc[flows[\"itemid\"] == 227582, \"itemid\"] = 223834\n",
    "flows.sort_values(\"storetime\", ascending=False, inplace=True)\n",
    "flows = flows.drop_duplicates(subset=[\"subject_id\", \"charttime\", \"itemid\"], keep=\"first\")\n",
    "\n",
    "# Get device rows and rank them\n",
    "ITEM_DEVICE = 226732\n",
    "o2 = ce[ce[\"itemid\"] == ITEM_DEVICE][[\"subject_id\", \"stay_id\", \"charttime\", \"value\"]].copy()\n",
    "o2.rename(columns={\"value\": \"o2_device\"}, inplace=True)\n",
    "o2[\"rank_key\"] = o2[\"o2_device\"].isna()\n",
    "o2.sort_values([\"subject_id\", \"charttime\", \"rank_key\", \"o2_device\"], inplace=True)\n",
    "o2[\"rn\"] = o2.groupby([\"subject_id\", \"charttime\"]).cumcount() + 1\n",
    "o2.drop(columns=\"rank_key\", inplace=True)\n",
    "\n",
    "# Merge FLOW and device data\n",
    "merged = flows.merge(o2, on=[\"subject_id\", \"charttime\"], how=\"outer\", suffixes=(\"\", \"_o2\"))\n",
    "merged[\"stay_id\"] = merged[\"stay_id\"].fillna(merged[\"stay_id_o2\"]).astype(\"Int32\")\n",
    "merged[\"rn\"] = merged[\"rn\"].fillna(0).astype(int)\n",
    "\n",
    "# Aggregate the data\n",
    "def agg_block(df):\n",
    "    out = {\n",
    "        \"stay_id\": df[\"stay_id\"].max(),\n",
    "        \"o2_flow\": df.loc[df[\"itemid\"] == 223834, \"valuenum\"].max(),\n",
    "        \"o2_flow_additional\": df.loc[df[\"itemid\"] == 227287, \"valuenum\"].max(),\n",
    "    }\n",
    "    for k in range(1, 5):\n",
    "        col = f\"o2_delivery_device_{k}\"\n",
    "        out[col] = df.loc[df[\"rn\"] == k, \"o2_device\"].max()\n",
    "    return pd.Series(out)\n",
    "\n",
    "final = merged.groupby([\"subject_id\", \"charttime\"], as_index=False).apply(agg_block)\n",
    "\n",
    "# Save the result\n",
    "final.to_csv(OUTFILE, index=False)\n",
    "print(f\"Done – {len(final):,} rows → {OUTFILE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c52629a",
   "metadata": {},
   "source": [
    "ventilation.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b51487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done – 143,420 ventilation intervals saved to ventilation.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "DERIVED = Path(\"./data/icu\")  # adjust if needed\n",
    "\n",
    "# Load CSV files\n",
    "vs = pd.read_csv(\n",
    "    DERIVED / \"ventilator_setting.csv\",\n",
    "    usecols=[\"stay_id\", \"charttime\", \"ventilator_mode\", \"ventilator_mode_hamilton\"],\n",
    "    parse_dates=[\"charttime\"]\n",
    ")\n",
    "od = pd.read_csv(\n",
    "    DERIVED / \"oxygen_delivery.csv\",\n",
    "    usecols=[\"stay_id\", \"charttime\", \"o2_delivery_device_1\"],\n",
    "    parse_dates=[\"charttime\"]\n",
    ")\n",
    "\n",
    "# Ensure matching types\n",
    "vs[\"stay_id\"] = od[\"stay_id\"] = vs[\"stay_id\"].astype(\"int32\")\n",
    "\n",
    "# Create unique timestamp entries\n",
    "tm = (\n",
    "    pd.concat([\n",
    "        vs[[\"stay_id\", \"charttime\"]],\n",
    "        od[[\"stay_id\", \"charttime\"]]\n",
    "    ], ignore_index=True)\n",
    "    .drop_duplicates()\n",
    ")\n",
    "\n",
    "# Merge data back with timestamps\n",
    "full = (\n",
    "    tm.merge(vs, how=\"left\", on=[\"stay_id\", \"charttime\"])\n",
    "      .merge(od, how=\"left\", on=[\"stay_id\", \"charttime\"])\n",
    ")\n",
    "\n",
    "# Define ventilation classification\n",
    "trach  = {\"Tracheostomy tube\", \"Trach mask \"}\n",
    "inv_o2 = {\"Endotracheal tube\"}\n",
    "inv_vm = {'(S) CMV', 'APRV', 'APRV/Biphasic+ApnPress', 'APRV/Biphasic+ApnVol',\n",
    "          'APV (cmv)', 'Ambient', 'Apnea Ventilation', 'CMV', 'CMV/ASSIST',\n",
    "          'CMV/ASSIST/AutoFlow', 'CMV/AutoFlow', 'CPAP/PPS', 'CPAP/PSV',\n",
    "          'CPAP/PSV+Apn TCPL', 'CPAP/PSV+ApnPres', 'CPAP/PSV+ApnVol', 'MMV',\n",
    "          'MMV/AutoFlow', 'MMV/PSV', 'MMV/PSV/AutoFlow', 'P-CMV', 'PCV+',\n",
    "          'PCV+/PSV', 'PCV+Assist', 'PRES/AC', 'PRVC/AC', 'PRVC/SIMV', 'PSV/SBT',\n",
    "          'SIMV', 'SIMV/AutoFlow', 'SIMV/PRES', 'SIMV/PSV', 'SIMV/PSV/AutoFlow',\n",
    "          'SIMV/VOL', 'SYNCHRON MASTER', 'SYNCHRON SLAVE', 'VOL/AC'}\n",
    "inv_ham = {'APRV', 'APV (cmv)', 'Ambient', '(S) CMV', 'P-CMV', 'SIMV', 'APV (simv)',\n",
    "           'P-SIMV', 'VS', 'ASV'}\n",
    "niv_o2  = {'Bipap mask ', 'CPAP mask '}\n",
    "niv_ham = {'DuoPaP', 'NIV', 'NIV-ST'}\n",
    "hfnc    = {'High flow nasal cannula'}\n",
    "supp_o2 = {'Non-rebreather', 'Face tent', 'Aerosol-cool', 'Venti mask ',\n",
    "           'Medium conc mask ', 'Ultrasonic neb', 'Vapomist', 'Oxymizer',\n",
    "           'High flow neb', 'Nasal cannula'}\n",
    "\n",
    "def classify(row):\n",
    "    dev = row.o2_delivery_device_1\n",
    "    vm  = row.ventilator_mode\n",
    "    vh  = row.ventilator_mode_hamilton\n",
    "    if dev in trach:\n",
    "        return \"Tracheostomy\"\n",
    "    if (dev in inv_o2) or (vm in inv_vm) or (vh in inv_ham):\n",
    "        return \"InvasiveVent\"\n",
    "    if (dev in niv_o2) or (vh in niv_ham):\n",
    "        return \"NonInvasiveVent\"\n",
    "    if dev in hfnc:\n",
    "        return \"HFNC\"\n",
    "    if dev in supp_o2:\n",
    "        return \"SupplementalOxygen\"\n",
    "    if dev == \"None\":\n",
    "        return \"None\"\n",
    "    return np.nan\n",
    "\n",
    "full[\"ventilation_status\"] = full.apply(classify, axis=1)\n",
    "full = full.dropna(subset=[\"ventilation_status\"])\n",
    "\n",
    "# Prepare window calculations per stay\n",
    "full = full.sort_values([\"stay_id\", \"charttime\"])\n",
    "grp = full.groupby(\"stay_id\")\n",
    "full[\"charttime_lag\"] = grp[\"charttime\"].shift(1)\n",
    "full[\"charttime_lead\"] = grp[\"charttime\"].shift(-1)\n",
    "full[\"ventilation_status_lag\"] = grp[\"ventilation_status\"].shift(1)\n",
    "\n",
    "# Time gap (hours) from previous row\n",
    "full[\"gap_prev_hr\"] = (full[\"charttime\"] - full[\"charttime_lag\"]).dt.total_seconds() / 3600\n",
    "\n",
    "# Check for a new ventilation event\n",
    "full[\"new_event\"] = (\n",
    "    full[\"ventilation_status_lag\"].isna() |\n",
    "    (full[\"gap_prev_hr\"] >= 14) |\n",
    "    (full[\"ventilation_status_lag\"] != full[\"ventilation_status\"])\n",
    ").astype(int)\n",
    "\n",
    "# Sequence ID per stay\n",
    "full[\"vent_seq\"] = grp[\"new_event\"].cumsum()\n",
    "\n",
    "# Determine interval end time\n",
    "gap_next_hr = (full[\"charttime_lead\"] - full[\"charttime\"]).dt.total_seconds() / 3600\n",
    "full[\"end_candidate\"] = np.where(\n",
    "    full[\"charttime_lead\"].isna() | (gap_next_hr >= 14),\n",
    "    full[\"charttime\"],\n",
    "    full[\"charttime_lead\"]\n",
    ")\n",
    "\n",
    "# Aggregate intervals and filter single time points\n",
    "out = (\n",
    "    full.groupby([\"stay_id\", \"vent_seq\"], as_index=False)\n",
    "        .agg(\n",
    "            starttime=(\"charttime\", \"min\"),\n",
    "            endtime=(\"end_candidate\", \"max\"),\n",
    "            ventilation_status=(\"ventilation_status\", \"last\"),\n",
    "        )\n",
    ")\n",
    "out = out[out[\"starttime\"] != out[\"endtime\"]]\n",
    "\n",
    "# Save results\n",
    "out.to_csv(\"ventilation.csv\", index=False)\n",
    "print(f\"Done – {len(out):,} ventilation intervals saved to ventilation.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c7705e",
   "metadata": {},
   "source": [
    "ventdurations.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5f027e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading icustays …\n",
      "Scanning ventilation table …\n",
      "Building ventdurations.csv …\n",
      "Wrote 73,141 rows ➜ data/icu/ventdurations.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Input: \n",
    "    • icu/icustays.csv – columns: stay_id, subject_id, hadm_id, intime\n",
    "    • icu/ventilation.csv – ventilation episode start and end times\n",
    "Output: \n",
    "    • icu/ventdurations.csv – columns: subject_id, hadm_id, stay_id, vent\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# Loader: finds and reads the first CSV file with the given name under ROOT.\n",
    "ROOT = Path(\"./data\")\n",
    "def load(name: str, *, columns=None, parse_dates=None) -> pd.DataFrame:\n",
    "        files = list(ROOT.rglob(f\"{name}.csv\"))\n",
    "        return pd.read_csv(files[0],\n",
    "                                             usecols=columns,\n",
    "                                             parse_dates=parse_dates,\n",
    "                                             low_memory=False)\n",
    "\n",
    "# Load ICU stays\n",
    "print(\"Loading icustays …\")\n",
    "icu = load(\"icustays\",\n",
    "                     columns=[\"stay_id\", \"subject_id\", \"hadm_id\", \"intime\"],\n",
    "                     parse_dates=[\"intime\"])\n",
    "icu.set_index(\"stay_id\", inplace=True)\n",
    "intime_series = icu[\"intime\"]\n",
    "\n",
    "# Process ventilation episodes and flag stays with first-day ventilation\n",
    "print(\"Scanning ventilation table …\")\n",
    "vent_flag = defaultdict(int)\n",
    "CHUNK = 500_000\n",
    "columns = [\"stay_id\", \"starttime\", \"endtime\"]\n",
    "\n",
    "vent_path = list(ROOT.rglob(\"ventilation.csv\"))[0]\n",
    "\n",
    "for chunk in pd.read_csv(vent_path,\n",
    "                                                 usecols=columns,\n",
    "                                                 parse_dates=[\"starttime\", \"endtime\"],\n",
    "                                                 chunksize=CHUNK,\n",
    "                                                 low_memory=False):\n",
    "\n",
    "        # Keep rows for known stays and attach ICU admission time\n",
    "        chunk = chunk[chunk[\"stay_id\"].isin(intime_series.index)]\n",
    "        if chunk.empty:\n",
    "                continue\n",
    "\n",
    "        chunk = chunk.join(intime_series, on=\"stay_id\", how=\"left\")\n",
    "\n",
    "        # Check for overlap with the first 24h: (start <= intime+24h) and (end >= intime)\n",
    "        t0  = chunk[\"intime\"]\n",
    "        t24 = t0 + pd.Timedelta(hours=24)\n",
    "        mask = (chunk[\"starttime\"] <= t24) & (chunk[\"endtime\"] >= t0)\n",
    "        for stay in chunk.loc[mask, \"stay_id\"]:\n",
    "                vent_flag[stay] = 1\n",
    "\n",
    "# Build and save ventdurations.csv\n",
    "print(\"Building ventdurations.csv …\")\n",
    "out = (icu.reset_index()[[\"subject_id\", \"hadm_id\", \"stay_id\"]]\n",
    "                    .assign(vent=lambda df: df[\"stay_id\"].map(vent_flag).fillna(0).astype(int))\n",
    "                    .sort_values([\"subject_id\", \"hadm_id\", \"stay_id\"]))\n",
    "\n",
    "out_path = ROOT / \"icu\" / \"ventdurations.csv\"\n",
    "out.to_csv(out_path, index=False)\n",
    "print(f\"Wrote {len(out):,} rows ➜ {out_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f40f7d",
   "metadata": {},
   "source": [
    "complete_blood_count.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3629d5d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done – 437,065 rows → complete_blood_count.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "DATA      = Path(\"./data\")\n",
    "LAB_CSV   = DATA / \"hosp\" / \"labevents.csv\"\n",
    "ICU_CSV   = DATA / \"icu\"  / \"icustays.csv\"\n",
    "OUT_CSV   = \"complete_blood_count.csv\"\n",
    "\n",
    "# Map itemid to analyte names\n",
    "ITEM_MAP = {\n",
    "    51221: \"hematocrit\",\n",
    "    51222: \"hemoglobin\",\n",
    "    51248: \"mch\",\n",
    "    51249: \"mchc\",\n",
    "    51250: \"mcv\",\n",
    "    51265: \"platelet\",\n",
    "    51279: \"rbc\",\n",
    "    51277: \"rdw\",\n",
    "    51301: \"wbc\",\n",
    "}\n",
    "\n",
    "USE_LAB = [\"specimen_id\", \"subject_id\", \"hadm_id\", \"charttime\", \"itemid\", \"valuenum\"]\n",
    "USE_ICU = [\"subject_id\", \"stay_id\", \"intime\", \"outtime\"]\n",
    "\n",
    "# Load lab data and filter rows\n",
    "le = (\n",
    "    pd.read_csv(LAB_CSV, usecols=USE_LAB, parse_dates=[\"charttime\"])\n",
    "      .query(\"itemid in @ITEM_MAP.keys() and valuenum.notna() and valuenum > 0\")\n",
    ")\n",
    "\n",
    "# Merge ICU stays and filter by lab time within ICU window\n",
    "icu = pd.read_csv(ICU_CSV, usecols=USE_ICU, parse_dates=[\"intime\", \"outtime\"])\n",
    "icu = icu.astype({\"stay_id\": \"int32\"})\n",
    "le = (\n",
    "    le.merge(icu, on=\"subject_id\", how=\"left\")\n",
    "      .query(\"charttime >= intime and charttime <= outtime\")\n",
    ")\n",
    "\n",
    "# Retain first occurrence if lab falls into multiple stays\n",
    "le.sort_values([\"specimen_id\", \"stay_id\"], inplace=True)\n",
    "le = le.drop_duplicates(subset=\"specimen_id\", keep=\"first\")\n",
    "\n",
    "# Pivot labs and join with identifying info\n",
    "pivot_vals = (\n",
    "    le.pivot_table(index=\"specimen_id\",\n",
    "                   columns=\"itemid\",\n",
    "                   values=\"valuenum\",\n",
    "                   aggfunc=\"max\")\n",
    "      .rename(columns=ITEM_MAP)\n",
    ")\n",
    "\n",
    "ids = (\n",
    "    le.groupby(\"specimen_id\", as_index=True)\n",
    "      .agg(subject_id=(\"subject_id\", \"max\"),\n",
    "           hadm_id   =(\"hadm_id\", \"max\"),\n",
    "           stay_id   =(\"stay_id\", \"max\"),\n",
    "           charttime =(\"charttime\", \"max\"))\n",
    ")\n",
    "\n",
    "cbc = ids.join(pivot_vals, how=\"left\").reset_index()\n",
    "\n",
    "# Add missing analyte columns\n",
    "for col in ITEM_MAP.values():\n",
    "    if col not in cbc:\n",
    "        cbc[col] = pd.NA\n",
    "\n",
    "# Arrange columns and save file\n",
    "cbc = cbc.loc[:, [\"subject_id\", \"hadm_id\", \"stay_id\", \"charttime\", \"specimen_id\"]\n",
    "                   + list(ITEM_MAP.values())] \\\n",
    "         .astype({\"subject_id\": \"int32\",\n",
    "                  \"hadm_id\": \"Int32\",\n",
    "                  \"stay_id\": \"Int32\",\n",
    "                  \"specimen_id\": \"int64\"}) \\\n",
    "         .sort_values([\"subject_id\", \"charttime\"])\n",
    "\n",
    "cbc.to_csv(OUT_CSV, index=False)\n",
    "print(f\"Done – {len(cbc):,} rows → {OUT_CSV}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97557060",
   "metadata": {},
   "source": [
    "chemistry.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a962a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done – 470,912 rows → chemistry.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "DATA     = Path(\"./data\")                     \n",
    "LAB_CSV  = DATA / \"hosp\" / \"labevents.csv\"\n",
    "ICU_CSV  = DATA / \"icu\"  / \"icustays.csv\"\n",
    "OUT_CSV  = \"chemistry.csv\"\n",
    "\n",
    "# Map itemid to (column, limit, allow_zero)\n",
    "CHEM = {\n",
    "    50862: (\"albumin\",       10, False),\n",
    "    # 50930: (\"globulin\",      10, False),\n",
    "    50976: (\"total_protein\", 20, False),\n",
    "    50868: (\"aniongap\",     10000, True),\n",
    "    50882: (\"bicarbonate\",  10000, False),\n",
    "    51006: (\"bun\",           300, False),\n",
    "    50893: (\"calcium\",      10000, False),\n",
    "    50902: (\"chloride\",     10000, False),\n",
    "    50912: (\"creatinine\",    150, False),\n",
    "    50931: (\"glucose\",     10000, False),\n",
    "    50983: (\"sodium\",         200, False),\n",
    "    50971: (\"potassium\",       30, False),\n",
    "}\n",
    "\n",
    "# Columns to load\n",
    "LAB_COLS = [\"specimen_id\", \"subject_id\", \"hadm_id\", \"charttime\", \"itemid\", \"valuenum\"]\n",
    "ICU_COLS = [\"subject_id\", \"stay_id\", \"intime\", \"outtime\"]\n",
    "\n",
    "# 1. Load and filter lab data\n",
    "lab = (\n",
    "    pd.read_csv(LAB_CSV, usecols=LAB_COLS, parse_dates=[\"charttime\"])\n",
    "      .query(\"itemid in @CHEM.keys()\")\n",
    ")\n",
    "\n",
    "def ok(row):\n",
    "    _, lim, allow0 = CHEM[row.itemid]\n",
    "    v = row.valuenum\n",
    "    return pd.notna(v) and v >= 0 and (allow0 or v > 0) and v <= lim\n",
    "\n",
    "lab = lab[lab.apply(ok, axis=1)].copy()\n",
    "\n",
    "# 2. Merge ICU data and filter labs by ICU stay time\n",
    "icu = (\n",
    "    pd.read_csv(ICU_CSV, usecols=ICU_COLS, parse_dates=[\"intime\", \"outtime\"])\n",
    "      .astype({\"stay_id\": \"int32\"})\n",
    ")\n",
    "\n",
    "lab = (\n",
    "    lab.merge(icu, on=\"subject_id\", how=\"left\")\n",
    "       .query(\"charttime >= intime and charttime <= outtime\")\n",
    ")\n",
    "\n",
    "lab.sort_values([\"specimen_id\", \"stay_id\"], inplace=True)\n",
    "lab = lab.drop_duplicates(subset=\"specimen_id\", keep=\"first\")\n",
    "\n",
    "# 3. Pivot lab data to wide format\n",
    "lab[\"col\"] = lab[\"itemid\"].map({k: v[0] for k, v in CHEM.items()})\n",
    "\n",
    "vals = lab.pivot_table(\n",
    "    index=\"specimen_id\", columns=\"col\", values=\"valuenum\", aggfunc=\"max\"\n",
    ")\n",
    "\n",
    "ids = lab.groupby(\"specimen_id\", as_index=True).agg(\n",
    "    subject_id=(\"subject_id\", \"max\"),\n",
    "    hadm_id=(\"hadm_id\", \"max\"),\n",
    "    stay_id=(\"stay_id\", \"max\"),\n",
    "    charttime=(\"charttime\", \"max\")\n",
    ")\n",
    "\n",
    "chem = ids.join(vals, how=\"left\").reset_index()\n",
    "\n",
    "cols = [\"subject_id\", \"hadm_id\", \"stay_id\", \"charttime\", \"specimen_id\"] + [v[0] for v in CHEM.values()]\n",
    "chem = (\n",
    "    chem[cols]\n",
    "        .astype({\"subject_id\": \"int32\", \"hadm_id\": \"Int32\", \"stay_id\": \"int32\", \"specimen_id\": \"int64\"})\n",
    "        .sort_values([\"subject_id\", \"charttime\"])\n",
    ")\n",
    "\n",
    "# 4. Save results\n",
    "chem.to_csv(OUT_CSV, index=False)\n",
    "print(f\"Done – {len(chem):,} rows → {OUT_CSV}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db641c23",
   "metadata": {},
   "source": [
    "blood_differential.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1cf25c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cm/blwm27yd02l6xpvwrm9_l41m0000gn/T/ipykernel_70599/1088665163.py:107: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[]' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  diff.loc[mask, abs_] = diff.loc[mask, pct] * diff.loc[mask, \"wbc\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done – 376,749 rows → blood_differential.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT     = Path(\"./data\")                     \n",
    "LAB_CSV  = ROOT / \"hosp\" / \"labevents.csv\"\n",
    "ICU_CSV  = ROOT / \"icu\"  / \"icustays.csv\"\n",
    "OUT_CSV  = \"blood_differential.csv\"\n",
    "\n",
    "# Map itemid to (name, conversion factor)\n",
    "ITEMS = {\n",
    "    51300: (\"wbc\", 1.0), 51301: (\"wbc\", 1.0), 51755: (\"wbc\", 1.0),\n",
    "    # 52069: (\"basophils_abs\", 1.0),\n",
    "    52073: (\"eosinophils_abs\", 1.0), 51199: (\"eosinophils_abs\", 1/1000),\n",
    "    51133: (\"lymphocytes_abs\", 1.0), 52769: (\"lymphocytes_abs\", 1/1000),\n",
    "    52074: (\"monocytes_abs\", 1.0),   51253: (\"monocytes_abs\", 1/1000),\n",
    "    52075: (\"neutrophils_abs\", 1.0),\n",
    "    51218: (\"granulocytes_abs\", 1/1000),\n",
    "    51146: (\"basophils\", 1.0),\n",
    "    51200: (\"eosinophils\", 1.0),\n",
    "    51244: (\"lymphocytes\", 1.0), 51245: (\"lymphocytes\", 1.0),\n",
    "    51254: (\"monocytes\", 1.0),\n",
    "    51256: (\"neutrophils\", 1.0),\n",
    "    51143: (\"atypical_lymphocytes\", 1.0),\n",
    "    51144: (\"bands\", 1.0),\n",
    "    52135: (\"immature_granulocytes\", 1.0),\n",
    "    51251: (\"metamyelocytes\", 1.0),\n",
    "    51257: (\"nrbc\", 1.0),\n",
    "}\n",
    "\n",
    "LAB_COLS = [\"specimen_id\", \"subject_id\", \"hadm_id\",\n",
    "            \"charttime\", \"itemid\", \"valuenum\"]\n",
    "ICU_COLS = [\"subject_id\", \"stay_id\", \"intime\", \"outtime\"]\n",
    "\n",
    "# 1. Load lab data and filter\n",
    "lab = (\n",
    "    pd.read_csv(LAB_CSV, usecols=LAB_COLS, parse_dates=[\"charttime\"])\n",
    "      .query(\"itemid in @ITEMS.keys() and valuenum.notna() and valuenum>=0\")\n",
    "      .copy()\n",
    ")\n",
    "\n",
    "# Convert units and set variable name\n",
    "lab[\"var\"] = lab[\"itemid\"].map({k: v[0] for k, v in ITEMS.items()})\n",
    "lab[\"val\"] = lab.apply(lambda r: r.valuenum * ITEMS[r.itemid][1], axis=1)\n",
    "\n",
    "# 2. Load ICU data and merge\n",
    "icu = (\n",
    "    pd.read_csv(ICU_CSV, usecols=ICU_COLS, parse_dates=[\"intime\", \"outtime\"])\n",
    "      .astype({\"stay_id\": \"int32\"})\n",
    ")\n",
    "\n",
    "lab = (\n",
    "    lab.merge(icu, on=\"subject_id\", how=\"left\")               \n",
    "       .query(\"charttime >= intime and charttime <= outtime\")  \n",
    ")\n",
    "\n",
    "# Keep first stay if a specimen overlaps multiple stays\n",
    "lab.sort_values([\"specimen_id\", \"stay_id\"], inplace=True)\n",
    "lab = lab.drop_duplicates(\"specimen_id\", keep=\"first\")\n",
    "\n",
    "# 3. Pivot table: maximum value per specimen\n",
    "vals = (\n",
    "    lab.pivot_table(index=\"specimen_id\",\n",
    "                    columns=\"var\",\n",
    "                    values=\"val\",\n",
    "                    aggfunc=\"max\")\n",
    ")\n",
    "\n",
    "ids = (\n",
    "    lab.groupby(\"specimen_id\", as_index=True)\n",
    "        .agg(subject_id=(\"subject_id\", \"max\"),\n",
    "             hadm_id   =(\"hadm_id\", \"max\"),\n",
    "             stay_id   =(\"stay_id\", \"max\"),\n",
    "             charttime =(\"charttime\", \"max\"))\n",
    ")\n",
    "\n",
    "diff = ids.join(vals, how=\"left\").reset_index()\n",
    "\n",
    "# 4. Impute *_abs values when possible\n",
    "pct_cols = [\"basophils\", \"eosinophils\", \"lymphocytes\",\n",
    "            \"monocytes\", \"neutrophils\"]\n",
    "abs_cols = [c + \"_abs\" for c in pct_cols]\n",
    "\n",
    "# Ensure columns exist\n",
    "for col in abs_cols + pct_cols:\n",
    "    if col not in diff.columns:\n",
    "        diff[col] = pd.NA\n",
    "\n",
    "diff[\"impute_abs\"] = (\n",
    "    (diff[\"wbc\"].fillna(0) > 0) &\n",
    "    diff[pct_cols].fillna(0).sum(axis=1).gt(0)\n",
    ")\n",
    "\n",
    "for pct, abs_ in zip(pct_cols, abs_cols):\n",
    "    mask = diff[abs_].isna() & diff[pct].notna() & diff[\"impute_abs\"]\n",
    "    diff.loc[mask, abs_] = diff.loc[mask, pct] * diff.loc[mask, \"wbc\"]\n",
    "\n",
    "diff[abs_cols] = diff[abs_cols].round(4)\n",
    "\n",
    "# 5. Order columns and save to CSV\n",
    "final_cols = ([\"subject_id\", \"hadm_id\", \"stay_id\", \"charttime\", \"specimen_id\", \"wbc\"]\n",
    "              + abs_cols\n",
    "              + pct_cols\n",
    "              + [\"atypical_lymphocytes\", \"bands\", \"immature_granulocytes\",\n",
    "                 \"metamyelocytes\", \"nrbc\"])\n",
    "\n",
    "available_cols = [col for col in final_cols if col in diff.columns]\n",
    "diff = diff[available_cols].astype({\n",
    "    \"subject_id\": \"int32\",\n",
    "    \"hadm_id\": \"Int32\",\n",
    "    \"stay_id\": \"int32\"\n",
    "})\n",
    "\n",
    "diff.to_csv(OUT_CSV, index=False)\n",
    "print(f\"Done – {len(diff):,} rows → {OUT_CSV}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f474f9ce",
   "metadata": {},
   "source": [
    "coagulation.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be58b6c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done – 1,545,930 rows → coagulation.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT     = Path(\"./data\")                      \n",
    "LAB_CSV  = ROOT / \"hosp\" / \"labevents.csv\"\n",
    "ICU_CSV  = ROOT / \"icu\"  / \"icustays.csv\"\n",
    "OUT_CSV  = \"coagulation.csv\"\n",
    "\n",
    "# Map itemid to column name\n",
    "ITEMS = {\n",
    "    51196: \"d_dimer\",\n",
    "    51214: \"fibrinogen\",\n",
    "    51297: \"thrombin\",\n",
    "    51237: \"inr\",\n",
    "    51274: \"pt\",\n",
    "    51275: \"ptt\",\n",
    "}\n",
    "\n",
    "LAB_COLS = [\"specimen_id\", \"subject_id\", \"hadm_id\", \"charttime\", \"itemid\", \"valuenum\"]\n",
    "\n",
    "# 1. Load labs and filter by ITEMS\n",
    "labs = (\n",
    "    pd.read_csv(LAB_CSV, usecols=LAB_COLS, parse_dates=[\"charttime\"])\n",
    "      .query(\"itemid in @ITEMS.keys() and valuenum.notna()\")\n",
    "      .copy()\n",
    ")\n",
    "\n",
    "# 2. Load ICU stays and create IntervalIndex for lookup\n",
    "icu = (\n",
    "    pd.read_csv(ICU_CSV,\n",
    "                usecols=[\"subject_id\", \"stay_id\", \"intime\", \"outtime\"],\n",
    "                parse_dates=[\"intime\", \"outtime\"])\n",
    "      .astype({\"stay_id\": \"int32\"})\n",
    ")\n",
    "ivl = pd.IntervalIndex.from_arrays(icu[\"intime\"], icu[\"outtime\"], closed=\"both\")\n",
    "icu_int = icu.set_index(ivl)\n",
    "\n",
    "def match_stay(row):\n",
    "    hits = icu_int[icu_int.index.contains(row.charttime)]\n",
    "    hits = hits[hits[\"subject_id\"] == row.subject_id]\n",
    "    return hits[\"stay_id\"].iloc[0] if not hits.empty else pd.NA\n",
    "\n",
    "labs[\"stay_id\"] = labs.apply(match_stay, axis=1)\n",
    "\n",
    "# 3. Pivot labs: max value per specimen_id\n",
    "labs[\"var\"] = labs[\"itemid\"].map(ITEMS)\n",
    "vals = (\n",
    "    labs.pivot_table(index=\"specimen_id\",\n",
    "                     columns=\"var\",\n",
    "                     values=\"valuenum\",\n",
    "                     aggfunc=\"max\")\n",
    ")\n",
    "ids = (\n",
    "    labs.groupby(\"specimen_id\", as_index=True)\n",
    "        .agg(subject_id=(\"subject_id\", \"max\"),\n",
    "             hadm_id   =(\"hadm_id\", \"max\"),\n",
    "             stay_id   =(\"stay_id\", \"max\"),\n",
    "             charttime =(\"charttime\", \"max\"))\n",
    ")\n",
    "coag = (\n",
    "    ids.join(vals, how=\"left\")\n",
    "       .reset_index()\n",
    "       .loc[:, [\"subject_id\", \"hadm_id\", \"stay_id\", \"charttime\", \"specimen_id\"]\n",
    "            + list(ITEMS.values())]\n",
    "       .astype({\"subject_id\": \"int32\",\n",
    "                \"hadm_id\": \"Int32\",\n",
    "                \"stay_id\": \"Int32\",\n",
    "                \"specimen_id\": \"int64\"})\n",
    "       .sort_values([\"subject_id\", \"charttime\"])\n",
    ")\n",
    "\n",
    "# 4. Save output\n",
    "coag.to_csv(OUT_CSV, index=False)\n",
    "print(f\"Done – {len(coag):,} rows → {OUT_CSV}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb94d23",
   "metadata": {},
   "source": [
    "enzyme.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352cf95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done – 1,637,082 rows → enzyme.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT     = Path(\"./data\")                       # change if needed\n",
    "LAB_CSV  = ROOT / \"hosp\" / \"labevents.csv\"\n",
    "ICU_CSV  = ROOT / \"icu\"  / \"icustays.csv\"\n",
    "OUT_CSV  = \"enzyme.csv\"\n",
    "\n",
    "# itemid → output column\n",
    "ITEMS = {\n",
    "    50861: \"alt\",    50863: \"alp\",    50878: \"ast\",\n",
    "    50867: \"amylase\",\n",
    "    50885: \"bilirubin_total\", 50883: \"bilirubin_direct\",\n",
    "    50884: \"bilirubin_indirect\",\n",
    "    50910: \"ck_cpk\", 50911: \"ck_mb\",\n",
    "    50927: \"ggt\",    50954: \"ld_ldh\",\n",
    "}\n",
    "\n",
    "LAB_COLS = [\"specimen_id\",\"subject_id\",\"hadm_id\",\n",
    "            \"charttime\",\"itemid\",\"valuenum\"]\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 1 ▸ load & filter labs\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "lab = (\n",
    "    pd.read_csv(LAB_CSV, usecols=LAB_COLS, parse_dates=[\"charttime\"])\n",
    "      .query(\"itemid in @ITEMS.keys() and valuenum.notna() and valuenum>0\")\n",
    "      .copy()\n",
    ")\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 2 ▸ attach stay_id via ICU interval match\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "icu = (\n",
    "    pd.read_csv(ICU_CSV,\n",
    "                usecols=[\"subject_id\",\"stay_id\",\"intime\",\"outtime\"],\n",
    "                parse_dates=[\"intime\",\"outtime\"])\n",
    "      .astype({\"stay_id\":\"int32\"})\n",
    ")\n",
    "\n",
    "# build IntervalIndex for fast lookup\n",
    "ivl = pd.IntervalIndex.from_arrays(icu[\"intime\"], icu[\"outtime\"], closed=\"both\")\n",
    "icu_int = icu.set_index(ivl)\n",
    "\n",
    "\n",
    "\n",
    "def find_stay(row):\n",
    "    hits = icu_int[icu_int.index.contains(row.charttime)]\n",
    "    hits = hits[hits[\"subject_id\"] == row.subject_id]\n",
    "    return hits[\"stay_id\"].iloc[0] if not hits.empty else pd.NA\n",
    "\n",
    "lab[\"stay_id\"] = lab.apply(find_stay, axis=1)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 3 ▸ pivot MAX() per specimen_id\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "lab[\"col\"] = lab[\"itemid\"].map(ITEMS)\n",
    "\n",
    "vals = (\n",
    "    lab.pivot_table(index=\"specimen_id\",\n",
    "                    columns=\"col\",\n",
    "                    values=\"valuenum\",\n",
    "                    aggfunc=\"max\")\n",
    ")\n",
    "\n",
    "ids = (\n",
    "    lab.groupby(\"specimen_id\", as_index=True)\n",
    "        .agg(subject_id=(\"subject_id\",\"max\"),\n",
    "             hadm_id   =(\"hadm_id\",\"max\"),\n",
    "             stay_id   =(\"stay_id\",\"max\"),   # may be <NA>\n",
    "             charttime =(\"charttime\",\"max\"))\n",
    ")\n",
    "\n",
    "enzyme = ids.join(vals, how=\"left\").reset_index()\n",
    "\n",
    "# guarantee every analyte column exists\n",
    "for col in ITEMS.values():\n",
    "    if col not in enzyme.columns:\n",
    "        enzyme[col] = pd.NA\n",
    "\n",
    "cols_order = [\"subject_id\",\"hadm_id\",\"stay_id\",\"charttime\",\"specimen_id\"] \\\n",
    "             + list(ITEMS.values())\n",
    "\n",
    "enzyme = (\n",
    "    enzyme[cols_order]\n",
    "        .astype({\"subject_id\":\"int32\",\n",
    "                 \"hadm_id\"  :\"Int32\",\n",
    "                 \"stay_id\"  :\"Int32\",\n",
    "                 \"specimen_id\":\"int64\"})\n",
    "        .sort_values([\"subject_id\",\"charttime\"])\n",
    ")\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 4 ▸ save\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "enzyme.to_csv(OUT_CSV, index=False)\n",
    "print(f\"Done – {len(enzyme):,} rows → {OUT_CSV}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5b4543",
   "metadata": {},
   "source": [
    "first_day_lab.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a129f9fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done – 73,141 rows → first_day_lab.csv\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# build_first_day_lab.py – MIMIC-IV day-1 lab panel\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path(\"./data\")\n",
    "ICU  = ROOT / \"icu\"\n",
    "DRV  = ROOT / \"derived\"\n",
    "\n",
    "OUT_CSV = \"first_day_lab.csv\"\n",
    "\n",
    "# Load ICU stays with subject_id, stay_id, intime\n",
    "icu = (\n",
    "    pd.read_csv(ICU / \"icustays.csv\",\n",
    "                usecols=[\"subject_id\", \"stay_id\", \"intime\"],\n",
    "                parse_dates=[\"intime\"])\n",
    "      .astype({\"stay_id\": \"int32\"})\n",
    ")\n",
    "icu_lookup = icu[[\"stay_id\", \"intime\"]]\n",
    "\n",
    "def within_day(df: pd.DataFrame, time_col: str) -> pd.DataFrame:\n",
    "    \"\"\"Keep rows with time_col between intime −6h and intime +24h.\"\"\"\n",
    "    d = df.merge(icu_lookup, on=\"stay_id\", how=\"left\", validate=\"many_to_one\")\n",
    "    lo = d[\"intime\"] - pd.Timedelta(hours=6)\n",
    "    hi = d[\"intime\"] + pd.Timedelta(days=1)\n",
    "    keep = (d[time_col] >= lo) & (d[time_col] <= hi)\n",
    "    return d.loc[keep].drop(columns=\"intime\")\n",
    "\n",
    "def build_block(csv: Path, cols_map: dict[str, str], time_col=\"charttime\"):\n",
    "    \"\"\"Aggregate min and max values from the CSV for each stay_id.\"\"\"\n",
    "    usecols = [\"stay_id\", time_col] + list(cols_map.keys())\n",
    "    df = pd.read_csv(csv, usecols=usecols, parse_dates=[time_col])\n",
    "    df[\"stay_id\"] = pd.to_numeric(df[\"stay_id\"], errors=\"coerce\").astype(\"Int32\")\n",
    "    df = within_day(df, time_col)\n",
    "    \n",
    "    agg = {}\n",
    "    for col, short in cols_map.items():\n",
    "        agg[f\"{short}_min\"] = (col, \"min\")\n",
    "        agg[f\"{short}_max\"] = (col, \"max\")\n",
    "    \n",
    "    return df.groupby(\"stay_id\", as_index=False).agg(**agg)\n",
    "\n",
    "# Mapping columns for lab panels\n",
    "cbc_cols  = {\n",
    "    \"hematocrit\": \"hematocrit\",\n",
    "    \"hemoglobin\": \"hemoglobin\",\n",
    "    \"platelet\": \"platelets\",\n",
    "    \"wbc\": \"wbc\",\n",
    "}\n",
    "chem_cols = {\n",
    "    \"albumin\": \"albumin\",\n",
    "    \"total_protein\": \"total_protein\",\n",
    "    \"aniongap\": \"aniongap\",\n",
    "    \"bicarbonate\": \"bicarbonate\",\n",
    "    \"bun\": \"bun\",\n",
    "    \"calcium\": \"calcium\",\n",
    "    \"chloride\": \"chloride\",\n",
    "    \"creatinine\": \"creatinine\",\n",
    "    \"glucose\": \"glucose\",\n",
    "    \"sodium\": \"sodium\",\n",
    "    \"potassium\": \"potassium\"\n",
    "}\n",
    "diff_cols = {\n",
    "    \"abs_basophils\": \"abs_basophils\",\n",
    "    \"abs_eosinophils\": \"abs_eosinophils\",\n",
    "    \"abs_lymphocytes\": \"abs_lymphocytes\",\n",
    "    \"abs_monocytes\": \"abs_monocytes\",\n",
    "    \"abs_neutrophils\": \"abs_neutrophils\",\n",
    "    \"atyps\": \"atyps\",\n",
    "    \"bands\": \"bands\",\n",
    "    \"imm_granulocytes\": \"imm_granulocytes\",\n",
    "    \"metas\": \"metas\",\n",
    "    \"nrbc\": \"nrbc\"\n",
    "}\n",
    "coag_cols = {\n",
    "    \"d_dimer\": \"d_dimer\",\n",
    "    \"fibrinogen\": \"fibrinogen\",\n",
    "    \"thrombin\": \"thrombin\",\n",
    "    \"inr\": \"inr\",\n",
    "    \"pt\": \"pt\",\n",
    "    \"ptt\": \"ptt\"\n",
    "}\n",
    "enz_cols  = {\n",
    "    \"alt\": \"alt\",\n",
    "    \"alp\": \"alp\",\n",
    "    \"ast\": \"ast\",\n",
    "    \"amylase\": \"amylase\",\n",
    "    \"bilirubin_total\": \"bilirubin_total\",\n",
    "    \"bilirubin_direct\": \"bilirubin_direct\",\n",
    "    \"bilirubin_indirect\": \"bilirubin_indirect\",\n",
    "    \"ck_cpk\": \"ck_cpk\",\n",
    "    \"ck_mb\": \"ck_mb\",\n",
    "    \"ggt\": \"ggt\",\n",
    "    \"ld_ldh\": \"ld_ldh\"\n",
    "}\n",
    "\n",
    "# Aggregate each lab panel\n",
    "cbc  = build_block(DRV / \"complete_blood_count.csv\", cbc_cols)\n",
    "chem = build_block(DRV / \"chemistry.csv\", chem_cols)\n",
    "diff = build_block(\n",
    "    DRV / \"blood_differential.csv\",\n",
    "    {\n",
    "        \"wbc\": \"wbc\",\n",
    "        \"basophils\": \"basophils\",\n",
    "        \"eosinophils\": \"eosinophils\",\n",
    "        \"lymphocytes\": \"lymphocytes\",\n",
    "        \"monocytes\": \"monocytes\",\n",
    "        \"neutrophils\": \"neutrophils\",\n",
    "        \"atypical_lymphocytes\": \"atypical_lymphocytes\",\n",
    "        \"bands\": \"bands\",\n",
    "        \"nrbc\": \"nrbc\"\n",
    "    }\n",
    ")\n",
    "coag = build_block(DRV / \"coagulation.csv\", coag_cols)\n",
    "coag[\"stay_id\"] = pd.to_numeric(coag[\"stay_id\"], errors=\"coerce\").astype(\"Int32\")\n",
    "enz  = build_block(DRV / \"enzyme.csv\", enz_cols)\n",
    "enz[\"stay_id\"] = pd.to_numeric(enz[\"stay_id\"], errors=\"coerce\").astype(\"Int32\")\n",
    "\n",
    "# Merge lab panels and attach subject_id from ICU stays\n",
    "lab = (\n",
    "    icu[[\"subject_id\", \"stay_id\"]]\n",
    "      .merge(cbc,  on=\"stay_id\", how=\"left\")\n",
    "      .merge(chem, on=\"stay_id\", how=\"left\")\n",
    "      .merge(diff, on=\"stay_id\", how=\"left\")\n",
    "      .merge(coag, on=\"stay_id\", how=\"left\")\n",
    "      .merge(enz,  on=\"stay_id\", how=\"left\")\n",
    ")\n",
    "\n",
    "# Save to CSV\n",
    "lab.to_csv(OUT_CSV, index=False)\n",
    "print(f\"Done – {len(lab):,} rows → {OUT_CSV}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28adcc27",
   "metadata": {},
   "source": [
    "sofa.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1359fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    stay_id  subject_id   hadm_id              intime  rate_dopamine  \\\n",
      "0  39553978    10000032  29079034 2180-07-23 14:00:00            NaN   \n",
      "1  39765666    10000980  26913865 2189-06-27 08:42:00            NaN   \n",
      "2  37067082    10001217  24597018 2157-11-20 19:18:02            NaN   \n",
      "3  34592300    10001217  27703517 2157-12-19 15:42:24            NaN   \n",
      "4  31205490    10001725  25563031 2110-04-11 15:52:22            NaN   \n",
      "\n",
      "   rate_epinephrine  rate_norepinephrine  pao2fio2_novent_min  \\\n",
      "0               NaN                  NaN                  NaN   \n",
      "1               NaN                  NaN                  NaN   \n",
      "2               NaN                  NaN                  NaN   \n",
      "3               NaN                  NaN                  NaN   \n",
      "4               NaN                  NaN                  NaN   \n",
      "\n",
      "   pao2fio2_vent_min  platelets_min  creatinine_max  bilirubin_total_max  \\\n",
      "0                NaN            NaN             NaN                  NaN   \n",
      "1                NaN            NaN             NaN                  NaN   \n",
      "2                NaN            NaN             NaN                  NaN   \n",
      "3                NaN            NaN             NaN                  NaN   \n",
      "4                NaN            NaN             NaN                  NaN   \n",
      "\n",
      "   UrineOutput  mingcs  \n",
      "0        175.0    14.0  \n",
      "1       3900.0    15.0  \n",
      "2       2645.0    15.0  \n",
      "3       2475.0    15.0  \n",
      "4       1965.0    15.0  \n",
      "Done – 73,141 rows → first_day_sofa.csv\n"
     ]
    }
   ],
   "source": [
    "# build_first_day_sofa.py\n",
    "import pandas as pd, numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "BASE = Path(\"./data\")\n",
    "ICU  = BASE / \"icu\"\n",
    "DRV  = BASE / \"derived\"\n",
    "\n",
    "# Clip rows to within a time window around ICU admission\n",
    "def clip_by_intime(df, time_col, lookup, start_h=-6, end_h=24):\n",
    "    df = df.merge(\n",
    "        lookup,\n",
    "        on=\"stay_id\",\n",
    "        how=\"left\",\n",
    "        suffixes=(\"\", \"_icu\"),\n",
    "        validate=\"many_to_one\",\n",
    "    )\n",
    "    if \"intime\" in df.columns:\n",
    "        icu_in = df[\"intime\"].fillna(df[\"intime_icu\"])\n",
    "    else:\n",
    "        icu_in = df.pop(\"intime_icu\")\n",
    "    lo = icu_in + pd.Timedelta(hours=start_h)\n",
    "    hi = icu_in + pd.Timedelta(hours=end_h)\n",
    "    keep = (df[time_col] >= lo) & (df[time_col] <= hi)\n",
    "    df = df.loc[keep]\n",
    "    df = df.drop(columns=[c for c in [\"intime\", \"intime_icu\"] if c in df])\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "# 1. Read ICU stays data\n",
    "icu = (\n",
    "    pd.read_csv(ICU / \"icustays.csv\", parse_dates=[\"intime\"])\n",
    "      .loc[:, [\"subject_id\", \"hadm_id\", \"stay_id\", \"intime\"]]\n",
    ")\n",
    "icu_lookup = icu[[\"stay_id\", \"intime\"]]\n",
    "icu.set_index(\"stay_id\", inplace=True)\n",
    "\n",
    "# 2. Process vasopressor rate events\n",
    "VASO_ITEMID = {\n",
    "    221906: \"norepinephrine\",\n",
    "    30047 : \"norepinephrine\",\n",
    "    221289: \"epinephrine\",\n",
    "    221653: \"dopamine\",\n",
    "    221286: \"dobutamine\",\n",
    "}\n",
    "\n",
    "def within_window(df, key, intime, start_h=-6, end_h=24):\n",
    "    lo = intime + pd.Timedelta(hours=start_h)\n",
    "    hi = intime + pd.Timedelta(hours=end_h)\n",
    "    return df[(df[key] >= lo) & (df[key] <= hi)]\n",
    "\n",
    "inp = pd.read_csv(\n",
    "    ICU / \"inputevents.csv\",\n",
    "    usecols=[\"stay_id\", \"starttime\", \"itemid\", \"rate\"],\n",
    "    parse_dates=[\"starttime\"],\n",
    ")\n",
    "inp = inp[inp[\"itemid\"].isin(VASO_ITEMID)]\n",
    "inp[\"drug\"] = inp[\"itemid\"].map(VASO_ITEMID)\n",
    "inp[\"stay_id\"] = inp[\"stay_id\"].astype(\"int32\")\n",
    "inp = (\n",
    "    inp.join(icu[\"intime\"], on=\"stay_id\", how=\"left\")\n",
    "       .pipe(lambda d: within_window(d, \"starttime\", d[\"intime\"]))\n",
    ")\n",
    "vaso_max = (\n",
    "    inp.groupby([\"stay_id\", \"drug\"], as_index=False)[\"rate\"]\n",
    "       .max()\n",
    "       .pivot(index=\"stay_id\", columns=\"drug\", values=\"rate\")\n",
    "       .rename(columns=lambda c: f\"rate_{c}\")\n",
    ")\n",
    "\n",
    "# 3. Process PaO2/FiO2 data and ventilation info\n",
    "bg = pd.read_csv(\n",
    "    DRV / \"first_day_bg_art.csv\",\n",
    "    usecols=[\"subject_id\", \"charttime\", \"PaO2FiO2\", \"stay_id\"],\n",
    "    parse_dates=[\"charttime\"],\n",
    ")\n",
    "bg = bg.merge(icu[\"intime\"], left_on=\"stay_id\", right_index=True, how=\"left\")\n",
    "bg = clip_by_intime(bg, \"charttime\", icu_lookup)\n",
    "inp = clip_by_intime(inp, \"starttime\", icu_lookup)\n",
    "\n",
    "vent = pd.read_csv(\n",
    "    DRV / \"ventilation.csv\",\n",
    "    usecols=[\"stay_id\", \"starttime\", \"endtime\", \"ventilation_status\"],\n",
    "    parse_dates=[\"starttime\", \"endtime\"],\n",
    ")\n",
    "vent = vent[vent[\"ventilation_status\"] == \"InvasiveVent\"]\n",
    "\n",
    "vent_grp = vent.groupby(\"stay_id\")\n",
    "def is_vent_row(row):\n",
    "    v = vent_grp.get_group(row.stay_id) if row.stay_id in vent_grp.groups else None\n",
    "    return 0 if v is None else int(((v.starttime <= row.charttime) & (row.charttime <= v.endtime)).any())\n",
    "\n",
    "bg[\"isvent\"] = bg.apply(is_vent_row, axis=1)\n",
    "\n",
    "pafi = (\n",
    "    bg.groupby(\"stay_id\")\n",
    "      .agg(\n",
    "        pao2fio2_novent_min=(\"PaO2FiO2\", lambda s: s[bg.loc[s.index, \"isvent\"] == 0].min()),\n",
    "        pao2fio2_vent_min=(\"PaO2FiO2\", lambda s: s[bg.loc[s.index, \"isvent\"] == 1].min()),\n",
    "      )\n",
    ")\n",
    "\n",
    "# 4. Read vitals, labs, urine, and GCS data\n",
    "vital = pd.read_csv(DRV / \"first_day_vitalsign.csv\", usecols=[\"stay_id\"]).set_index(\"stay_id\")\n",
    "lab   = pd.read_csv(\n",
    "    DRV / \"first_day_lab.csv\",\n",
    "    usecols=[\"stay_id\", \"creatinine_max\", \"bilirubin_total_max\", \"platelets_min\"]\n",
    ").set_index(\"stay_id\")\n",
    "uo    = pd.read_csv(DRV / \"first_day_urine_output.csv\", usecols=[\"stay_id\", \"UrineOutput\"]).set_index(\"stay_id\")\n",
    "gcs   = pd.read_csv(DRV / \"first_day_gcs.csv\", usecols=[\"stay_id\", \"mingcs\"]).set_index(\"stay_id\")\n",
    "\n",
    "# 5. Combine all components\n",
    "comp = (\n",
    "    icu.join([vaso_max, pafi, vital, lab, uo, gcs], how=\"left\")\n",
    "       .reset_index()\n",
    ")\n",
    "print(comp.head())\n",
    "\n",
    "for drug in [\"norepinephrine\", \"epinephrine\", \"dopamine\", \"dobutamine\"]:\n",
    "    col = f\"rate_{drug}\"\n",
    "    if col not in comp.columns:\n",
    "        comp[col] = np.nan\n",
    "    else:\n",
    "        comp[col] = comp[col].astype(float)\n",
    "\n",
    "# 6. Define score functions\n",
    "def resp(r):\n",
    "    if pd.isna(r.pao2fio2_vent_min) and pd.isna(r.pao2fio2_novent_min):\n",
    "        return np.nan\n",
    "    if r.pao2fio2_vent_min < 100:\n",
    "        return 4\n",
    "    if r.pao2fio2_vent_min < 200:\n",
    "        return 3\n",
    "    if r.pao2fio2_novent_min < 300:\n",
    "        return 2\n",
    "    if r.pao2fio2_novent_min < 400:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def coag(p):\n",
    "    if pd.isna(p):\n",
    "        return np.nan\n",
    "    if p < 20:\n",
    "        return 4\n",
    "    if p < 50:\n",
    "        return 3\n",
    "    if p < 100:\n",
    "        return 2\n",
    "    if p < 150:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def liver(b):\n",
    "    if pd.isna(b):\n",
    "        return np.nan\n",
    "    if b >= 12:\n",
    "        return 4\n",
    "    if b >= 6:\n",
    "        return 3\n",
    "    if b >= 2:\n",
    "        return 2\n",
    "    if b >= 1.2:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def cardio(row):\n",
    "    dopa, dobu, epi, norepi = (row.rate_dopamine, row.rate_dobutamine,\n",
    "                               row.rate_epinephrine, row.rate_norepinephrine)\n",
    "    if (dopa > 15) or (epi > 0.1) or (norepi > 0.1):\n",
    "        return 4\n",
    "    if (dopa > 5) or (0 <= epi <= 0.1) or (0 <= norepi <= 0.1):\n",
    "        return 3\n",
    "    if (dopa > 0) or (dobu > 0):\n",
    "        return 2\n",
    "    if all(pd.isna(x) for x in [dopa, dobu, epi, norepi]):\n",
    "        return np.nan\n",
    "    return 0\n",
    "\n",
    "def cns(g):\n",
    "    if pd.isna(g):\n",
    "        return np.nan\n",
    "    if g < 6:\n",
    "        return 4\n",
    "    if 6 <= g <= 9:\n",
    "        return 3\n",
    "    if 10 <= g <= 12:\n",
    "        return 2\n",
    "    if 13 <= g <= 14:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def renal(row):\n",
    "    cr, uo = row.creatinine_max, row.UrineOutput\n",
    "    if (cr >= 5) or (uo < 200):\n",
    "        return 4\n",
    "    if (3.5 <= cr < 5) or (uo < 500):\n",
    "        return 3\n",
    "    if 2 <= cr < 3.5:\n",
    "        return 2\n",
    "    if 1.2 <= cr < 2:\n",
    "        return 1\n",
    "    if pd.isna(cr) and pd.isna(uo):\n",
    "        return np.nan\n",
    "    return 0\n",
    "\n",
    "# 7. Compute component scores\n",
    "comp[\"respiration\"]    = comp.apply(resp, axis=1)\n",
    "comp[\"coagulation\"]    = comp.platelets_min.map(coag)\n",
    "comp[\"liver\"]          = comp.bilirubin_total_max.map(liver)\n",
    "comp[\"cardiovascular\"] = comp.apply(cardio, axis=1)\n",
    "comp[\"cns\"]            = comp[\"mingcs\"].map(cns)\n",
    "comp[\"renal\"]          = comp.apply(renal, axis=1)\n",
    "\n",
    "cols = [\"respiration\", \"coagulation\", \"liver\", \"cardiovascular\", \"cns\", \"renal\"]\n",
    "comp[\"sofa\"] = comp[cols].fillna(0).sum(axis=1)\n",
    "\n",
    "# 8. Save output\n",
    "out = comp[[\"subject_id\", \"hadm_id\", \"stay_id\", \"sofa\"] + cols]\n",
    "out.to_csv(\"first_day_sofa.csv\", index=False)\n",
    "print(f\"Done – {len(out):,} rows → first_day_sofa.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766bd940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   subject_id   hadm_id   stay_id  heartrate_min  heartrate_max  \\\n",
      "0    12466550  23998182  30000153           83.0          128.0   \n",
      "1    13180007  27543152  30000213           66.0           91.0   \n",
      "2    12207593  22795209  30000646           69.0          102.0   \n",
      "3    12980335  23552849  30001148           64.0           80.0   \n",
      "4    12168737  29283664  30001336           53.0           73.0   \n",
      "\n",
      "   heartrate_mean  sysbp_min  sysbp_max  sysbp_mean  diasbp_min  ...  \\\n",
      "0      106.840000      108.0      171.0  136.088235        55.0  ...   \n",
      "1       81.680000      116.0      168.0  134.920000        47.0  ...   \n",
      "2       86.423729       71.0      151.0   92.703704        34.0  ...   \n",
      "3       75.520000       92.0      125.0  107.900000        48.0  ...   \n",
      "4       63.040000      100.0      120.0  110.625000        47.0  ...   \n",
      "\n",
      "   resprate_mean  tempc_min  tempc_max  tempc_mean  spo2_min  spo2_max  \\\n",
      "0      14.923077  37.222222  38.222222   37.500000      92.0     100.0   \n",
      "1      19.437500  36.333333  37.555556   37.006944      96.0     100.0   \n",
      "2      25.627119  36.555556  39.111111   37.375000      78.0     100.0   \n",
      "3      14.517241  35.333333  38.166667   36.388889      92.0     100.0   \n",
      "4      26.038462  36.722222  36.944444   36.857143      93.0      99.0   \n",
      "\n",
      "   spo2_mean  glucose_min  glucose_max  glucose_mean  \n",
      "0  96.640000        144.0        192.0    167.857143  \n",
      "1  98.960000         97.0        275.0    168.900000  \n",
      "2  95.610169        102.0        144.0    118.000000  \n",
      "3  98.360000         86.0        149.0    119.500000  \n",
      "4  96.461538         94.0        166.0    128.142857  \n",
      "\n",
      "[5 rows x 27 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"./data/icu/first_day_vitalsign.csv\", nrows=5)\n",
    "\n",
    "print(df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
