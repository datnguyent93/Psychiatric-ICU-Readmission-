{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "16736929",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>stay_id</th>\n",
       "      <th>charttime</th>\n",
       "      <th>storetime</th>\n",
       "      <th>itemid</th>\n",
       "      <th>value</th>\n",
       "      <th>valuenum</th>\n",
       "      <th>valueuom</th>\n",
       "      <th>warning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000032</td>\n",
       "      <td>29079034</td>\n",
       "      <td>39553978</td>\n",
       "      <td>2180-07-23 12:36:00</td>\n",
       "      <td>2180-07-23 14:45:00</td>\n",
       "      <td>226512</td>\n",
       "      <td>39.4</td>\n",
       "      <td>39.4</td>\n",
       "      <td>kg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject_id   hadm_id   stay_id            charttime            storetime  \\\n",
       "0    10000032  29079034  39553978  2180-07-23 12:36:00  2180-07-23 14:45:00   \n",
       "\n",
       "   itemid  value  valuenum valueuom  warning  \n",
       "0  226512   39.4      39.4       kg        0  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./data/icu/chartevents.csv', low_memory=False, nrows=1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59eec13e",
   "metadata": {},
   "source": [
    "first_day_vitalsign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19465040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading icustays …\n",
      "Scanning chartevents …\n",
      "Building wide DataFrame …\n",
      "Wrote 73,126 rows to data/icu/first_day_vitalsign.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "---------------------------------\n",
    "Create first-24-hour vital-sign summary per ICU stay (MIMIC-IV).\n",
    "\n",
    "• Input  : ./data/icu/icustays.csv\n",
    "           ./data/icu/chartevents.csv(.gz)\n",
    "\n",
    "• Output : ./data/icu/first_day_vitalsign.csv\n",
    "           one row per stay_id, columns:\n",
    "             heartrate_{min/max/mean}\n",
    "             sysbp_…  diasbp_…  meanbp_…\n",
    "             resprate_…  tempc_…  spo2_…  glucose_…\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 0.  Paths\n",
    "# ------------------------------------------------------------------\n",
    "BASE = Path(\"./data/icu\")          \n",
    "CHART = BASE / \"chartevents.csv\"  \n",
    "ICU   = BASE / \"icustays.csv\"\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1.  Vital-sign ITEMID → category\n",
    "# ------------------------------------------------------------------\n",
    "VITAL_MAP = {\n",
    "    # Heart rate\n",
    "    211: 1, 220045: 1,\n",
    "    # Systolic BP\n",
    "    51: 2, 442: 2, 455: 2, 6701: 2, 220179: 2, 220050: 2, 225309: 2,\n",
    "    # Diastolic BP\n",
    "    8368: 3, 8440: 3, 8441: 3, 8555: 3, 220180: 3, 220051: 3, 225310: 3,\n",
    "    # Mean BP\n",
    "    456: 4, 52: 4, 6702: 4, 443: 4, 220052: 4, 220181: 4, 225312: 4,\n",
    "    # Resp. rate\n",
    "    615: 5, 618: 5, 220210: 5, 224690: 5,\n",
    "    # Temperature °F\n",
    "    223761: 6, 678: 6,\n",
    "    # Temperature °C\n",
    "    223762: 6, 676: 6,\n",
    "    # SpO₂\n",
    "    646: 7, 220277: 7,\n",
    "    # Glucose\n",
    "    807: 8, 811: 8, 1529: 8, 3745: 8, 3744: 8,\n",
    "    225664: 8, 220621: 8, 226537: 8,\n",
    "}\n",
    "TEMP_F_IDS = {223761, 678}         # Fahrenheit → Celsius\n",
    "\n",
    "CAT2NAME = {\n",
    "    1: \"heartrate\", 2: \"sysbp\", 3: \"diasbp\", 4: \"meanbp\",\n",
    "    5: \"resprate\", 6: \"tempc\", 7: \"spo2\",   8: \"glucose\",\n",
    "}\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2.  Load icustays (stay-level times)\n",
    "# ------------------------------------------------------------------\n",
    "print(\"Loading icustays …\")\n",
    "icu = pd.read_csv(ICU, usecols=[\"stay_id\", \"subject_id\", \"hadm_id\", \"intime\"],\n",
    "                  parse_dates=[\"intime\"])\n",
    "icu.set_index(\"stay_id\", inplace=True)         # fast lookup\n",
    "intime_series = icu[\"intime\"]\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3.  Prepare aggregation holders\n",
    "# ------------------------------------------------------------------\n",
    "agg_min = {}     # (stay, cat) → min\n",
    "agg_max = {}     # (stay, cat) → max\n",
    "agg_sum = {}     # (stay, cat) → running sum\n",
    "agg_cnt = {}     # (stay, cat) → count for mean\n",
    "\n",
    "def update(stay, cat, val):\n",
    "    key = (stay, cat)\n",
    "    if key not in agg_min:\n",
    "        agg_min[key] = val\n",
    "        agg_max[key] = val\n",
    "        agg_sum[key] = val\n",
    "        agg_cnt[key] = 1\n",
    "    else:\n",
    "        agg_min[key] = min(agg_min[key], val)\n",
    "        agg_max[key] = max(agg_max[key], val)\n",
    "        agg_sum[key] += val\n",
    "        agg_cnt[key] += 1\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4.  Stream chartevents in chunks (huge file)\n",
    "# ------------------------------------------------------------------\n",
    "USECOLS = [\"stay_id\", \"itemid\", \"valuenum\", \"charttime\"]\n",
    "CHUNK = 1_000_000\n",
    "print(\"Scanning chartevents …\")\n",
    "for chunk in pd.read_csv(CHART, usecols=USECOLS,\n",
    "                         parse_dates=[\"charttime\"],\n",
    "                         chunksize=CHUNK, low_memory=False):\n",
    "    # keep rows we might want\n",
    "    chunk = chunk[chunk[\"itemid\"].isin(VITAL_MAP)]\n",
    "    # drop junk\n",
    "    chunk = chunk[pd.notnull(chunk[\"valuenum\"])]\n",
    "\n",
    "    # attach ICU intime (vectorised map)\n",
    "    chunk = chunk.join(intime_series, on=\"stay_id\", how=\"inner\")\n",
    "    # time since admit, in hours\n",
    "    delta = (chunk[\"charttime\"] - chunk[\"intime\"]).dt.total_seconds() / 3600.0\n",
    "    chunk = chunk[(delta > 0) & (delta <= 24)]\n",
    "    if chunk.empty:\n",
    "        continue\n",
    "\n",
    "    # Fahrenheit → Celsius\n",
    "    f_mask = chunk[\"itemid\"].isin(TEMP_F_IDS)\n",
    "    chunk.loc[f_mask, \"valuenum\"] = (chunk.loc[f_mask, \"valuenum\"] - 32.0) / 1.8\n",
    "\n",
    "    # map to category\n",
    "    chunk[\"cat\"] = chunk[\"itemid\"].map(VITAL_MAP).astype(np.int8)\n",
    "\n",
    "    # aggregate\n",
    "    for row in chunk.itertuples(index=False):\n",
    "        update(row.stay_id, row.cat, row.valuenum)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 5.  Build wide table\n",
    "# ------------------------------------------------------------------\n",
    "print(\"Building wide DataFrame …\")\n",
    "records = []\n",
    "for (stay, cat), _ in agg_min.items():\n",
    "    prefix = CAT2NAME[cat]\n",
    "    records.append({\n",
    "        \"stay_id\": stay,\n",
    "        f\"{prefix}_min\" : agg_min[(stay, cat)],\n",
    "        f\"{prefix}_max\" : agg_max[(stay, cat)],\n",
    "        f\"{prefix}_mean\": agg_sum[(stay, cat)] / agg_cnt[(stay, cat)],\n",
    "    })\n",
    "wide = pd.DataFrame(records)\n",
    "\n",
    "# pivot to one row per stay\n",
    "wide = wide.groupby(\"stay_id\").first().reset_index()\n",
    "\n",
    "# add subject/hadm for convenience\n",
    "wide = wide.merge(icu.reset_index()[[\"stay_id\", \"subject_id\", \"hadm_id\"]],\n",
    "                  on=\"stay_id\", how=\"left\")\n",
    "\n",
    "# column order\n",
    "cols = [\"subject_id\", \"hadm_id\", \"stay_id\"]\n",
    "for name in CAT2NAME.values():\n",
    "    cols += [f\"{name}_min\", f\"{name}_max\", f\"{name}_mean\"]\n",
    "wide = wide.reindex(columns=cols)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 6.  Save\n",
    "# ------------------------------------------------------------------\n",
    "OUT = BASE / \"first_day_vitalsign.csv\"\n",
    "wide.to_csv(OUT, index=False)\n",
    "print(f\"Wrote {len(wide):,} rows to {OUT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7a2e2b",
   "metadata": {},
   "source": [
    "first_day_urine_output.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cd3e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading icustays …\n",
      "Scanning outputevents …\n",
      "Building CSV …\n",
      "Wrote 69,730 rows ➜ data/icu/first_day_urine_output.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    ".py\n",
    "-------------------------\n",
    "Sum urine output for the first 24 h of every ICU stay.\n",
    "\n",
    "• Input  : ./data/icu/icustays.csv(.gz)\n",
    "           ./data/icu/outputevents.csv(.gz)\n",
    "• Output : ./data/icu/first_day_urine_output.csv\n",
    "           columns = subject_id, hadm_id, stay_id, UrineOutput\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1.  File locations\n",
    "# ------------------------------------------------------------------\n",
    "BASE = Path(\"./data/icu\")         \n",
    "ICU   = BASE / \"icustays.csv\"   \n",
    "OE    = BASE / \"outputevents.csv\"\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2.  ITEMIDs that represent urine output in MIMIC-IV\n",
    "# ------------------------------------------------------------------\n",
    "URINE_IDS = {\n",
    "    # CareVue\n",
    "    40055, 43175, 40069, 40094, 40715, 40473, 40085, 40057, 40056,\n",
    "    40405, 40428, 40086, 40096, 40651,\n",
    "    # MetaVision\n",
    "    226559, 226560, 226561, 226584, 226563, 226564,\n",
    "    226565, 226567, 226557, 226558,\n",
    "    # GU irrigant (in/out)\n",
    "    227488, 227489,\n",
    "}\n",
    "IRRIGANT_IN = 227488      # subtract this one\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3.  Load ICU stays (gives us intime & patient IDs)\n",
    "# ------------------------------------------------------------------\n",
    "print(\"Loading icustays …\")\n",
    "icu = pd.read_csv(\n",
    "    ICU,\n",
    "    usecols=[\"stay_id\", \"subject_id\", \"hadm_id\", \"intime\"],\n",
    "    parse_dates=[\"intime\"],\n",
    ")\n",
    "icu.set_index(\"stay_id\", inplace=True)\n",
    "intime = icu[\"intime\"]          # Series for fast lookup\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4.  Aggregate urine output in one pass\n",
    "# ------------------------------------------------------------------\n",
    "totals = defaultdict(float)      # stay_id → cumulative volume (mL)\n",
    "USECOLS = [\"stay_id\", \"itemid\", \"charttime\", \"value\"]\n",
    "CHUNK = 1_000_000\n",
    "\n",
    "print(\"Scanning outputevents …\")\n",
    "for chunk in pd.read_csv(\n",
    "        OE,\n",
    "        usecols=USECOLS,\n",
    "        parse_dates=[\"charttime\"],\n",
    "        low_memory=False,\n",
    "        chunksize=CHUNK,\n",
    "):\n",
    "    # keep only relevant itemids and non-null values\n",
    "    chunk = chunk[chunk[\"itemid\"].isin(URINE_IDS)]\n",
    "    chunk = chunk[pd.notnull(chunk[\"value\"])]\n",
    "    if chunk.empty:\n",
    "        continue\n",
    "\n",
    "    # attach ICU admit time\n",
    "    chunk = chunk.join(intime, on=\"stay_id\", how=\"inner\")\n",
    "    # restrict to first 24 h\n",
    "    delta_hr = (chunk[\"charttime\"] - chunk[\"intime\"]).dt.total_seconds() / 3600.0\n",
    "    chunk = chunk[(delta_hr > 0) & (delta_hr <= 24)]\n",
    "    if chunk.empty:\n",
    "        continue\n",
    "\n",
    "    # apply GU-irrigant rule (value becomes negative)\n",
    "    irr_mask = chunk[\"itemid\"] == IRRIGANT_IN\n",
    "    chunk.loc[irr_mask, \"value\"] = -1.0 * chunk.loc[irr_mask, \"value\"]\n",
    "\n",
    "    # accumulate\n",
    "    for row in chunk.itertuples(index=False):\n",
    "        totals[row.stay_id] += row.value\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 5.  Build result DataFrame\n",
    "# ------------------------------------------------------------------\n",
    "print(\"Building CSV …\")\n",
    "res = (\n",
    "    pd.DataFrame({\"stay_id\": list(totals.keys()),\n",
    "                  \"UrineOutput\": list(totals.values())})\n",
    "      .merge(icu.reset_index()[[\"stay_id\", \"subject_id\", \"hadm_id\"]],\n",
    "             on=\"stay_id\", how=\"left\")\n",
    "      .loc[:, [\"subject_id\", \"hadm_id\", \"stay_id\", \"UrineOutput\"]]\n",
    "      .sort_values([\"subject_id\", \"hadm_id\", \"stay_id\"])\n",
    ")\n",
    "\n",
    "OUT = BASE / \"first_day_urine_output.csv\"\n",
    "res.to_csv(OUT, index=False)\n",
    "print(f\"Wrote {len(res):,} rows ➜ {OUT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad2e1a2",
   "metadata": {},
   "source": [
    "first_day_gcs.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213bc4d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading icustays …\n",
      "Scanning chartevents …\n",
      "Building result …\n",
      "Wrote 72,538 rows ➜ data/icu/first_day_gcs.csv\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "----------------\n",
    "Compute the minimum Glasgow-Coma-Scale score in the first 24 h of every ICU\n",
    "stay, following the exact logic in the original SQL.\n",
    "\n",
    "Input  (./data/icu/)\n",
    "    • icustays.csv[.gz]         – gives stay_id, subject_id, hadm_id, intime\n",
    "    • chartevents.csv[.gz]      – raw bedside charting data\n",
    "\n",
    "Output\n",
    "    • first_day_gcs.csv         – one row per stay_id with:\n",
    "        subject_id, hadm_id, stay_id,\n",
    "        mingcs, gcsmotor, gcsverbal, gcseyes, endotrachflag\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 1.  File locations / parameters\n",
    "# ----------------------------------------------------------------------\n",
    "BASE   = Path(\"./data/icu\")          \n",
    "ICU_FN = BASE / \"icustays.csv\"    \n",
    "CE_FN  = BASE / \"chartevents.csv\"\n",
    "CHUNK  = 1_000_000                    # streaming\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 2.  ITEMID mapping  (Metavision → CareVue IDs)\n",
    "# ----------------------------------------------------------------------\n",
    "MAP_IDS = {\n",
    "    223900: 723,   # Verbal\n",
    "    223901: 454,   # Motor\n",
    "    220739: 184,   # Eyes\n",
    "}\n",
    "ALL_IDS = {184, 454, 723, *MAP_IDS.keys()}\n",
    "\n",
    "TEMP_INTUB_IDS = {   # (itemid, value_text) pairs that mean “intubated”\n",
    "    (723,    \"1.0 ET/Trach\"),\n",
    "    (223900, \"No Response-ETT\"),\n",
    "}\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 3.  Load icustays (intime & identifiers)\n",
    "# ----------------------------------------------------------------------\n",
    "print(\"Loading icustays …\")\n",
    "icu = pd.read_csv(\n",
    "    ICU_FN,\n",
    "    usecols=[\"stay_id\", \"subject_id\", \"hadm_id\", \"intime\"],\n",
    "    parse_dates=[\"intime\"],\n",
    ")\n",
    "icu.set_index(\"stay_id\", inplace=True)\n",
    "intime = icu[\"intime\"]                       # Series for fast lookup\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 4.  Holders for per-stay minimum GCS info\n",
    "# ----------------------------------------------------------------------\n",
    "min_gcs      = {}          # stay_id → minimal GCS score\n",
    "min_components = {}        # stay_id → (motor, verbal, eyes, endotrachflag)\n",
    "\n",
    "# For each stay we also track the “previous” GCS components (<=6 h old)\n",
    "prev_val   = {}            # stay_id → dict(component → (value, time))\n",
    "\n",
    "def update_prev(stay, comp, val, t):\n",
    "    \"\"\"Store the latest component value for a stay (used for ≤6 h look-back).\"\"\"\n",
    "    if stay not in prev_val:\n",
    "        prev_val[stay] = {}\n",
    "    prev_val[stay][comp] = (val, t)\n",
    "\n",
    "def get_prev(stay, comp, now):\n",
    "    \"\"\"Return previous value of <comp> if within 6 h, else None.\"\"\"\n",
    "    if stay not in prev_val or comp not in prev_val[stay]:\n",
    "        return None\n",
    "    val, t_prev = prev_val[stay][comp]\n",
    "    if (now - t_prev).total_seconds() / 3600.0 <= 6:\n",
    "        return val\n",
    "    return None\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 5.  Stream chartevents and process in time order\n",
    "# ----------------------------------------------------------------------\n",
    "USECOLS = [\"stay_id\", \"itemid\", \"charttime\", \"valuenum\", \"value\"]\n",
    "print(\"Scanning chartevents …\")\n",
    "for chunk in pd.read_csv(\n",
    "        CE_FN,\n",
    "        usecols=USECOLS,\n",
    "        parse_dates=[\"charttime\"],\n",
    "        chunksize=CHUNK,\n",
    "        low_memory=False):\n",
    "\n",
    "    # keep only rows we want\n",
    "    chunk = chunk[chunk[\"itemid\"].isin(ALL_IDS)]\n",
    "    chunk = chunk.merge(intime, on=\"stay_id\", how=\"inner\")      # add ICU intime\n",
    "    # first-24-h window\n",
    "    delta = (chunk[\"charttime\"] - chunk[\"intime\"]).dt.total_seconds() / 3600.0\n",
    "    chunk = chunk[(delta > 0) & (delta <= 24)]\n",
    "    if chunk.empty:\n",
    "        continue\n",
    "\n",
    "    # map Metavision IDs → CareVue IDs\n",
    "    chunk[\"itemid\"] = chunk[\"itemid\"].replace(MAP_IDS)\n",
    "\n",
    "    # figure out numeric component value\n",
    "    def _num(row):\n",
    "        # Intubated verbal response → 0\n",
    "        if (row.itemid, str(row.value).strip()) in TEMP_INTUB_IDS:\n",
    "            return 0.0\n",
    "        return row.valuenum\n",
    "    chunk[\"comp_val\"] = chunk.apply(_num, axis=1)\n",
    "\n",
    "    # drop rows with no numeric value\n",
    "    chunk = chunk[pd.notnull(chunk[\"comp_val\"])]\n",
    "\n",
    "    # sort so we visit events in chronological order per stay\n",
    "    chunk.sort_values([\"stay_id\", \"charttime\"], inplace=True)\n",
    "\n",
    "    # process rows\n",
    "    for row in chunk.itertuples(index=False):\n",
    "        stay   = row.stay_id\n",
    "        comp   = row.itemid          # 454 motor, 723 verbal, 184 eyes\n",
    "        val    = float(row.comp_val)\n",
    "        tstamp = row.charttime\n",
    "\n",
    "        # record component value for this timestamp\n",
    "        update_prev(stay, comp, val, tstamp)\n",
    "\n",
    "        # Get current set of components (some may be None)\n",
    "        motor  = val if comp == 454 else get_prev(stay, 454, tstamp)\n",
    "        verbal = val if comp == 723 else get_prev(stay, 723, tstamp)\n",
    "        eyes   = val if comp == 184 else get_prev(stay, 184, tstamp)\n",
    "\n",
    "        # We only compute if at least one component is new/non-null\n",
    "        if motor is None and verbal is None and eyes is None:\n",
    "            continue\n",
    "\n",
    "        endotrachflag = 1 if verbal == 0 else 0\n",
    "\n",
    "        # previous (≤6 h) components for imputation\n",
    "        pmotor  = get_prev(stay, 454, tstamp)\n",
    "        pverbal = get_prev(stay, 723, tstamp)\n",
    "        peyes   = get_prev(stay, 184, tstamp)\n",
    "\n",
    "        # ------- SQL logic replicated -------\n",
    "        if verbal == 0 or (verbal is None and pverbal == 0):\n",
    "            gcs = 15\n",
    "        elif pverbal == 0:\n",
    "            gcs = (\n",
    "                (motor  if motor  is not None else 6) +\n",
    "                (verbal if verbal is not None else 5) +\n",
    "                (eyes   if eyes   is not None else 4)\n",
    "            )\n",
    "        else:\n",
    "            gcs = (\n",
    "                (motor  if motor  is not None else (pmotor  if pmotor  is not None else 6)) +\n",
    "                (verbal if verbal is not None else (pverbal if pverbal is not None else 5)) +\n",
    "                (eyes   if eyes   is not None else (peyes   if peyes   is not None else 4))\n",
    "            )\n",
    "        # ------------------------------------\n",
    "\n",
    "        # keep minimal GCS\n",
    "        if (stay not in min_gcs) or (gcs < min_gcs[stay]):\n",
    "            min_gcs[stay]        = gcs\n",
    "            min_components[stay] = (\n",
    "                motor  if motor  is not None else pmotor  if pmotor  is not None else 6,\n",
    "                verbal if verbal is not None else pverbal if pverbal is not None else 5,\n",
    "                eyes   if eyes   is not None else peyes   if peyes   is not None else 4,\n",
    "                endotrachflag,\n",
    "            )\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 6.  Assemble result table\n",
    "# ----------------------------------------------------------------------\n",
    "print(\"Building result …\")\n",
    "records = []\n",
    "for stay, gcs in min_gcs.items():\n",
    "    motor, verbal, eyes, etflag = min_components[stay]\n",
    "    subj  = icu.at[stay, \"subject_id\"]\n",
    "    hadm  = icu.at[stay, \"hadm_id\"]\n",
    "    records.append(\n",
    "        (subj, hadm, stay, gcs, motor, verbal, eyes, etflag)\n",
    "    )\n",
    "\n",
    "res = pd.DataFrame(\n",
    "    records,\n",
    "    columns=[\n",
    "        \"subject_id\", \"hadm_id\", \"stay_id\",\n",
    "        \"mingcs\", \"gcsmotor\", \"gcsverbal\", \"gcseyes\", \"endotrachflag\"\n",
    "    ],\n",
    ").sort_values([\"subject_id\", \"hadm_id\", \"stay_id\"])\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 7.  Save\n",
    "# ----------------------------------------------------------------------\n",
    "OUT = BASE / \"first_day_gcs.csv\"\n",
    "res.to_csv(OUT, index=False)\n",
    "print(f\"Wrote {len(res):,} rows ➜ {OUT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6616e6",
   "metadata": {},
   "source": [
    "first_day_lab.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbf9e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading icustays …\n",
      "Scanning labevents …\n",
      "Building wide DataFrame …\n",
      "Wrote 71,945 rows ➜ data/icu/first_day_lab.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "================\n",
    "Build first_day_lab.csv from local MIMIC-IV CSVs.\n",
    "\n",
    "Input  (./data/)\n",
    "    • icu/icustays.csv[.gz]       – stay_id, subject_id, hadm_id, intime\n",
    "    • hosp/labevents.csv[.gz]     – lab results\n",
    "\n",
    "Output\n",
    "    • icu/first_day_lab.csv\n",
    "      columns:\n",
    "        subject_id, hadm_id, stay_id,\n",
    "        aniongap_min, aniongap_max, albumin_min, albumin_max, …, wbc_max\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 1  Paths & parameters\n",
    "# ----------------------------------------------------------------------\n",
    "ROOT  = Path(\"./data\")\n",
    "ICU   = ROOT / \"icu\"  / \"icustays.csv\"\n",
    "LAB   = ROOT / \"hosp\" / \"labevents.csv\"\n",
    "CHUNK = 1_000_000                       # rows streamed at once\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 2  ItemID → label mapping  (unchanged from official SQL)\n",
    "# ----------------------------------------------------------------------\n",
    "ID2LAB = {\n",
    "    50868: \"aniongap\",\n",
    "    50862: \"albumin\",\n",
    "    51144: \"bands\",\n",
    "    50882: \"bicarbonate\",\n",
    "    50885: \"bilirubin\",\n",
    "    50912: \"creatinine\",\n",
    "    50902: \"chloride\",\n",
    "    50806: \"chloride\",\n",
    "    50809: \"glucose\",\n",
    "    50931: \"glucose\",\n",
    "    50810: \"hematocrit\",\n",
    "    51221: \"hematocrit\",\n",
    "    50811: \"hemoglobin\",\n",
    "    51222: \"hemoglobin\",\n",
    "    50813: \"lactate\",\n",
    "    51265: \"platelet\",\n",
    "    50822: \"potassium\",\n",
    "    50971: \"potassium\",\n",
    "    51275: \"ptt\",\n",
    "    51237: \"inr\",\n",
    "    51274: \"pt\",\n",
    "    50824: \"sodium\",\n",
    "    50983: \"sodium\",\n",
    "    51006: \"bun\",\n",
    "    51301: \"wbc\",\n",
    "    51300: \"wbc\",\n",
    "}\n",
    "\n",
    "# sanity-check upper limits (itemid → max allowed); values above are discarded\n",
    "UPPER = {\n",
    "    50862: 10,     50868: 1e4,  51144: 100,\n",
    "    50882: 1e4,    50885: 150,  50912: 150,\n",
    "    50806: 1e4,    50902: 1e4,  50809: 1e4,\n",
    "    50931: 1e4,    50810: 100,  51221: 100,\n",
    "    50811: 50,     51222: 50,   50813: 50,\n",
    "    51265: 1e4,    50822: 30,   50971: 30,\n",
    "    51275: 150,    51237: 50,   51274: 150,\n",
    "    50824: 200,    50983: 200,  51006: 300,\n",
    "    51300: 1000,   51301: 1000,\n",
    "}\n",
    "\n",
    "KEEP_IDS = set(ID2LAB.keys())\n",
    "\n",
    "LABS = [\"aniongap\",\"albumin\",\"bands\",\"bicarbonate\",\"bilirubin\",\"creatinine\",\n",
    "        \"chloride\",\"glucose\",\"hematocrit\",\"hemoglobin\",\"lactate\",\"platelet\",\n",
    "        \"potassium\",\"ptt\",\"inr\",\"pt\",\"sodium\",\"bun\",\"wbc\"]\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 3  Load icustays (small) – gives intime and IDs\n",
    "# ----------------------------------------------------------------------\n",
    "print(\"Loading icustays …\")\n",
    "icu = pd.read_csv(\n",
    "    ICU,\n",
    "    usecols=[\"stay_id\", \"subject_id\", \"hadm_id\", \"intime\"],\n",
    "    parse_dates=[\"intime\"],\n",
    ")\n",
    "icu.set_index(\"stay_id\", inplace=True)\n",
    "intime = icu[\"intime\"]                      # Series for quick lookup\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 4  Dictionaries to collect min / max per stay & lab\n",
    "# ----------------------------------------------------------------------\n",
    "lab_min = {}      # (stay, lab) → min value\n",
    "lab_max = {}      # (stay, lab) → max value\n",
    "\n",
    "def update(stay, lab, val):\n",
    "    key = (stay, lab)\n",
    "    if key not in lab_min:\n",
    "        lab_min[key] = lab_max[key] = val\n",
    "    else:\n",
    "        lab_min[key] = min(lab_min[key], val)\n",
    "        lab_max[key] = max(lab_max[key], val)\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 5  Stream labevents\n",
    "# ----------------------------------------------------------------------\n",
    "print(\"Scanning labevents …\")\n",
    "USE = [\"subject_id\",\"hadm_id\",\"itemid\",\"charttime\",\"valuenum\"]\n",
    "for chunk in pd.read_csv(\n",
    "        LAB, usecols=USE, parse_dates=[\"charttime\"],\n",
    "        chunksize=CHUNK, low_memory=False):\n",
    "\n",
    "    chunk = chunk[chunk[\"itemid\"].isin(KEEP_IDS)]\n",
    "    chunk = chunk[pd.notnull(chunk[\"valuenum\"]) & (chunk[\"valuenum\"] > 0)]\n",
    "    if chunk.empty:\n",
    "        continue\n",
    "\n",
    "    # merge to icustays via (subject_id, hadm_id)\n",
    "    chunk = chunk.merge(\n",
    "        icu.reset_index()[[\"stay_id\",\"subject_id\",\"hadm_id\",\"intime\"]],\n",
    "        on=[\"subject_id\",\"hadm_id\"], how=\"inner\",\n",
    "    )\n",
    "    if chunk.empty:\n",
    "        continue\n",
    "\n",
    "    # time filter: intime-6 h ≤ charttime ≤ intime+24 h\n",
    "    dt = (chunk[\"charttime\"] - chunk[\"intime\"]).dt.total_seconds() / 3600.0\n",
    "    chunk = chunk[(dt >= -6) & (dt <= 24)]\n",
    "    if chunk.empty:\n",
    "        continue\n",
    "\n",
    "    # quality filters\n",
    "    mask_good = np.ones(len(chunk), dtype=bool)\n",
    "    for item, lim in UPPER.items():\n",
    "        idx = (chunk[\"itemid\"] == item) & (chunk[\"valuenum\"] > lim)\n",
    "        mask_good &= ~idx\n",
    "    # bands allow 0–100 only\n",
    "    mask_good &= ~((chunk[\"itemid\"] == 51144) & (chunk[\"valuenum\"] < 0))\n",
    "    chunk = chunk[mask_good]\n",
    "    if chunk.empty:\n",
    "        continue\n",
    "\n",
    "    # map itemid → lab label\n",
    "    chunk[\"lab\"] = chunk[\"itemid\"].map(ID2LAB)\n",
    "\n",
    "    # aggregate\n",
    "    for row in chunk.itertuples(index=False):\n",
    "        update(row.stay_id, row.lab, row.valuenum)\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 6  Build wide table\n",
    "# ----------------------------------------------------------------------\n",
    "print(\"Building wide DataFrame …\")\n",
    "records = []\n",
    "for (stay, lab) in lab_min:\n",
    "    records.append({\n",
    "        \"stay_id\": stay,\n",
    "        f\"{lab}_min\": lab_min[(stay, lab)],\n",
    "        f\"{lab}_max\": lab_max[(stay, lab)],\n",
    "    })\n",
    "wide = pd.DataFrame(records).groupby(\"stay_id\").first().reset_index()\n",
    "\n",
    "# add identifiers\n",
    "wide = wide.merge(\n",
    "    icu.reset_index()[[\"stay_id\",\"subject_id\",\"hadm_id\"]],\n",
    "    on=\"stay_id\", how=\"left\",\n",
    ")\n",
    "\n",
    "# column order\n",
    "cols = [\"subject_id\",\"hadm_id\",\"stay_id\"]\n",
    "for lb in LABS:\n",
    "    cols += [f\"{lb}_min\", f\"{lb}_max\"]\n",
    "wide = wide.reindex(columns=cols)\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 7  Save\n",
    "# ----------------------------------------------------------------------\n",
    "OUT = ROOT / \"icu\" / \"first_day_lab.csv\"\n",
    "wide.to_csv(OUT, index=False)\n",
    "print(f\"Wrote {len(wide):,} rows ➜ {OUT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c078b247",
   "metadata": {},
   "source": [
    "first_day_blood_gas.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f23d5ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading icustays …\n",
      "Scanning labevents …\n",
      "Pivoting …\n",
      "Writing CSV …\n",
      "Wrote 222,866 rows ➜ data/icu/first_day_blood_gas.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "first_day_blood_gas.py\n",
    "----------------------\n",
    "Pivot all blood-gas / chemistry results taken in the first 24 h of each ICU\n",
    "stay into one wide table (one row per stay+chart-time).\n",
    "\n",
    "Inputs   ./data/icu/\n",
    "    • icustays.csv[.gz]         – stay_id, subject_id, hadm_id, intime\n",
    "    • ../hosp/labevents.csv[.gz]  (or same folder) – raw lab events\n",
    "\n",
    "Output   ./data/icu/first_day_blood_gas.csv\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 1.  File locations (edit if your layout differs)\n",
    "# ----------------------------------------------------------------------\n",
    "ROOT   = Path(\"./data\")\n",
    "ICU_F  = ROOT / \"icu\"  / \"icustays.csv\"\n",
    "LAB_F  = ROOT / \"hosp\" / \"labevents.csv\"     # typical kaggle export\n",
    "        \n",
    "OUT_F  = ROOT / \"icu\" / \"first_day_blood_gas.csv\"\n",
    "CHUNK  = 1_000_000\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 2.  ITEMID → label mapping  (exact copy of the SQL CASE block)\n",
    "# ----------------------------------------------------------------------\n",
    "ID2LBL = {\n",
    "    50800:'specimen', 50801:'aado2', 50802:'baseexcess', 50803:'bicarbonate',\n",
    "    50804:'totalco2', 50805:'carboxyhemoglobin', 50806:'chloride',\n",
    "    50808:'calcium',   50809:'glucose', 50810:'hematocrit', 50811:'hemoglobin',\n",
    "    50812:'intubated', 50813:'lactate', 50814:'methemoglobin', 50815:'o2flow',\n",
    "    50816:'fio2',      50817:'so2',      50818:'pco2', 50819:'peep',\n",
    "    50820:'ph',        50821:'po2',      50822:'potassium', 50823:'requiredo2',\n",
    "    50824:'sodium',    50825:'temperature', 50826:'tidalvolume',\n",
    "    50827:'ventilationrate', 50828:'ventilator',\n",
    "}\n",
    "\n",
    "KEEP_IDS = set(ID2LBL.keys()) | {51545}      # 51545 = “something” in old view\n",
    "LABELS = list(dict.fromkeys(ID2LBL.values()))  # keep order, remove dup\n",
    "\n",
    "# upper-limit sanity filters (same as SQL WHEN … THEN NULL)\n",
    "UPPER = {50810:100, 50816:100, 50817:100, 50815:70, 50821:800}\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 3.  Load icustays (small)\n",
    "# ----------------------------------------------------------------------\n",
    "print(\"Loading icustays …\")\n",
    "icu = pd.read_csv(ICU_F,\n",
    "                  usecols=[\"stay_id\",\"subject_id\",\"hadm_id\",\"intime\"],\n",
    "                  parse_dates=[\"intime\"])\n",
    "icu.set_index(\"stay_id\", inplace=True)\n",
    "intime = icu[\"intime\"]\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 4.  Streaming pivot\n",
    "# ----------------------------------------------------------------------\n",
    "print(\"Scanning labevents …\")\n",
    "rows = []\n",
    "\n",
    "usecols = [\"subject_id\",\"hadm_id\",\n",
    "           \"itemid\",\"charttime\",\"value\",\"valuenum\"]\n",
    "for chunk in pd.read_csv(LAB_F, usecols=usecols, parse_dates=[\"charttime\"],\n",
    "                         chunksize=CHUNK, low_memory=False):\n",
    "\n",
    "    chunk = chunk[chunk[\"itemid\"].isin(KEEP_IDS)]\n",
    "    chunk = chunk[pd.notnull(chunk[\"valuenum\"])]\n",
    "    if chunk.empty:\n",
    "        continue\n",
    "\n",
    "    # joinOn stay_id quickly via subject+hadm\n",
    "    chunk = chunk.merge(icu.reset_index(),\n",
    "                        on=[\"subject_id\",\"hadm_id\"], how=\"inner\")\n",
    "    if chunk.empty:\n",
    "        continue\n",
    "\n",
    "    # first-day window  [-6h, +24h]\n",
    "    dt = (chunk[\"charttime\"] - chunk[\"intime\"]).dt.total_seconds()/3600\n",
    "    chunk = chunk[(dt >= -6) & (dt <= 24)]\n",
    "    if chunk.empty:\n",
    "        continue\n",
    "\n",
    "    # sanity truncations\n",
    "    ok = np.ones(len(chunk), bool)\n",
    "    for iid, limit in UPPER.items():\n",
    "        ok &= ~((chunk[\"itemid\"] == iid) & (chunk[\"valuenum\"] > limit))\n",
    "    chunk = chunk[ok]\n",
    "    if chunk.empty:\n",
    "        continue\n",
    "\n",
    "    # FiO2 must lie 21-100\n",
    "    mask_fio2 = chunk[\"itemid\"] == 50816\n",
    "    chunk.loc[mask_fio2 & (chunk[\"valuenum\"] < 20), \"valuenum\"] = np.nan\n",
    "    chunk.loc[mask_fio2 & (chunk[\"valuenum\"] > 100), \"valuenum\"] = np.nan\n",
    "\n",
    "    # O2 sat ≤ 100\n",
    "    mask_so2 = chunk[\"itemid\"] == 50817\n",
    "    chunk.loc[mask_so2 & (chunk[\"valuenum\"] > 100), \"valuenum\"] = np.nan\n",
    "\n",
    "    # negative values not allowed except BASEEXCESS (50802)\n",
    "    neg_mask = (chunk[\"valuenum\"] <= 0) & (chunk[\"itemid\"] != 50802)\n",
    "    chunk.loc[neg_mask, \"valuenum\"] = np.nan\n",
    "\n",
    "    # map to short label\n",
    "    chunk[\"label\"] = chunk[\"itemid\"].map(ID2LBL)\n",
    "\n",
    "    # keep shortest representation of specimen (text “value” column)\n",
    "    chunk.loc[chunk[\"label\"] == \"specimen\", \"valuenum\"] = np.nan\n",
    "\n",
    "    rows.append(chunk[[\"stay_id\",\"charttime\",\"label\",\"value\",\"valuenum\"]])\n",
    "\n",
    "if not rows:\n",
    "    raise RuntimeError(\"No blood-gas rows found!\")\n",
    "\n",
    "df = pd.concat(rows, ignore_index=True)\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 5.  Pivot to wide format\n",
    "# ----------------------------------------------------------------------\n",
    "print(\"Pivoting …\")\n",
    "wide = (\n",
    "    df.pivot_table(index=[\"stay_id\",\"charttime\"],\n",
    "                   columns=\"label\",\n",
    "                   values=\"valuenum\",\n",
    "                   aggfunc=\"max\")         # duplicates same chart-time → max()\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "# specimen is textual → pull from original rows via first non-null value\n",
    "spec = (df[df[\"label\"]==\"specimen\"]\n",
    "        .drop_duplicates(subset=[\"stay_id\",\"charttime\"])\n",
    "        .set_index([\"stay_id\",\"charttime\"])[\"value\"])\n",
    "wide[\"specimen\"] = wide.set_index([\"stay_id\",\"charttime\"]).index.map(spec)\n",
    "\n",
    "# add subject_id / hadm_id\n",
    "wide = wide.merge(icu.reset_index()[[\"stay_id\",\"subject_id\",\"hadm_id\"]],\n",
    "                  on=\"stay_id\", how=\"left\")\n",
    "\n",
    "# column order: IDs, charttime, specimen, then every lab\n",
    "cols = [\"subject_id\",\"hadm_id\",\"stay_id\",\"charttime\",\"specimen\"]\n",
    "for lb in LABELS:\n",
    "    cols.append(lb)\n",
    "wide = wide.reindex(columns=cols)\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 6.  Save\n",
    "# ----------------------------------------------------------------------\n",
    "print(\"Writing CSV …\")\n",
    "wide.to_csv(OUT_F, index=False)\n",
    "print(f\"Wrote {len(wide):,} rows ➜ {OUT_F}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11834aed",
   "metadata": {},
   "source": [
    "first_day_bg_art.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5060defc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading chartevents …\n",
      "Loading first-day blood-gases …\n",
      "Merging SpO₂ …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cm/blwm27yd02l6xpvwrm9_l41m0000gn/T/ipykernel_36181/3513169847.py:113: UserWarning: Sortedness of columns cannot be checked when 'by' groups provided\n",
      "  bg_pl.join_asof(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5, 36)\n",
      "┌────────────┬──────────┬──────────┬────────────┬───┬────────────┬────────────┬────────────┬───────┐\n",
      "│ subject_id ┆ hadm_id  ┆ stay_id  ┆ charttime  ┆ … ┆ ventilator ┆ subject_id ┆ hadm_id_ri ┆ Spo2  │\n",
      "│ ---        ┆ ---      ┆ ---      ┆ ---        ┆   ┆ ---        ┆ _right     ┆ ght        ┆ ---   │\n",
      "│ i64        ┆ i64      ┆ i32      ┆ datetime[μ ┆   ┆ f64        ┆ ---        ┆ ---        ┆ f64   │\n",
      "│            ┆          ┆          ┆ s]         ┆   ┆            ┆ i64        ┆ i64        ┆       │\n",
      "╞════════════╪══════════╪══════════╪════════════╪═══╪════════════╪════════════╪════════════╪═══════╡\n",
      "│ 12466550   ┆ 23998182 ┆ 30000153 ┆ 2174-09-29 ┆ … ┆ null       ┆ 12466550   ┆ 23998182   ┆ 100.0 │\n",
      "│            ┆          ┆          ┆ 13:27:00   ┆   ┆            ┆            ┆            ┆       │\n",
      "│ 12466550   ┆ 23998182 ┆ 30000153 ┆ 2174-09-29 ┆ … ┆ null       ┆ 12466550   ┆ 23998182   ┆ 100.0 │\n",
      "│            ┆          ┆          ┆ 14:07:00   ┆   ┆            ┆            ┆            ┆       │\n",
      "│ 12466550   ┆ 23998182 ┆ 30000153 ┆ 2174-09-29 ┆ … ┆ null       ┆ 12466550   ┆ 23998182   ┆ 100.0 │\n",
      "│            ┆          ┆          ┆ 16:05:00   ┆   ┆            ┆            ┆            ┆       │\n",
      "│ 13180007   ┆ 27543152 ┆ 30000213 ┆ 2162-06-21 ┆ … ┆ null       ┆ 13180007   ┆ 27543152   ┆ 100.0 │\n",
      "│            ┆          ┆          ┆ 08:27:00   ┆   ┆            ┆            ┆            ┆       │\n",
      "│ 13180007   ┆ 27543152 ┆ 30000213 ┆ 2162-06-21 ┆ … ┆ null       ┆ 13180007   ┆ 27543152   ┆ 100.0 │\n",
      "│            ┆          ┆          ┆ 14:13:00   ┆   ┆            ┆            ┆            ┆       │\n",
      "└────────────┴──────────┴──────────┴────────────┴───┴────────────┴────────────┴────────────┴───────┘\n",
      "shape: (5, 37)\n",
      "┌────────────┬──────────┬──────────┬────────────┬───┬────────────┬────────────┬───────┬────────────┐\n",
      "│ subject_id ┆ hadm_id  ┆ stay_id  ┆ charttime  ┆ … ┆ subject_id ┆ hadm_id_ri ┆ Spo2  ┆ fio2_chart │\n",
      "│ ---        ┆ ---      ┆ ---      ┆ ---        ┆   ┆ _right     ┆ ght        ┆ ---   ┆ events     │\n",
      "│ i64        ┆ i64      ┆ i32      ┆ datetime[μ ┆   ┆ ---        ┆ ---        ┆ f64   ┆ ---        │\n",
      "│            ┆          ┆          ┆ s]         ┆   ┆ i64        ┆ i64        ┆       ┆ f32        │\n",
      "╞════════════╪══════════╪══════════╪════════════╪═══╪════════════╪════════════╪═══════╪════════════╡\n",
      "│ 12466550   ┆ 23998182 ┆ 30000153 ┆ 2174-09-29 ┆ … ┆ 12466550   ┆ 23998182   ┆ 100.0 ┆ 50.0       │\n",
      "│            ┆          ┆          ┆ 13:27:00   ┆   ┆            ┆            ┆       ┆            │\n",
      "│ 12466550   ┆ 23998182 ┆ 30000153 ┆ 2174-09-29 ┆ … ┆ 12466550   ┆ 23998182   ┆ 100.0 ┆ 50.0       │\n",
      "│            ┆          ┆          ┆ 14:07:00   ┆   ┆            ┆            ┆       ┆            │\n",
      "│ 12466550   ┆ 23998182 ┆ 30000153 ┆ 2174-09-29 ┆ … ┆ 12466550   ┆ 23998182   ┆ 100.0 ┆ 50.0       │\n",
      "│            ┆          ┆          ┆ 16:05:00   ┆   ┆            ┆            ┆       ┆            │\n",
      "│ 13180007   ┆ 27543152 ┆ 30000213 ┆ 2162-06-21 ┆ … ┆ 13180007   ┆ 27543152   ┆ 100.0 ┆ 50.0       │\n",
      "│            ┆          ┆          ┆ 08:27:00   ┆   ┆            ┆            ┆       ┆            │\n",
      "│ 13180007   ┆ 27543152 ┆ 30000213 ┆ 2162-06-21 ┆ … ┆ 13180007   ┆ 27543152   ┆ 100.0 ┆ 50.0       │\n",
      "│            ┆          ┆          ┆ 14:13:00   ┆   ┆            ┆            ┆       ┆            │\n",
      "└────────────┴──────────┴──────────┴────────────┴───┴────────────┴────────────┴───────┴────────────┘\n",
      "Done. 162,522 rows written to first_day_bg_art.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cm/blwm27yd02l6xpvwrm9_l41m0000gn/T/ipykernel_36181/3513169847.py:190: UserWarning: Sortedness of columns cannot be checked when 'by' groups provided\n",
      "  .join_asof(\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# extract_bg_oxygen.py\n",
    "# Build an arterial-blood-gas view with inferred specimen type (ART)\n",
    "# using MIMIC-IV (v2.2) CSVs.\n",
    "\n",
    "import pathlib, numpy as np, pandas as pd\n",
    "import polars as pl\n",
    "import pyarrow\n",
    "from math import exp\n",
    "\n",
    "DATA_DIR = pathlib.Path(\"./data\")\n",
    "ICU_DIR  = DATA_DIR / \"icu\"\n",
    "\n",
    "# ───────────────────────────────\n",
    "# 1. Load the raw files we need\n",
    "# ───────────────────────────────\n",
    "print(\"Loading chartevents …\")\n",
    "ce = pd.read_csv(\n",
    "    ICU_DIR / \"chartevents.csv\",\n",
    "    usecols=[\n",
    "        \"subject_id\", \"hadm_id\", \"stay_id\",\n",
    "        \"charttime\", \"itemid\", \"valuenum\"\n",
    "    ],\n",
    "    dtype={\n",
    "        \"subject_id\": \"int32\",\n",
    "        \"hadm_id\":    \"Int32\",\n",
    "        \"stay_id\":    \"int32\",\n",
    "        \"itemid\":     \"int32\",\n",
    "        \"valuenum\":   \"float32\",\n",
    "    },\n",
    "    parse_dates=[\"charttime\"]\n",
    ")\n",
    "\n",
    "print(\"Loading first-day blood-gases …\")\n",
    "bg = pd.read_csv(\n",
    "    ICU_DIR / \"first_day_blood_gas.csv\",\n",
    "    parse_dates=[\"charttime\"]\n",
    ")\n",
    "# stay_id in MIMIC-IV == icustay_id in the original query\n",
    "bg[\"stay_id\"] = bg[\"stay_id\"].astype(\"int32\")\n",
    "# ───────────────────────────────\n",
    "# 2. Stage 1 ─ SpO₂\n",
    "# ───────────────────────────────\n",
    "SPO2_ITEMS = [646, 220277]   # same codes in MIMIC-IV\n",
    "\n",
    "spo2 = (\n",
    "    ce.loc[ce.itemid.isin(SPO2_ITEMS)]\n",
    "      .assign(valuenum=lambda d: d.valuenum.where(\n",
    "          (d.valuenum > 0) & (d.valuenum <= 100)\n",
    "      ))\n",
    "      .groupby([\"subject_id\", \"hadm_id\", \"stay_id\", \"charttime\"], as_index=False)\n",
    "      .agg(SpO2=(\"valuenum\", \"max\"))\n",
    ")\n",
    "\n",
    "# ───────────────────────────────\n",
    "# 3. Stage 1 ─ FiO₂\n",
    "# ───────────────────────────────\n",
    "FIO2_ITEMS = [3420, 3422, 190, 223835]\n",
    "\n",
    "def _clean_fio2(row):\n",
    "    v = row.valuenum\n",
    "    if row.itemid == 223835:              # Inspired O₂ Fraction\n",
    "        if 0 < v <= 1:   return v * 100\n",
    "        if 21 <= v <= 100: return v\n",
    "    elif row.itemid in (3420, 3422):      # well-formatted %\n",
    "        return v\n",
    "    elif row.itemid == 190:               # 0-1 range\n",
    "        if 0.20 < v < 1: return v * 100\n",
    "    return np.nan\n",
    "\n",
    "fio2_raw = ce.loc[ce.itemid.isin(FIO2_ITEMS)].copy()\n",
    "fio2_raw[\"fio2_clean\"] = fio2_raw.apply(_clean_fio2, axis=1)\n",
    "\n",
    "fio2 = (\n",
    "    fio2_raw\n",
    "      .groupby([\"subject_id\", \"hadm_id\", \"stay_id\", \"charttime\"], as_index=False)\n",
    "      .agg(fio2_chartevents=(\"fio2_clean\", \"max\"))\n",
    ")\n",
    "\n",
    "# ───────────────────────────────\n",
    "# 4. Attach the most recent SpO₂ ≤ 2 h\n",
    "# ───────────────────────────────\n",
    "print(\"Merging SpO₂ …\")\n",
    "\n",
    "bg_pl = pl.from_pandas(bg)\n",
    "spo2_pl = (\n",
    "    pl.read_csv(\n",
    "        \"./data/icu/chartevents.csv\",\n",
    "        columns=[\"subject_id\",\"hadm_id\",\"stay_id\",\"charttime\",\"itemid\",\"valuenum\"],\n",
    "        try_parse_dates=True\n",
    "    )\n",
    "    .filter(pl.col(\"itemid\").is_in([646,220277]) &\n",
    "            (pl.col(\"valuenum\") > 0) & (pl.col(\"valuenum\") <= 100))\n",
    "    .select([\n",
    "        \"subject_id\",\"hadm_id\",\"stay_id\",\n",
    "        pl.col(\"charttime\").alias(\"charttime\"),\n",
    "        pl.col(\"valuenum\").alias(\"Spo2\")\n",
    "    ])\n",
    ")\n",
    "\n",
    "bg_pl   = bg_pl.with_columns(pl.col(\"stay_id\").cast(pl.Int32))\n",
    "spo2_pl = spo2_pl.with_columns(pl.col(\"stay_id\").cast(pl.Int32))\n",
    "\n",
    "TIME_UNIT = \"us\"  # polars default is nanoseconds, we want microseconds \n",
    "bg_pl   = bg_pl.with_columns(\n",
    "             pl.col(\"charttime\").dt.cast_time_unit(TIME_UNIT)\n",
    "         )\n",
    "spo2_pl = spo2_pl.with_columns(\n",
    "             pl.col(\"charttime\").dt.cast_time_unit(TIME_UNIT)\n",
    "         )\n",
    "\n",
    "result_pl = (\n",
    "    bg_pl.join_asof(\n",
    "        spo2_pl,\n",
    "        on=\"charttime\",\n",
    "        by=\"stay_id\",\n",
    "        strategy=\"backward\",\n",
    "        tolerance=\"2h\"))\n",
    "\n",
    "print(result_pl.head())\n",
    "# ───────────────────────────────\n",
    "# 5. Attach the most recent FiO₂ ≤ 4 h\n",
    "# ───────────────────────────────\n",
    "# ── Build FiO₂ table and attach it (≤ 4 h backward) ───────────────\n",
    "FIO2_ITEMS = [3420, 3422, 190, 223835]\n",
    "\n",
    "fio2_pl = (\n",
    "    pl.read_csv(\n",
    "        \"./data/icu/chartevents.csv\",\n",
    "        columns=[\"stay_id\", \"charttime\", \"itemid\", \"valuenum\"],\n",
    "        try_parse_dates=True,\n",
    "    )\n",
    "    # keep only FiO₂ rows\n",
    "    .filter(pl.col(\"itemid\").is_in(FIO2_ITEMS))\n",
    "    # apply the cleansing rules -> numeric %\n",
    "    .with_columns(\n",
    "        (\n",
    "            pl.when(pl.col(\"itemid\") == 223835)\n",
    "              .then(\n",
    "                  pl.when((pl.col(\"valuenum\") > 0) & (pl.col(\"valuenum\") <= 1))\n",
    "                    .then(pl.col(\"valuenum\") * 100)\n",
    "                  .when((pl.col(\"valuenum\") >= 21) & (pl.col(\"valuenum\") <= 100))\n",
    "                    .then(pl.col(\"valuenum\"))\n",
    "                  .otherwise(None)\n",
    "              )\n",
    "            .when(pl.col(\"itemid\").is_in([3420, 3422]))\n",
    "              .then(pl.col(\"valuenum\"))\n",
    "            .when(\n",
    "                (pl.col(\"itemid\") == 190)\n",
    "                & (pl.col(\"valuenum\") > 0.20)\n",
    "                & (pl.col(\"valuenum\") < 1)\n",
    "            )\n",
    "              .then(pl.col(\"valuenum\") * 100)\n",
    "            .otherwise(None)\n",
    "            .cast(pl.Float32)\n",
    "            .alias(\"fio2_ce\")\n",
    "        )\n",
    "    )\n",
    "    # one FiO₂ per stay_id-charttime → take the max\n",
    "    .group_by([\"stay_id\", \"charttime\"])\n",
    "    .agg(pl.col(\"fio2_ce\").max())\n",
    "    # keep only join keys + value\n",
    "    .select([\"stay_id\", \"charttime\", \"fio2_ce\"])\n",
    "    # match dtypes with result_pl\n",
    "    .with_columns(\n",
    "        [\n",
    "            pl.col(\"stay_id\").cast(pl.Int32),\n",
    "            pl.col(\"charttime\").dt.cast_time_unit(\"us\"),\n",
    "        ]\n",
    "    )\n",
    "    .drop_nulls([\"stay_id\", \"charttime\"])\n",
    "    .sort([\"stay_id\", \"charttime\"])        # required for join_asof\n",
    ")\n",
    "\n",
    "# ensure left frame keys are sorted & typed\n",
    "result_pl = (\n",
    "    result_pl\n",
    "    .with_columns(\n",
    "        [\n",
    "            pl.col(\"stay_id\").cast(pl.Int32),\n",
    "            pl.col(\"charttime\").dt.cast_time_unit(\"us\"),\n",
    "        ]\n",
    "    )\n",
    "    .sort([\"stay_id\", \"charttime\"])\n",
    ")\n",
    "\n",
    "# as-of join: latest FiO₂ ≤ 4 h before the blood gas\n",
    "result_pl = (\n",
    "    result_pl\n",
    "    .join_asof(\n",
    "        fio2_pl,\n",
    "        by=\"stay_id\",\n",
    "        on=\"charttime\",\n",
    "        strategy=\"backward\",\n",
    "        tolerance=\"4h\",\n",
    "    )\n",
    "    .rename({\"fio2_ce\": \"fio2_chartevents\"})\n",
    ")\n",
    "\n",
    "print(result_pl.head())\n",
    "# ───────────────────────────────\n",
    "# 6. Compute SPECIMEN_PROB\n",
    "# ───────────────────────────────\n",
    "coef  = {\n",
    "    \"intercept\": -0.02544,\n",
    "    \"po2\":        0.04598,\n",
    "    \"spo2\":      -0.15356,\n",
    "    \"fio2_ce\":    0.00621,\n",
    "    \"hemoglobin\": 0.10559,\n",
    "    \"so2\":        0.13251,\n",
    "    \"pco2\":      -0.01511,\n",
    "    \"fio2\":       0.01480,\n",
    "    \"aado2\":     -0.00200,\n",
    "    \"bicarbonate\":-0.03220,\n",
    "    \"totalco2\":   0.05384,\n",
    "    \"lactate\":    0.08202,\n",
    "    \"ph\":         0.10956,\n",
    "    \"o2flow\":     0.00848,\n",
    "}\n",
    "mean  = {\n",
    "    \"spo2\":        97.49420,\n",
    "    \"fio2_ce\":     51.49550,\n",
    "    \"hemoglobin\":  10.32307,\n",
    "    \"so2\":         93.66539,\n",
    "    \"pco2\":        42.08866,\n",
    "    \"fio2\":        63.97836,\n",
    "    \"aado2\":      442.21186,\n",
    "    \"bicarbonate\": 22.96894,\n",
    "    \"totalco2\":    24.72632,\n",
    "    \"lactate\":      3.06436,\n",
    "    \"ph\":           7.36233,\n",
    "    \"o2flow\":       7.59362,\n",
    "}\n",
    "\n",
    "def logistic_expr(expr: pl.Expr) -> pl.Expr:\n",
    "    \"\"\"Polars-friendly logistic.\"\"\"\n",
    "    return 1 / (1 + (-expr).exp())\n",
    "\n",
    "result_pl = (\n",
    "    result_pl\n",
    "    .with_columns([\n",
    "        # replace NaNs with hard-coded means\n",
    "        pl.col(\"Spo2\").fill_null(mean[\"spo2\"]).alias(\"_spo2\"),\n",
    "        pl.col(\"fio2_chartevents\").fill_null(mean[\"fio2_ce\"]).alias(\"_fio2_ce\"),\n",
    "        pl.col(\"hemoglobin\").fill_null(mean[\"hemoglobin\"]).alias(\"_hgb\"),\n",
    "        pl.col(\"so2\").fill_null(mean[\"so2\"]).alias(\"_so2\"),\n",
    "        pl.col(\"pco2\").fill_null(mean[\"pco2\"]).alias(\"_pco2\"),\n",
    "        pl.col(\"fio2\").fill_null(mean[\"fio2\"]).alias(\"_fio2\"),\n",
    "        pl.col(\"aado2\").fill_null(mean[\"aado2\"]).alias(\"_aado2\"),\n",
    "        pl.col(\"bicarbonate\").fill_null(mean[\"bicarbonate\"]).alias(\"_bicarb\"),\n",
    "        pl.col(\"totalco2\").fill_null(mean[\"totalco2\"]).alias(\"_tco2\"),\n",
    "        pl.col(\"lactate\").fill_null(mean[\"lactate\"]).alias(\"_lact\"),\n",
    "        pl.col(\"ph\").fill_null(mean[\"ph\"]).alias(\"_ph\"),\n",
    "        pl.col(\"o2flow\").fill_null(mean[\"o2flow\"]).alias(\"_o2flow\"),\n",
    "    ])\n",
    "    .with_columns([\n",
    "        (\n",
    "            coef[\"intercept\"]\n",
    "            + coef[\"po2\"]        * pl.col(\"po2\")\n",
    "            + coef[\"spo2\"]       * pl.col(\"_spo2\")          + 0.13429\n",
    "            + coef[\"fio2_ce\"]    * pl.col(\"_fio2_ce\")       - 0.24958\n",
    "            + coef[\"hemoglobin\"] * pl.col(\"_hgb\")           + 0.05954\n",
    "            + coef[\"so2\"]        * pl.col(\"_so2\")           - 0.23172\n",
    "            + coef[\"pco2\"]       * pl.col(\"_pco2\")          - 0.01630\n",
    "            + coef[\"fio2\"]       * pl.col(\"_fio2\")          - 0.31142\n",
    "            + coef[\"aado2\"]      * pl.col(\"_aado2\")         - 0.01328\n",
    "            + coef[\"bicarbonate\"]* pl.col(\"_bicarb\")        - 0.06535\n",
    "            + coef[\"totalco2\"]   * pl.col(\"_tco2\")          - 0.01405\n",
    "            + coef[\"lactate\"]    * pl.col(\"_lact\")          + 0.06038\n",
    "            + coef[\"ph\"]         * pl.col(\"_ph\")            - 0.00617\n",
    "            + coef[\"o2flow\"]     * pl.col(\"_o2flow\")        - 0.35803\n",
    "        ).alias(\"_z\")\n",
    "    ])\n",
    "    .with_columns([\n",
    "        logistic_expr(pl.col(\"_z\")).alias(\"SPECIMEN_PROB\")\n",
    "    ])\n",
    ")\n",
    "\n",
    "# ───────────────────────────────\n",
    "# 7. Derived metrics\n",
    "# ───────────────────────────────\n",
    "result_pl = (\n",
    "    result_pl\n",
    "    .with_columns([\n",
    "        # A–a DO2\n",
    "        pl.when(\n",
    "            pl.all_horizontal(\n",
    "                pl.col(\"po2\").is_not_null(),\n",
    "                pl.col(\"pco2\").is_not_null(),\n",
    "                (pl.col(\"fio2\").is_not_null() | pl.col(\"fio2_chartevents\").is_not_null())\n",
    "            )\n",
    "        ).then(\n",
    "            (pl.coalesce([pl.col(\"fio2\"), pl.col(\"fio2_chartevents\")]) / 100)\n",
    "            * (760 - 47)\n",
    "            - (pl.col(\"pco2\") / 0.8)\n",
    "            - pl.col(\"po2\")\n",
    "        ).otherwise(None).alias(\"AADO2_calc\"),\n",
    "\n",
    "        # PaO2 / FiO2\n",
    "        pl.when(\n",
    "            pl.col(\"po2\").is_not_null() &\n",
    "            (pl.col(\"fio2\").is_not_null() | pl.col(\"fio2_chartevents\").is_not_null())\n",
    "        ).then(\n",
    "            100 * pl.col(\"po2\") /\n",
    "            pl.coalesce([pl.col(\"fio2\"), pl.col(\"fio2_chartevents\")])\n",
    "        ).otherwise(None).alias(\"PaO2FiO2\")\n",
    "    ])\n",
    ")\n",
    "\n",
    "# ───────────────────────────────\n",
    "# 8. Keep arterial samples only\n",
    "# ───────────────────────────────\n",
    "result_pl = result_pl.with_columns(\n",
    "    pl.col(\"specimen\").cast(pl.Utf8)\n",
    ")\n",
    "result_pl = (\n",
    "    result_pl\n",
    "    .filter(\n",
    "        (pl.col(\"specimen\") == \"ART\") |\n",
    "        (pl.col(\"SPECIMEN_PROB\") > 0.75)\n",
    "    )\n",
    "    .sort([\"stay_id\", \"charttime\"])\n",
    ")\n",
    "\n",
    "# ───────────────────────────────\n",
    "# 9. Save\n",
    "# ───────────────────────────────\n",
    "result_pl.write_csv(\"first_day_bg_art.csv\")\n",
    "print(f\"Done. {result_pl.height:,} rows written to first_day_bg_art.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9672572e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fio2_pl schema: Schema([('stay_id', Int32), ('charttime', Datetime(time_unit='us', time_zone=None)), ('fio2_ce', Float32)])\n",
      "result_pl schema: Schema([('subject_id', Int64), ('hadm_id', Int64), ('stay_id', Int32), ('charttime', Datetime(time_unit='us', time_zone=None)), ('specimen', Float64), ('specimen.1', Float64), ('aado2', Float64), ('baseexcess', Float64), ('bicarbonate', Float64), ('totalco2', Float64), ('carboxyhemoglobin', Float64), ('chloride', Float64), ('calcium', Float64), ('glucose', Float64), ('hematocrit', Float64), ('hemoglobin', Float64), ('intubated', Float64), ('lactate', Float64), ('methemoglobin', Float64), ('o2flow', Float64), ('fio2', Float64), ('so2', Float64), ('pco2', Float64), ('peep', Float64), ('ph', Float64), ('po2', Float64), ('potassium', Float64), ('requiredo2', Float64), ('sodium', Float64), ('temperature', Float64), ('tidalvolume', Float64), ('ventilationrate', Float64), ('ventilator', Float64), ('subject_id_right', Int64), ('hadm_id_right', Int64), ('Spo2', Float64)])\n"
     ]
    }
   ],
   "source": [
    "print(\"fio2_pl schema:\", fio2_pl.schema)   # OR\n",
    "\n",
    "print(\"result_pl schema:\", result_pl.schema)  # OR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7af67bc",
   "metadata": {},
   "source": [
    "ventilator_setting.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "591265e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done – 1,010,169 rows → ventilator_setting.csv\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# build_ventilator_setting.py\n",
    "# ---------------------------------------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# location of the raw CHARTEVENTS CSV\n",
    "CE_CSV   = Path(\"./data/icu/chartevents.csv\")   # change if needed\n",
    "OUT_FILE = \"ventilator_setting.csv\"\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 1 ▸ load only the itemids we care about\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "ITEMS = [\n",
    "    224688, 224689, 224690, 224687, 224685, 224684, 224686,\n",
    "    224696, 220339, 224700, 223835, 223849, 229314, 223848, 224691\n",
    "]\n",
    "\n",
    "usecols = [\"subject_id\",\"stay_id\",\"charttime\",\"itemid\",\n",
    "           \"value\",\"valuenum\",\"valueuom\",\"storetime\"]\n",
    "\n",
    "ce = pd.read_csv(\n",
    "        CE_CSV,\n",
    "        usecols = usecols,\n",
    "        parse_dates = [\"charttime\",\"storetime\"]\n",
    ")\n",
    "# basic filters: non-null value and stay_id, item in list\n",
    "ce = ce[\n",
    "    ce[\"itemid\"].isin(ITEMS) &\n",
    "    ce[\"value\"].notna() &\n",
    "    ce[\"stay_id\"].notna()\n",
    "].copy()\n",
    "\n",
    "# unify dtypes\n",
    "ce = ce.astype({\"stay_id\":\"Int32\",\"itemid\":\"int64\"})\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 2 ▸ clean VALUENUM per SQL rules\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "def clean_valuenum(row):\n",
    "    iid, v = row.itemid, row.valuenum\n",
    "    if iid == 223835:                         # FiO₂\n",
    "        if 0.20 <= v <= 1:  return v*100          # fraction → %\n",
    "        if 1 < v < 20:     return np.nan          # junk (flow rate)\n",
    "        if 20 <= v <=100:  return v               # already %\n",
    "        return np.nan\n",
    "    if iid in (220339,224700):                # PEEP, keep 0-100\n",
    "        if 0 <= v <= 100: return v\n",
    "        return np.nan\n",
    "    return v                                   # others unchanged\n",
    "\n",
    "ce[\"valuenum\"] = ce.apply(clean_valuenum, axis=1)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 3 ▸ keep the *latest* row per (subject_id, charttime, itemid)\n",
    "#     (mimics ROW_NUMBER ORDER BY storetime DESC rn=1)\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "ce.sort_values(\"storetime\", ascending=False, inplace=True)\n",
    "ce = ce.drop_duplicates(\n",
    "        subset=[\"subject_id\",\"charttime\",\"itemid\"],\n",
    "        keep=\"first\"\n",
    ")\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 4 ▸ aggregate to wide format\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "mapping_valnum = {\n",
    "    224688: \"respiratory_rate_set\",\n",
    "    224690: \"respiratory_rate_total\",\n",
    "    224689: \"respiratory_rate_spontaneous\",\n",
    "    224687: \"minute_volume\",\n",
    "    224684: \"tidal_volume_set\",\n",
    "    224685: \"tidal_volume_observed\",\n",
    "    224686: \"tidal_volume_spontaneous\",\n",
    "    224696: \"plateau_pressure\",\n",
    "    220339: \"peep\",                # one of two itemids\n",
    "    224700: \"peep\",\n",
    "    223835: \"fio2\",\n",
    "    224691: \"flow_rate\",\n",
    "}\n",
    "mapping_value  = {\n",
    "    223849: \"ventilator_mode\",\n",
    "    229314: \"ventilator_mode_hamilton\",\n",
    "    223848: \"ventilator_type\",\n",
    "}\n",
    "\n",
    "# split into numeric & string frames\n",
    "num_df = ce[ce[\"itemid\"].isin(mapping_valnum)].copy()\n",
    "str_df = ce[ce[\"itemid\"].isin(mapping_value)].copy()\n",
    "\n",
    "num_df[\"column\"] = num_df[\"itemid\"].map(mapping_valnum)\n",
    "str_df[\"column\"] = str_df[\"itemid\"].map(mapping_value)\n",
    "\n",
    "# pivot numeric\n",
    "num_wide = (\n",
    "    num_df.pivot_table(\n",
    "        index=[\"subject_id\",\"charttime\"],\n",
    "        columns=\"column\",\n",
    "        values=\"valuenum\",\n",
    "        aggfunc=\"max\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# pivot string\n",
    "str_wide = (\n",
    "    str_df.pivot_table(\n",
    "        index=[\"subject_id\",\"charttime\"],\n",
    "        columns=\"column\",\n",
    "        values=\"value\",\n",
    "        aggfunc=\"max\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# combine & add stay_id = max(stay_id)\n",
    "stay = ce.groupby([\"subject_id\",\"charttime\"])[\"stay_id\"].max()\n",
    "\n",
    "wide = (\n",
    "    pd.concat([stay, num_wide, str_wide], axis=1)\n",
    "      .reset_index()\n",
    "      .astype({\"stay_id\":\"Int32\"})\n",
    ")\n",
    "\n",
    "# reorder columns roughly as SQL output\n",
    "col_order = [\"subject_id\",\"stay_id\",\"charttime\",\n",
    "             \"respiratory_rate_set\",\"respiratory_rate_total\",\n",
    "             \"respiratory_rate_spontaneous\",\"minute_volume\",\n",
    "             \"tidal_volume_set\",\"tidal_volume_observed\",\n",
    "             \"tidal_volume_spontaneous\",\"plateau_pressure\",\n",
    "             \"peep\",\"fio2\",\"flow_rate\",\n",
    "             \"ventilator_mode\",\"ventilator_mode_hamilton\",\n",
    "             \"ventilator_type\"]\n",
    "wide = wide.reindex(columns=col_order)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 5 ▸ save\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "wide.sort_values([\"subject_id\",\"charttime\"]).to_csv(OUT_FILE, index=False)\n",
    "print(f\"Done – {len(wide):,} rows → {OUT_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c537901",
   "metadata": {},
   "source": [
    "oxygen_delivery.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e40bf1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done – 1,503,989 rows → oxygen_delivery.csv\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# build_oxygen_delivery.py  –  recreate mimiciv_derived.oxygen_delivery\n",
    "# --------------------------------------------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "ICU_PATH = Path(\"./data/icu/chartevents.csv\")   # adjust if needed\n",
    "OUTFILE  = \"oxygen_delivery.csv\"\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 1 ▸ load only the itemids we need\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "ITEMS_FLOW  = [223834, 227582, 227287]\n",
    "ITEM_DEVICE = 226732\n",
    "\n",
    "usecols = [\"subject_id\",\"stay_id\",\"charttime\",\"itemid\",\n",
    "           \"value\",\"valuenum\",\"valueuom\",\"storetime\"]\n",
    "\n",
    "ce = pd.read_csv(ICU_PATH, usecols=usecols, parse_dates=[\"charttime\",\"storetime\"])\n",
    "ce = ce.astype({\"stay_id\":\"Int32\",\"itemid\":\"int64\"})\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 2 ▸ ce_stg2  – latest FLOW rows per (subject,charttime,itemid)\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "flows = ce[ce[\"itemid\"].isin(ITEMS_FLOW) & ce[\"value\"].notna()].copy()\n",
    "\n",
    "# map 227582 → 223834\n",
    "flows.loc[flows[\"itemid\"] == 227582, \"itemid\"] = 223834\n",
    "\n",
    "flows.sort_values(\"storetime\", ascending=False, inplace=True)\n",
    "flows = flows.drop_duplicates(subset=[\"subject_id\",\"charttime\",\"itemid\"], keep=\"first\")\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 3 ▸ o2  – device rows with row-number per charttime\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "o2 = ce[ce[\"itemid\"] == ITEM_DEVICE][[\"subject_id\",\"stay_id\",\"charttime\",\"value\"]].copy()\n",
    "o2.rename(columns={\"value\":\"o2_device\"}, inplace=True)\n",
    "\n",
    "# rank devices (NULL first, then alphabetic)\n",
    "o2[\"rank_key\"] = o2[\"o2_device\"].isna()\n",
    "o2.sort_values([\"subject_id\",\"charttime\",\"rank_key\",\"o2_device\"], inplace=True)\n",
    "o2[\"rn\"] = (\n",
    "    o2.groupby([\"subject_id\",\"charttime\"]).cumcount() + 1\n",
    ")                                              # 1,2,3,4 …\n",
    "\n",
    "o2.drop(columns=\"rank_key\", inplace=True)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 4 ▸ merge & aggregate to final view\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "#   join on (subject_id, charttime)  ≈ FULL OUTER\n",
    "merged = flows.merge(\n",
    "    o2, on=[\"subject_id\",\"charttime\"], how=\"outer\", suffixes=(\"\",\"_o2\")\n",
    ")\n",
    "\n",
    "# prefer stay_id from flows, else from o2\n",
    "merged[\"stay_id\"] = merged[\"stay_id\"].fillna(merged[\"stay_id_o2\"]).astype(\"Int32\")\n",
    "merged[\"rn\"]      = merged[\"rn\"].fillna(0).astype(int)\n",
    "\n",
    "def agg_block(df):\n",
    "    out = {\n",
    "        \"stay_id\" : df[\"stay_id\"].max(),\n",
    "        \"o2_flow\" : df.loc[df[\"itemid\"]==223834, \"valuenum\"].max(),\n",
    "        \"o2_flow_additional\" : df.loc[df[\"itemid\"]==227287, \"valuenum\"].max(),\n",
    "    }\n",
    "    # devices by rank (1..4)\n",
    "    for k in range(1,5):\n",
    "        col = f\"o2_delivery_device_{k}\"\n",
    "        out[col] = df.loc[df[\"rn\"]==k, \"o2_device\"].max()\n",
    "    return pd.Series(out)\n",
    "\n",
    "final = (\n",
    "    merged.groupby([\"subject_id\",\"charttime\"], as_index=False)\n",
    "          .apply(agg_block)\n",
    ")\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 5 ▸ save\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "final.to_csv(OUTFILE, index=False)\n",
    "print(f\"Done – {len(final):,} rows → {OUTFILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c52629a",
   "metadata": {},
   "source": [
    "ventilation.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06b51487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done – 143,420 ventilation intervals saved to ventilation.csv\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# build_ventilation.py – derive ventilation durations from MIMIC-IV CSV\n",
    "# ---------------------------------------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "DERIVED = Path(\"./data/icu\")          # adjust if needed\n",
    "\n",
    "# ─────────────────────────────────────────\n",
    "# 1 ▸ load the two source views\n",
    "# ─────────────────────────────────────────\n",
    "vs  = pd.read_csv(\n",
    "        DERIVED / \"ventilator_setting.csv\",\n",
    "        usecols=[\"stay_id\",\"charttime\",\n",
    "                 \"ventilator_mode\",\"ventilator_mode_hamilton\"],\n",
    "        parse_dates=[\"charttime\"]\n",
    ")\n",
    "od  = pd.read_csv(\n",
    "        DERIVED / \"oxygen_delivery.csv\",\n",
    "        usecols=[\"stay_id\",\"charttime\",\"o2_delivery_device_1\"],\n",
    "        parse_dates=[\"charttime\"]\n",
    ")\n",
    "\n",
    "# make sure types match\n",
    "vs[\"stay_id\"] = od[\"stay_id\"] = vs[\"stay_id\"].astype(\"int32\")\n",
    "\n",
    "# ─────────────────────────────────────────\n",
    "# 2 ▸ tm  = union of timestamps\n",
    "# ─────────────────────────────────────────\n",
    "tm = (\n",
    "    pd.concat([\n",
    "        vs[[\"stay_id\",\"charttime\"]],\n",
    "        od[[\"stay_id\",\"charttime\"]]\n",
    "    ], ignore_index=True)\n",
    "    .drop_duplicates()\n",
    ")\n",
    "\n",
    "# ─────────────────────────────────────────\n",
    "# 3 ▸ join rows back to full context\n",
    "# ─────────────────────────────────────────\n",
    "full = (\n",
    "    tm\n",
    "    .merge(vs, how=\"left\", on=[\"stay_id\",\"charttime\"])\n",
    "    .merge(od, how=\"left\", on=[\"stay_id\",\"charttime\"])\n",
    ")\n",
    "\n",
    "# ─────────────────────────────────────────\n",
    "# 4 ▸ classify ventilation_status\n",
    "# ─────────────────────────────────────────\n",
    "trach  = {\"Tracheostomy tube\",\"Trach mask \"}\n",
    "inv_o2 = {\"Endotracheal tube\"}\n",
    "inv_vm = {'(S) CMV','APRV','APRV/Biphasic+ApnPress','APRV/Biphasic+ApnVol',\n",
    "          'APV (cmv)','Ambient','Apnea Ventilation','CMV','CMV/ASSIST',\n",
    "          'CMV/ASSIST/AutoFlow','CMV/AutoFlow','CPAP/PPS','CPAP/PSV',\n",
    "          'CPAP/PSV+Apn TCPL','CPAP/PSV+ApnPres','CPAP/PSV+ApnVol','MMV',\n",
    "          'MMV/AutoFlow','MMV/PSV','MMV/PSV/AutoFlow','P-CMV','PCV+',\n",
    "          'PCV+/PSV','PCV+Assist','PRES/AC','PRVC/AC','PRVC/SIMV','PSV/SBT',\n",
    "          'SIMV','SIMV/AutoFlow','SIMV/PRES','SIMV/PSV','SIMV/PSV/AutoFlow',\n",
    "          'SIMV/VOL','SYNCHRON MASTER','SYNCHRON SLAVE','VOL/AC'}\n",
    "inv_ham = {'APRV','APV (cmv)','Ambient','(S) CMV','P-CMV','SIMV','APV (simv)',\n",
    "           'P-SIMV','VS','ASV'}\n",
    "niv_o2  = {'Bipap mask ','CPAP mask '}\n",
    "niv_ham = {'DuoPaP','NIV','NIV-ST'}\n",
    "hfnc    = {'High flow nasal cannula'}\n",
    "supp_o2 = {'Non-rebreather','Face tent','Aerosol-cool','Venti mask ',\n",
    "           'Medium conc mask ','Ultrasonic neb','Vapomist','Oxymizer',\n",
    "           'High flow neb','Nasal cannula'}\n",
    "\n",
    "def classify(row):\n",
    "    dev = row.o2_delivery_device_1\n",
    "    vm  = row.ventilator_mode\n",
    "    vh  = row.ventilator_mode_hamilton\n",
    "    if dev in trach:  return \"Tracheostomy\"\n",
    "    if (dev in inv_o2) or (vm in inv_vm) or (vh in inv_ham): return \"InvasiveVent\"\n",
    "    if (dev in niv_o2) or (vh in niv_ham):                   return \"NonInvasiveVent\"\n",
    "    if dev in hfnc:     return \"HFNC\"\n",
    "    if dev in supp_o2:  return \"SupplementalOxygen\"\n",
    "    if dev == \"None\":   return \"None\"\n",
    "    return np.nan\n",
    "\n",
    "full[\"ventilation_status\"] = full.apply(classify, axis=1)\n",
    "full = full.dropna(subset=[\"ventilation_status\"])\n",
    "\n",
    "# ─────────────────────────────────────────\n",
    "# 5 ▸ window calculations per stay\n",
    "# ─────────────────────────────────────────\n",
    "full = full.sort_values([\"stay_id\",\"charttime\"])\n",
    "grp  = full.groupby(\"stay_id\")\n",
    "\n",
    "full[\"charttime_lag\"]           = grp[\"charttime\"].shift(1)\n",
    "full[\"charttime_lead\"]          = grp[\"charttime\"].shift(-1)\n",
    "full[\"ventilation_status_lag\"]  = grp[\"ventilation_status\"].shift(1)\n",
    "\n",
    "# gap (hours) between current row and previous row\n",
    "full[\"gap_prev_hr\"] = (\n",
    "    (full[\"charttime\"] - full[\"charttime_lag\"])\n",
    "    .dt.total_seconds()/3600\n",
    ")\n",
    "\n",
    "# new ventilation event?\n",
    "full[\"new_event\"] = (\n",
    "    full[\"ventilation_status_lag\"].isna()\n",
    "    | (full[\"gap_prev_hr\"] >= 14)\n",
    "    | (full[\"ventilation_status_lag\"] != full[\"ventilation_status\"])\n",
    ").astype(int)\n",
    "\n",
    "# sequence id within stay\n",
    "full[\"vent_seq\"] = grp[\"new_event\"].cumsum()\n",
    "\n",
    "# end-time candidate for each row\n",
    "gap_next_hr = (\n",
    "    (full[\"charttime_lead\"] - full[\"charttime\"])\n",
    "    .dt.total_seconds()/3600\n",
    ")\n",
    "full[\"end_candidate\"] = np.where(\n",
    "    full[\"charttime_lead\"].isna() | (gap_next_hr >= 14),\n",
    "    full[\"charttime\"],\n",
    "    full[\"charttime_lead\"]\n",
    ")\n",
    "\n",
    "# ─────────────────────────────────────────\n",
    "# 6 ▸ aggregate to intervals\n",
    "# ─────────────────────────────────────────\n",
    "out = (\n",
    "    full.groupby([\"stay_id\",\"vent_seq\"], as_index=False)\n",
    "        .agg(\n",
    "            starttime = (\"charttime\", \"min\"),\n",
    "            endtime   = (\"end_candidate\",\"max\"),\n",
    "            ventilation_status = (\"ventilation_status\",\"last\"),\n",
    "        )\n",
    ")\n",
    "\n",
    "# discard single-timestamp intervals\n",
    "out = out[out[\"starttime\"] != out[\"endtime\"]]\n",
    "\n",
    "# save\n",
    "out.to_csv(\"ventilation.csv\", index=False)\n",
    "print(f\"Done – {len(out):,} ventilation intervals saved to ventilation.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c7705e",
   "metadata": {},
   "source": [
    "ventdurations.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5f027e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading icustays …\n",
      "Scanning ventilation table …\n",
      "Building ventdurations.csv …\n",
      "Wrote 73,141 rows ➜ data/icu/ventdurations.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "----------------\n",
    "Create ventdurations.csv from local MIMIC-IV files.\n",
    "\n",
    "Input  (./data/)\n",
    "    • icu/icustays.csv[.gz]          – stay_id, subject_id, hadm_id, intime\n",
    "    • icu/ventilation.csv[.gz]       – start/end times of ventilation episodes\n",
    "Output\n",
    "    • icu/ventdurations.csv          – subject_id, hadm_id, stay_id, vent\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 1.  Recursive loader\n",
    "# ----------------------------------------------------------------------\n",
    "ROOT = Path(\"./data\")                \n",
    "\n",
    "def load(name: str, *, columns=None, parse_dates=None) -> pd.DataFrame:\n",
    "    \"\"\"Return DataFrame for the first <name>.csv(.gz) found under ROOT.\"\"\"\n",
    "    files = list(ROOT.rglob(f\"{name}.csv\"))\n",
    "    return pd.read_csv(files[0],\n",
    "                       usecols=columns,\n",
    "                       parse_dates=parse_dates,\n",
    "                       low_memory=False)\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 2.  ICU stays (gives us `intime`)\n",
    "# ----------------------------------------------------------------------\n",
    "print(\"Loading icustays …\")\n",
    "icu = load(\"icustays\",\n",
    "           columns=[\"stay_id\", \"subject_id\", \"hadm_id\", \"intime\"],\n",
    "           parse_dates=[\"intime\"])\n",
    "icu.set_index(\"stay_id\", inplace=True)         # for O(1) lookup\n",
    "intime_series = icu[\"intime\"]\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 3.  Scan ventilation episodes and flag qualifying stays\n",
    "# ----------------------------------------------------------------------\n",
    "print(\"Scanning ventilation table …\")\n",
    "vent_flag = defaultdict(int)\n",
    "CHUNK = 500_000\n",
    "columns = [\"stay_id\", \"starttime\", \"endtime\"]\n",
    "\n",
    "vent_path = list(ROOT.rglob(\"ventilation.csv\"))\n",
    "\n",
    "vent_path = vent_path[0]\n",
    "\n",
    "for chunk in pd.read_csv(vent_path,\n",
    "                         usecols=columns,\n",
    "                         parse_dates=[\"starttime\", \"endtime\"],\n",
    "                         chunksize=CHUNK,\n",
    "                         low_memory=False):\n",
    "\n",
    "    # keep rows belonging to stays we know\n",
    "    chunk = chunk[chunk[\"stay_id\"].isin(intime_series.index)]\n",
    "    if chunk.empty:\n",
    "        continue\n",
    "\n",
    "    # attach ICU admit time\n",
    "    chunk = chunk.join(intime_series, on=\"stay_id\", how=\"left\")\n",
    "\n",
    "    # First-day overlap test:\n",
    "    #   (start ≤ intime+24 h) AND (end ≥ intime)\n",
    "    t0   = chunk[\"intime\"]\n",
    "    t24  = t0 + pd.Timedelta(hours=24)\n",
    "    mask = (chunk[\"starttime\"] <= t24) & (chunk[\"endtime\"] >= t0)\n",
    "    for stay in chunk.loc[mask, \"stay_id\"]:\n",
    "        vent_flag[stay] = 1            # once flagged, always 1\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 4.  Assemble result\n",
    "# ----------------------------------------------------------------------\n",
    "print(\"Building ventdurations.csv …\")\n",
    "out = (icu.reset_index()[[\"subject_id\", \"hadm_id\", \"stay_id\"]]\n",
    "          .assign(vent=lambda df: df[\"stay_id\"].map(vent_flag).fillna(0).astype(int))\n",
    "          .sort_values([\"subject_id\", \"hadm_id\", \"stay_id\"]))\n",
    "\n",
    "out_path = ROOT / \"icu\" / \"ventdurations.csv\"\n",
    "out.to_csv(out_path, index=False)\n",
    "print(f\"Wrote {len(out):,} rows ➜ {out_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f40f7d",
   "metadata": {},
   "source": [
    "complete_blood_count.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3629d5d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done – 437,065 rows → complete_blood_count.csv\n"
     ]
    }
   ],
   "source": [
    "# build_complete_blood_count.py  –  with stay_id\n",
    "# ---------------------------------------------------------------\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "DATA      = Path(\"./data\")          \n",
    "LAB_CSV   = DATA / \"hosp\" / \"labevents.csv\"\n",
    "ICU_CSV   = DATA / \"icu\"  / \"icustays.csv\"\n",
    "OUT_CSV   = \"complete_blood_count.csv\"\n",
    "\n",
    "# itemid → column name\n",
    "ITEM_MAP = {\n",
    "    51221: \"hematocrit\",\n",
    "    51222: \"hemoglobin\",\n",
    "    51248: \"mch\",\n",
    "    51249: \"mchc\",\n",
    "    51250: \"mcv\",\n",
    "    51265: \"platelet\",\n",
    "    51279: \"rbc\",\n",
    "    51277: \"rdw\",\n",
    "    51301: \"wbc\",\n",
    "}\n",
    "\n",
    "USE_LAB = [\"specimen_id\",\"subject_id\",\"hadm_id\",\n",
    "           \"charttime\",\"itemid\",\"valuenum\"]\n",
    "USE_ICU = [\"subject_id\",\"stay_id\",\"intime\",\"outtime\"]\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 1 ▸ load lab rows \n",
    "# ──────────────────────────────────────────────────────────────\n",
    "le = (\n",
    "    pd.read_csv(LAB_CSV, usecols=USE_LAB, parse_dates=[\"charttime\"])\n",
    "      .query(\"itemid in @ITEM_MAP.keys() and valuenum.notna() and valuenum>0\")\n",
    ")\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 2 ▸ attach stay_id (charttime inside ICU window)\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "icu = pd.read_csv(ICU_CSV, usecols=USE_ICU, parse_dates=[\"intime\",\"outtime\"])\n",
    "icu = icu.astype({\"stay_id\":\"int32\"})\n",
    "\n",
    "# merge (cartesian on subject_id) then filter by time window\n",
    "le = (\n",
    "    le.merge(icu, on=\"subject_id\", how=\"left\")\n",
    "      .query(\"charttime >= intime and charttime <= outtime\")\n",
    ")\n",
    "\n",
    "# keep 1-to-1 (if a lab falls into multiple overlapping stays, keep first)\n",
    "le.sort_values([\"specimen_id\",\"stay_id\"], inplace=True)\n",
    "le = le.drop_duplicates(subset=\"specimen_id\", keep=\"first\")\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 3 ▸ pivot MAX() per specimen_id\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "pivot_vals = (\n",
    "    le.pivot_table(index=\"specimen_id\",\n",
    "                   columns=\"itemid\",\n",
    "                   values=\"valuenum\",\n",
    "                   aggfunc=\"max\")\n",
    "      .rename(columns=ITEM_MAP)\n",
    ")\n",
    "\n",
    "ids = (\n",
    "    le.groupby(\"specimen_id\", as_index=True)\n",
    "      .agg(subject_id=(\"subject_id\",\"max\"),\n",
    "           hadm_id   =(\"hadm_id\",\"max\"),\n",
    "           stay_id   =(\"stay_id\",\"max\"),\n",
    "           charttime =(\"charttime\",\"max\"))\n",
    ")\n",
    "\n",
    "cbc = (\n",
    "    ids.join(pivot_vals, how=\"left\")\n",
    "       .reset_index()\n",
    ")\n",
    "\n",
    "# ensure every expected analyte column exists\n",
    "for col in ITEM_MAP.values():\n",
    "    if col not in cbc:\n",
    "        cbc[col] = pd.NA                     # or np.nan\n",
    "\n",
    "# final column order\n",
    "cbc = cbc.loc[:, [\"subject_id\",\"hadm_id\",\"stay_id\",\"charttime\",\"specimen_id\"]\n",
    "                   + list(ITEM_MAP.values())] \\\n",
    "         .astype({\"subject_id\":\"int32\",\n",
    "                  \"hadm_id\":\"Int32\",\n",
    "                  \"stay_id\":\"Int32\",\n",
    "                  \"specimen_id\":\"int64\"}) \\\n",
    "         .sort_values([\"subject_id\",\"charttime\"])\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 4 ▸ save\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "cbc.to_csv(OUT_CSV, index=False)\n",
    "print(f\"Done – {len(cbc):,} rows → {OUT_CSV}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97557060",
   "metadata": {},
   "source": [
    "chemistry.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a962a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done – 470,912 rows → chemistry.csv\n"
     ]
    }
   ],
   "source": [
    "# build_chemistry.py  –  chemistry panel with stay_id\n",
    "# ---------------------------------------------------\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "DATA     = Path(\"./data\")                      \n",
    "LAB_CSV  = DATA / \"hosp\" / \"labevents.csv\"\n",
    "ICU_CSV  = DATA / \"icu\"  / \"icustays.csv\"\n",
    "OUT_CSV  = \"chemistry.csv\"\n",
    "\n",
    "# itemid → (column, upper-limit, allow_zero)\n",
    "CHEM = {\n",
    "    50862: (\"albumin\"       ,   10 , False),\n",
    "    # 50930: (\"globulin\"      ,   10 , False),\n",
    "    50976: (\"total_protein\" ,   20 , False),\n",
    "    50868: (\"aniongap\"      ,10000 ,  True),   # zero OK\n",
    "    50882: (\"bicarbonate\"   ,10000 , False),\n",
    "    51006: (\"bun\"           ,  300 , False),\n",
    "    50893: (\"calcium\"       ,10000 , False),\n",
    "    50902: (\"chloride\"      ,10000 , False),\n",
    "    50912: (\"creatinine\"    ,  150 , False),\n",
    "    50931: (\"glucose\"       ,10000 , False),\n",
    "    50983: (\"sodium\"        ,  200 , False),\n",
    "    50971: (\"potassium\"     ,   30 , False),\n",
    "}\n",
    "\n",
    "# ── columns to load ───────────────────────────────────────────\n",
    "LAB_COLS = [\"specimen_id\",\"subject_id\",\"hadm_id\",\n",
    "            \"charttime\",\"itemid\",\"valuenum\"]\n",
    "ICU_COLS = [\"subject_id\",\"stay_id\",\"intime\",\"outtime\"]\n",
    "\n",
    "# ── 1 ▸ load and validate lab rows ────────────────────────────\n",
    "lab = (\n",
    "    pd.read_csv(LAB_CSV, usecols=LAB_COLS, parse_dates=[\"charttime\"])\n",
    "      .query(\"itemid in @CHEM.keys()\")\n",
    ")\n",
    "\n",
    "def ok(row):\n",
    "    _, lim, allow0 = CHEM[row.itemid]\n",
    "    v = row.valuenum\n",
    "    return pd.notna(v) and v >= 0 and (allow0 or v > 0) and v <= lim\n",
    "\n",
    "lab = lab[lab.apply(ok, axis=1)].copy()\n",
    "\n",
    "# ── 2 ▸ attach stay_id via merge + window filter ──────────────\n",
    "icu = (\n",
    "    pd.read_csv(ICU_CSV, usecols=ICU_COLS, parse_dates=[\"intime\",\"outtime\"])\n",
    "      .astype({\"stay_id\":\"int32\"})\n",
    ")\n",
    "\n",
    "lab = (\n",
    "    lab.merge(icu, on=\"subject_id\", how=\"left\")           # cartesian merge\n",
    "       .query(\"charttime >= intime and charttime <= outtime\")\n",
    ")\n",
    "\n",
    "lab.sort_values([\"specimen_id\",\"stay_id\"], inplace=True)\n",
    "lab = lab.drop_duplicates(subset=\"specimen_id\", keep=\"first\")\n",
    "\n",
    "# ── 3 ▸ pivot MAX() per specimen_id ───────────────────────────\n",
    "lab[\"col\"] = lab[\"itemid\"].map({k:v[0] for k,v in CHEM.items()})\n",
    "\n",
    "vals = (\n",
    "    lab.pivot_table(index=\"specimen_id\",\n",
    "                    columns=\"col\",\n",
    "                    values=\"valuenum\",\n",
    "                    aggfunc=\"max\")\n",
    ")\n",
    "\n",
    "ids = (\n",
    "    lab.groupby(\"specimen_id\", as_index=True)\n",
    "        .agg(subject_id=(\"subject_id\",\"max\"),\n",
    "             hadm_id   =(\"hadm_id\",\"max\"),\n",
    "             stay_id   =(\"stay_id\",\"max\"),\n",
    "             charttime =(\"charttime\",\"max\"))\n",
    ")\n",
    "\n",
    "chem = ids.join(vals, how=\"left\").reset_index()\n",
    "\n",
    "\n",
    "cols = [\"subject_id\",\"hadm_id\",\"stay_id\",\"charttime\",\"specimen_id\"] \\\n",
    "       + [v[0] for v in CHEM.values()]\n",
    "\n",
    "chem = (\n",
    "    chem[cols]\n",
    "        .astype({\"subject_id\":\"int32\",\n",
    "                 \"hadm_id\"  :\"Int32\",\n",
    "                 \"stay_id\"  :\"Int32\",\n",
    "                 \"specimen_id\":\"int64\"})\n",
    "        .sort_values([\"subject_id\",\"charttime\"])\n",
    ")\n",
    "\n",
    "# ── 4 ▸ save ──────────────────────────────────────────────────\n",
    "chem.to_csv(OUT_CSV, index=False)\n",
    "print(f\"Done – {len(chem):,} rows → {OUT_CSV}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db641c23",
   "metadata": {},
   "source": [
    "blood_differential.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1cf25c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cm/blwm27yd02l6xpvwrm9_l41m0000gn/T/ipykernel_70599/1088665163.py:107: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[]' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  diff.loc[mask, abs_] = diff.loc[mask, pct] * diff.loc[mask, \"wbc\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done – 376,749 rows → blood_differential.csv\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# build_blood_differential.py  –  with stay_id (robust merge version)\n",
    "# ------------------------------------------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT     = Path(\"./data\")                     \n",
    "LAB_CSV  = ROOT / \"hosp\" / \"labevents.csv\"\n",
    "ICU_CSV  = ROOT / \"icu\"  / \"icustays.csv\"\n",
    "OUT_CSV  = \"blood_differential.csv\"\n",
    "\n",
    "# ── ITEMID → (column, factor) ─────────────────────────────────\n",
    "ITEMS = {\n",
    "    51300: (\"wbc\", 1.0), 51301: (\"wbc\", 1.0), 51755: (\"wbc\", 1.0),\n",
    "    # 52069: (\"basophils_abs\", 1.0),\n",
    "    52073: (\"eosinophils_abs\", 1.0), 51199: (\"eosinophils_abs\", 1/1000),\n",
    "    51133: (\"lymphocytes_abs\", 1.0), 52769: (\"lymphocytes_abs\", 1/1000),\n",
    "    52074: (\"monocytes_abs\", 1.0),   51253: (\"monocytes_abs\", 1/1000),\n",
    "    52075: (\"neutrophils_abs\", 1.0),\n",
    "    51218: (\"granulocytes_abs\", 1/1000),\n",
    "    51146: (\"basophils\", 1.0),\n",
    "    51200: (\"eosinophils\", 1.0),\n",
    "    51244: (\"lymphocytes\", 1.0), 51245: (\"lymphocytes\", 1.0),\n",
    "    51254: (\"monocytes\", 1.0),\n",
    "    51256: (\"neutrophils\", 1.0),\n",
    "    51143: (\"atypical_lymphocytes\", 1.0),\n",
    "    51144: (\"bands\", 1.0),\n",
    "    52135: (\"immature_granulocytes\", 1.0),\n",
    "    51251: (\"metamyelocytes\", 1.0),\n",
    "    51257: (\"nrbc\", 1.0),\n",
    "}\n",
    "\n",
    "LAB_COLS = [\"specimen_id\",\"subject_id\",\"hadm_id\",\n",
    "            \"charttime\",\"itemid\",\"valuenum\"]\n",
    "ICU_COLS = [\"subject_id\",\"stay_id\",\"intime\",\"outtime\"]\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 1 ▸ load & basic filter\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "lab = (\n",
    "    pd.read_csv(LAB_CSV, usecols=LAB_COLS, parse_dates=[\"charttime\"])\n",
    "      .query(\"itemid in @ITEMS.keys() and valuenum.notna() and valuenum>=0\")\n",
    "      .copy()\n",
    ")\n",
    "\n",
    "# convert units & set variable name\n",
    "lab[\"var\"] = lab[\"itemid\"].map({k:v[0] for k,v in ITEMS.items()})\n",
    "lab[\"val\"] = lab.apply(lambda r: r.valuenum * ITEMS[r.itemid][1], axis=1)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 2 ▸ attach stay_id (merge + time-window filter)\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "icu = (\n",
    "    pd.read_csv(ICU_CSV, usecols=ICU_COLS, parse_dates=[\"intime\",\"outtime\"])\n",
    "      .astype({\"stay_id\":\"int32\"})\n",
    ")\n",
    "\n",
    "lab = (\n",
    "    lab.merge(icu, on=\"subject_id\", how=\"left\")                # cartesian\n",
    "       .query(\"charttime >= intime and charttime <= outtime\")  # within stay\n",
    ")\n",
    "\n",
    "# if a specimen overlaps multiple stays keep first\n",
    "lab.sort_values([\"specimen_id\",\"stay_id\"], inplace=True)\n",
    "lab = lab.drop_duplicates(\"specimen_id\", keep=\"first\")\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 3 ▸ pivot MAX() per specimen_id\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "vals = (\n",
    "    lab.pivot_table(index=\"specimen_id\",\n",
    "                    columns=\"var\",\n",
    "                    values=\"val\",\n",
    "                    aggfunc=\"max\")\n",
    ")\n",
    "\n",
    "ids = (\n",
    "    lab.groupby(\"specimen_id\", as_index=True)\n",
    "        .agg(subject_id=(\"subject_id\",\"max\"),\n",
    "             hadm_id   =(\"hadm_id\",\"max\"),\n",
    "             stay_id   =(\"stay_id\",\"max\"),\n",
    "             charttime =(\"charttime\",\"max\"))\n",
    ")\n",
    "\n",
    "diff = ids.join(vals, how=\"left\").reset_index()\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 4 ▸ impute *_abs when possible\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "pct_cols = [\"basophils\", \"eosinophils\", \"lymphocytes\",\n",
    "            \"monocytes\", \"neutrophils\"]\n",
    "abs_cols = [c + \"_abs\" for c in pct_cols]\n",
    "\n",
    "# make sure every absolute/percent column exists\n",
    "for col in abs_cols + pct_cols:\n",
    "    if col not in diff.columns:\n",
    "        diff[col] = pd.NA          # create the column filled with <NA>\n",
    "\n",
    "diff[\"impute_abs\"] = (\n",
    "    (diff[\"wbc\"].fillna(0) > 0) &\n",
    "    diff[pct_cols].fillna(0).sum(axis=1).gt(0)\n",
    ")\n",
    "\n",
    "for pct, abs_ in zip(pct_cols, abs_cols):\n",
    "    mask = diff[abs_].isna() & diff[pct].notna() & diff[\"impute_abs\"]\n",
    "    diff.loc[mask, abs_] = diff.loc[mask, pct] * diff.loc[mask, \"wbc\"]\n",
    "\n",
    "diff[abs_cols] = diff[abs_cols].round(4)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 5 ▸ order & save\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "final_cols = ([\"subject_id\",\"hadm_id\",\"stay_id\",\"charttime\",\"specimen_id\",\"wbc\"]\n",
    "              + abs_cols\n",
    "              + pct_cols\n",
    "              + [\"atypical_lymphocytes\",\"bands\",\"immature_granulocytes\",\n",
    "                 \"metamyelocytes\",\"nrbc\"])\n",
    "\n",
    "available_cols = [col for col in final_cols if col in diff.columns]\n",
    "diff = diff[available_cols].astype({\n",
    "    \"subject_id\": \"int32\",\n",
    "    \"hadm_id\": \"Int32\",\n",
    "    \"stay_id\": \"int32\"\n",
    "})\n",
    "\n",
    "diff.to_csv(OUT_CSV, index=False)\n",
    "print(f\"Done – {len(diff):,} rows → {OUT_CSV}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f474f9ce",
   "metadata": {},
   "source": [
    "coagulation.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be58b6c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done – 1,545,930 rows → coagulation.csv\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# build_coagulation.py  –  with stay_id\n",
    "# -------------------------------------------------------------\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT     = Path(\"./data\")                      \n",
    "LAB_CSV  = ROOT / \"hosp\" / \"labevents.csv\"\n",
    "ICU_CSV  = ROOT / \"icu\"  / \"icustays.csv\"\n",
    "OUT_CSV  = \"coagulation.csv\"\n",
    "\n",
    "# itemid → column name\n",
    "ITEMS = {\n",
    "    51196: \"d_dimer\",\n",
    "    51214: \"fibrinogen\",\n",
    "    51297: \"thrombin\",\n",
    "    51237: \"inr\",\n",
    "    51274: \"pt\",\n",
    "    51275: \"ptt\",\n",
    "}\n",
    "\n",
    "LAB_COLS = [\"specimen_id\",\"subject_id\",\"hadm_id\",\n",
    "            \"charttime\",\"itemid\",\"valuenum\"]\n",
    "\n",
    "# ── 1. load & filter labs ────────────────────────────────────\n",
    "labs = (\n",
    "    pd.read_csv(LAB_CSV, usecols=LAB_COLS, parse_dates=[\"charttime\"])\n",
    "      .query(\"itemid in @ITEMS.keys() and valuenum.notna()\")\n",
    "      .copy()\n",
    ")\n",
    "\n",
    "# ── 2. load ICU stays & build IntervalIndex for fast lookup ──\n",
    "icu = (\n",
    "    pd.read_csv(ICU_CSV,\n",
    "                usecols=[\"subject_id\",\"stay_id\",\"intime\",\"outtime\"],\n",
    "                parse_dates=[\"intime\",\"outtime\"])\n",
    "      .astype({\"stay_id\":\"int32\"})\n",
    ")\n",
    "\n",
    "ivl = pd.IntervalIndex.from_arrays(icu[\"intime\"], icu[\"outtime\"], closed=\"both\")\n",
    "icu_int = icu.set_index(ivl)\n",
    "\n",
    "\n",
    "def match_stay(row):\n",
    "    hits = icu_int[icu_int.index.contains(row.charttime)]\n",
    "    hits = hits[hits[\"subject_id\"] == row.subject_id]\n",
    "    return hits[\"stay_id\"].iloc[0] if not hits.empty else pd.NA\n",
    "\n",
    "labs[\"stay_id\"] = labs.apply(match_stay, axis=1)\n",
    "\n",
    "# ── 3. pivot MAX() per specimen_id ────────────────────────────\n",
    "labs[\"var\"] = labs[\"itemid\"].map(ITEMS)\n",
    "\n",
    "vals = (\n",
    "    labs.pivot_table(index=\"specimen_id\",\n",
    "                     columns=\"var\",\n",
    "                     values=\"valuenum\",\n",
    "                     aggfunc=\"max\")\n",
    ")\n",
    "\n",
    "ids = (\n",
    "    labs.groupby(\"specimen_id\", as_index=True)\n",
    "        .agg(subject_id=(\"subject_id\",\"max\"),\n",
    "             hadm_id   =(\"hadm_id\",\"max\"),\n",
    "             stay_id   =(\"stay_id\",\"max\"),        # may stay <NA>\n",
    "             charttime =(\"charttime\",\"max\"))\n",
    ")\n",
    "\n",
    "coag = (\n",
    "    ids.join(vals, how=\"left\")\n",
    "       .reset_index()\n",
    "       .loc[:, [\"subject_id\",\"hadm_id\",\"stay_id\",\"charttime\",\"specimen_id\"]\n",
    "               + list(ITEMS.values())]\n",
    "       .astype({\"subject_id\":\"int32\",\n",
    "                \"hadm_id\"  :\"Int32\",\n",
    "                \"stay_id\"  :\"Int32\",\n",
    "                \"specimen_id\":\"int64\"})\n",
    "       .sort_values([\"subject_id\",\"charttime\"])\n",
    ")\n",
    "\n",
    "# ── 4. save ──────────────────────────────────────────────────\n",
    "coag.to_csv(OUT_CSV, index=False)\n",
    "print(f\"Done – {len(coag):,} rows → {OUT_CSV}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb94d23",
   "metadata": {},
   "source": [
    "enzyme.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "352cf95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done – 1,637,082 rows → enzyme.csv\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# build_enzyme.py  –  enzymes & bilirubin panel with stay_id\n",
    "# ----------------------------------------------------------------\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT     = Path(\"./data\")                       # change if needed\n",
    "LAB_CSV  = ROOT / \"hosp\" / \"labevents.csv\"\n",
    "ICU_CSV  = ROOT / \"icu\"  / \"icustays.csv\"\n",
    "OUT_CSV  = \"enzyme.csv\"\n",
    "\n",
    "# itemid → output column\n",
    "ITEMS = {\n",
    "    50861: \"alt\",    50863: \"alp\",    50878: \"ast\",\n",
    "    50867: \"amylase\",\n",
    "    50885: \"bilirubin_total\", 50883: \"bilirubin_direct\",\n",
    "    50884: \"bilirubin_indirect\",\n",
    "    50910: \"ck_cpk\", 50911: \"ck_mb\",\n",
    "    50927: \"ggt\",    50954: \"ld_ldh\",\n",
    "}\n",
    "\n",
    "LAB_COLS = [\"specimen_id\",\"subject_id\",\"hadm_id\",\n",
    "            \"charttime\",\"itemid\",\"valuenum\"]\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 1 ▸ load & filter labs\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "lab = (\n",
    "    pd.read_csv(LAB_CSV, usecols=LAB_COLS, parse_dates=[\"charttime\"])\n",
    "      .query(\"itemid in @ITEMS.keys() and valuenum.notna() and valuenum>0\")\n",
    "      .copy()\n",
    ")\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 2 ▸ attach stay_id via ICU interval match\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "icu = (\n",
    "    pd.read_csv(ICU_CSV,\n",
    "                usecols=[\"subject_id\",\"stay_id\",\"intime\",\"outtime\"],\n",
    "                parse_dates=[\"intime\",\"outtime\"])\n",
    "      .astype({\"stay_id\":\"int32\"})\n",
    ")\n",
    "\n",
    "# build IntervalIndex for fast lookup\n",
    "ivl = pd.IntervalIndex.from_arrays(icu[\"intime\"], icu[\"outtime\"], closed=\"both\")\n",
    "icu_int = icu.set_index(ivl)\n",
    "\n",
    "\n",
    "\n",
    "def find_stay(row):\n",
    "    hits = icu_int[icu_int.index.contains(row.charttime)]\n",
    "    hits = hits[hits[\"subject_id\"] == row.subject_id]\n",
    "    return hits[\"stay_id\"].iloc[0] if not hits.empty else pd.NA\n",
    "\n",
    "lab[\"stay_id\"] = lab.apply(find_stay, axis=1)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 3 ▸ pivot MAX() per specimen_id\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "lab[\"col\"] = lab[\"itemid\"].map(ITEMS)\n",
    "\n",
    "vals = (\n",
    "    lab.pivot_table(index=\"specimen_id\",\n",
    "                    columns=\"col\",\n",
    "                    values=\"valuenum\",\n",
    "                    aggfunc=\"max\")\n",
    ")\n",
    "\n",
    "ids = (\n",
    "    lab.groupby(\"specimen_id\", as_index=True)\n",
    "        .agg(subject_id=(\"subject_id\",\"max\"),\n",
    "             hadm_id   =(\"hadm_id\",\"max\"),\n",
    "             stay_id   =(\"stay_id\",\"max\"),   # may be <NA>\n",
    "             charttime =(\"charttime\",\"max\"))\n",
    ")\n",
    "\n",
    "enzyme = ids.join(vals, how=\"left\").reset_index()\n",
    "\n",
    "# guarantee every analyte column exists\n",
    "for col in ITEMS.values():\n",
    "    if col not in enzyme.columns:\n",
    "        enzyme[col] = pd.NA\n",
    "\n",
    "cols_order = [\"subject_id\",\"hadm_id\",\"stay_id\",\"charttime\",\"specimen_id\"] \\\n",
    "             + list(ITEMS.values())\n",
    "\n",
    "enzyme = (\n",
    "    enzyme[cols_order]\n",
    "        .astype({\"subject_id\":\"int32\",\n",
    "                 \"hadm_id\"  :\"Int32\",\n",
    "                 \"stay_id\"  :\"Int32\",\n",
    "                 \"specimen_id\":\"int64\"})\n",
    "        .sort_values([\"subject_id\",\"charttime\"])\n",
    ")\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 4 ▸ save\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "enzyme.to_csv(OUT_CSV, index=False)\n",
    "print(f\"Done – {len(enzyme):,} rows → {OUT_CSV}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5b4543",
   "metadata": {},
   "source": [
    "first_day_lab.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a129f9fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done – 73,141 rows → first_day_lab.csv\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# build_first_day_lab.py  –  MIMIC-IV day-1 laboratory panel\n",
    "# -----------------------------------------------------------\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path(\"./data\")            \n",
    "ICU  = ROOT / \"icu\"\n",
    "DRV  = ROOT / \"derived\"         \n",
    "\n",
    "OUT_CSV = \"first_day_lab.csv\"\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 1 ▸ ICU stays  (need stay_id, subject_id, intime)            │\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "icu = (\n",
    "    pd.read_csv(ICU / \"icustays.csv\",\n",
    "                usecols=[\"subject_id\",\"stay_id\",\"intime\"],\n",
    "                parse_dates=[\"intime\"])\n",
    "      .astype({\"stay_id\":\"int32\"})\n",
    ")\n",
    "\n",
    "icu_lookup = icu[[\"stay_id\",\"intime\"]]        # for time-window filter\n",
    "\n",
    "def within_day(df: pd.DataFrame, time_col: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Keep rows whose *time_col* lies in [intime −6 h … intime +24 h].\n",
    "    Assumes df has a `stay_id` column; joins icu_lookup internally.\n",
    "    \"\"\"\n",
    "    d = df.merge(icu_lookup, on=\"stay_id\", how=\"left\", validate=\"many_to_one\")\n",
    "    lo = d[\"intime\"] - pd.Timedelta(hours=6)\n",
    "    hi = d[\"intime\"] + pd.Timedelta(days=1)\n",
    "    keep = (d[time_col] >= lo) & (d[time_col] <= hi)\n",
    "    return d.loc[keep].drop(columns=\"intime\")\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 2 ▸ helper to build min/max blocks                            │\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "def build_block(csv: Path, cols_map: dict[str, str], time_col=\"charttime\"):\n",
    "    \"\"\"\n",
    "    csv       : derived table path\n",
    "    cols_map  : { column_in_csv : short_name_in_output }\n",
    "    returns   : dataframe indexed by stay_id with *_min/_max columns\n",
    "    \"\"\"\n",
    "    usecols = [\"stay_id\", time_col] + list(cols_map.keys())\n",
    "    df = pd.read_csv(csv, usecols=usecols, parse_dates=[time_col])\n",
    "    # Convert stay_id to a nullable integer type to handle non-finite values.\n",
    "    df[\"stay_id\"] = pd.to_numeric(df[\"stay_id\"], errors=\"coerce\").astype(\"Int32\")\n",
    "\n",
    "    df = within_day(df, time_col)\n",
    "\n",
    "    agg = {}\n",
    "    for col, short in cols_map.items():\n",
    "        agg[f\"{short}_min\"] = (col, \"min\")\n",
    "        agg[f\"{short}_max\"] = (col, \"max\")\n",
    "\n",
    "    df = (\n",
    "        df.groupby(\"stay_id\", as_index=False)\n",
    "          .agg(**agg)\n",
    "    )\n",
    "    return df\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 3 ▸ blocks following SQL CTEs                                │\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "cbc_cols  = {\n",
    "    \"hematocrit\" : \"hematocrit\",\n",
    "    \"hemoglobin\" : \"hemoglobin\",\n",
    "    \"platelet\"   : \"platelets\",\n",
    "    \"wbc\"        : \"wbc\",\n",
    "}\n",
    "chem_cols = {\n",
    "    \"albumin\":\"albumin\",\"total_protein\":\"total_protein\",\n",
    "    \"aniongap\":\"aniongap\",\"bicarbonate\":\"bicarbonate\",\"bun\":\"bun\",\n",
    "    \"calcium\":\"calcium\",\"chloride\":\"chloride\",\"creatinine\":\"creatinine\",\n",
    "    \"glucose\":\"glucose\",\"sodium\":\"sodium\",\"potassium\":\"potassium\"\n",
    "}\n",
    "diff_cols = {\n",
    "    \"abs_basophils\":\"abs_basophils\",\"abs_eosinophils\":\"abs_eosinophils\",\n",
    "    \"abs_lymphocytes\":\"abs_lymphocytes\",\"abs_monocytes\":\"abs_monocytes\",\n",
    "    \"abs_neutrophils\":\"abs_neutrophils\",\"atyps\":\"atyps\",\"bands\":\"bands\",\n",
    "    \"imm_granulocytes\":\"imm_granulocytes\",\"metas\":\"metas\",\"nrbc\":\"nrbc\"\n",
    "}\n",
    "coag_cols = {\n",
    "    \"d_dimer\":\"d_dimer\",\"fibrinogen\":\"fibrinogen\",\"thrombin\":\"thrombin\",\n",
    "    \"inr\":\"inr\",\"pt\":\"pt\",\"ptt\":\"ptt\"\n",
    "}\n",
    "enz_cols  = {\n",
    "    \"alt\":\"alt\",\"alp\":\"alp\",\"ast\":\"ast\",\"amylase\":\"amylase\",\n",
    "    \"bilirubin_total\":\"bilirubin_total\",\n",
    "    \"bilirubin_direct\":\"bilirubin_direct\",\n",
    "    \"bilirubin_indirect\":\"bilirubin_indirect\",\n",
    "    \"ck_cpk\":\"ck_cpk\",\"ck_mb\":\"ck_mb\",\"ggt\":\"ggt\",\"ld_ldh\":\"ld_ldh\"\n",
    "}\n",
    "\n",
    "cbc  = build_block(DRV / \"complete_blood_count.csv\",    cbc_cols)\n",
    "chem = build_block(DRV / \"chemistry.csv\",               chem_cols)\n",
    "diff = build_block(\n",
    "    DRV / \"blood_differential.csv\",\n",
    "    {\n",
    "        \"wbc\": \"wbc\",\n",
    "        \"basophils\": \"basophils\",\n",
    "        \"eosinophils\": \"eosinophils\",\n",
    "        \"lymphocytes\": \"lymphocytes\",\n",
    "        \"monocytes\": \"monocytes\",\n",
    "        \"neutrophils\": \"neutrophils\",\n",
    "        \"atypical_lymphocytes\": \"atypical_lymphocytes\",\n",
    "        \"bands\": \"bands\",\n",
    "        \"nrbc\": \"nrbc\"\n",
    "    }\n",
    ")\n",
    "coag = build_block(DRV / \"coagulation.csv\", coag_cols)\n",
    "coag[\"stay_id\"] = pd.to_numeric(coag[\"stay_id\"], errors=\"coerce\").astype(\"Int32\")\n",
    "enz  = build_block(DRV / \"enzyme.csv\", enz_cols)\n",
    "enz[\"stay_id\"] = pd.to_numeric(enz[\"stay_id\"], errors=\"coerce\").astype(\"Int32\")\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 4 ▸ merge everything & attach subject_id                     │\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "lab = (\n",
    "    icu[[\"subject_id\",\"stay_id\"]]\n",
    "      .merge(cbc,  on=\"stay_id\", how=\"left\")\n",
    "      .merge(chem, on=\"stay_id\", how=\"left\")\n",
    "      .merge(diff, on=\"stay_id\", how=\"left\")\n",
    "      .merge(coag, on=\"stay_id\", how=\"left\")\n",
    "      .merge(enz,  on=\"stay_id\", how=\"left\")\n",
    ")\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 5 ▸ save                                                     │\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "lab.to_csv(OUT_CSV, index=False)\n",
    "print(f\"Done – {len(lab):,} rows → {OUT_CSV}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28adcc27",
   "metadata": {},
   "source": [
    "sofa.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1359fd5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Usecols do not match columns, columns expected but not found: ['urineoutput']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 128\u001b[0m\n\u001b[1;32m    122\u001b[0m vital \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(DRV \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfirst_day_vitalsign.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    123\u001b[0m                     usecols\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstay_id\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mset_index(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstay_id\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    124\u001b[0m lab   \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(DRV \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfirst_day_lab.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    125\u001b[0m                     usecols\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstay_id\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreatinine_max\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    126\u001b[0m                              \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbilirubin_total_max\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplatelets_min\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    127\u001b[0m                    )\u001b[38;5;241m.\u001b[39mset_index(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstay_id\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 128\u001b[0m uo    \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDRV\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfirst_day_urine_output.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m                    \u001b[49m\u001b[43musecols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstay_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43murineoutput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m                   \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mset_index(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstay_id\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    131\u001b[0m gcs   \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(DRV \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfirst_day_gcs.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    132\u001b[0m                     usecols\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstay_id\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgcs_min\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    133\u001b[0m                    )\u001b[38;5;241m.\u001b[39mset_index(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstay_id\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    135\u001b[0m \u001b[38;5;66;03m# 5 ─ gather all components --------------------------------------\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1723\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1720\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m   1722\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1723\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1724\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1725\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/io/parsers/c_parser_wrapper.py:140\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morig_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39musecols_dtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstring\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mset\u001b[39m(usecols)\u001b[38;5;241m.\u001b[39missubset(\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morig_names\n\u001b[1;32m    139\u001b[0m ):\n\u001b[0;32m--> 140\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_usecols_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43musecols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43morig_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnames) \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlen\u001b[39m(usecols):  \u001b[38;5;66;03m# type: ignore[has-type]\u001b[39;00m\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/io/parsers/base_parser.py:969\u001b[0m, in \u001b[0;36mParserBase._validate_usecols_names\u001b[0;34m(self, usecols, names)\u001b[0m\n\u001b[1;32m    967\u001b[0m missing \u001b[38;5;241m=\u001b[39m [c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m usecols \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m names]\n\u001b[1;32m    968\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 969\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    970\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsecols do not match columns, columns expected but not found: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    971\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    972\u001b[0m     )\n\u001b[1;32m    974\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m usecols\n",
      "\u001b[0;31mValueError\u001b[0m: Usecols do not match columns, columns expected but not found: ['urineoutput']"
     ]
    }
   ],
   "source": [
    "# build_first_day_sofa.py  – \n",
    "# -----------------------------------------------------------------\n",
    "import pandas as pd, numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "BASE = Path(\"./data\")                   \n",
    "ICU  = BASE / \"icu\"                    \n",
    "DRV  = BASE / \"derived\"                  \n",
    "# helper ----------------------------------------------------------\n",
    "def clip_by_intime(df, time_col, lookup, start_h=-6, end_h=24):\n",
    "    \"\"\"\n",
    "    Keep rows whose time_col is within [intime+start_h … intime+end_h].\n",
    "    lookup must have columns 'stay_id' and 'intime'.\n",
    "    \"\"\"\n",
    "    df = df.merge(\n",
    "        lookup,\n",
    "        on=\"stay_id\",\n",
    "        how=\"left\",\n",
    "        suffixes=(\"\", \"_icu\"),          # avoid _x/_y randomness\n",
    "        validate=\"many_to_one\",\n",
    "    )\n",
    "\n",
    "    # choose which column holds the admission time\n",
    "    if \"intime\" in df.columns:\n",
    "        icu_in = df[\"intime\"].fillna(df[\"intime_icu\"])\n",
    "    else:                               # df had none → use the one from lookup\n",
    "        icu_in = df.pop(\"intime_icu\")\n",
    "\n",
    "    lo = icu_in + pd.Timedelta(hours=start_h)\n",
    "    hi = icu_in + pd.Timedelta(hours=end_h)\n",
    "\n",
    "    keep = (df[time_col] >= lo) & (df[time_col] <= hi)\n",
    "    df = df.loc[keep]\n",
    "\n",
    "    # tidy up extra column if it exists\n",
    "    df = df.drop(columns=[c for c in [\"intime\", \"intime_icu\"] if c in df])\n",
    "\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "# 1 ─ ICU rows ----------------------------------------------------\n",
    "icu = (\n",
    "    pd.read_csv(ICU / \"icustays.csv\", parse_dates=[\"intime\"])\n",
    "      .loc[:, [\"subject_id\",\"hadm_id\",\"stay_id\",\"intime\"]]\n",
    ")\n",
    "icu_lookup = icu.copy()                     # stay_id + intime\n",
    "icu_lookup = icu[[\"stay_id\",\"intime\"]]        # for clip_by_intime\n",
    "icu.set_index(\"stay_id\", inplace=True)        # rest of the script unchanged\n",
    "\n",
    "# 2 ─ vasopressor max rates (derived tables) ----------------------\n",
    "VASO_ITEMID = {\n",
    "    221906: \"norepinephrine\",\n",
    "    30047 : \"norepinephrine\",\n",
    "    221289: \"epinephrine\",\n",
    "    221653: \"dopamine\",\n",
    "    221286: \"dobutamine\",\n",
    "}\n",
    "\n",
    "def within_window(df, key, intime, start_h=-6, end_h=24):\n",
    "    \"\"\"Keep rows whose key-time lies in [intime+start_h, intime+end_h].\"\"\"\n",
    "    lo = intime + pd.Timedelta(hours=start_h)\n",
    "    hi = intime + pd.Timedelta(hours=end_h)\n",
    "    return df[(df[key] >= lo) & (df[key] <= hi)]\n",
    "\n",
    "inp = pd.read_csv(\n",
    "        ICU / \"inputevents.csv\",\n",
    "        usecols=[\"stay_id\", \"starttime\", \"itemid\", \"rate\"],\n",
    "        parse_dates=[\"starttime\"],\n",
    "    )\n",
    "inp = inp[inp[\"itemid\"].isin(VASO_ITEMID)]\n",
    "inp[\"drug\"] = inp[\"itemid\"].map(VASO_ITEMID)\n",
    "inp[\"stay_id\"] = inp[\"stay_id\"].astype(\"int32\")\n",
    "\n",
    "# keep only rows within -6 h … +24 h of ICU admission\n",
    "inp = (\n",
    "    inp.join(icu[\"intime\"], on=\"stay_id\", how=\"left\")\n",
    "       .pipe(lambda d: within_window(d, \"starttime\", d[\"intime\"]))\n",
    ")\n",
    "\n",
    "vaso_max = (\n",
    "    inp.groupby([\"stay_id\", \"drug\"], as_index=False)[\"rate\"]\n",
    "       .max()\n",
    "       .pivot(index=\"stay_id\", columns=\"drug\", values=\"rate\")\n",
    "       .rename(columns=lambda c: f\"rate_{c}\")\n",
    ")\n",
    "\n",
    "# 3 ─ PaO2/FiO2 minima, with & w/o ventilation -------------------\n",
    "bg  = pd.read_csv(DRV / \"first_day_bg_art.csv\",\n",
    "                  usecols=[\"subject_id\",\"charttime\",\"PaO2FiO2\",\"stay_id\"],\n",
    "                  parse_dates=[\"charttime\"])\n",
    "bg  = bg.merge(icu[\"intime\"], left_on=\"stay_id\", right_index=True, how=\"left\")\n",
    "\n",
    "\n",
    "bg  = clip_by_intime(bg, \"charttime\", icu_lookup)\n",
    "inp = clip_by_intime(inp, \"starttime\", icu_lookup)\n",
    "\n",
    "vent = pd.read_csv(DRV / \"ventilation.csv\",\n",
    "                   usecols=[\"stay_id\",\"starttime\",\"endtime\",\"ventilation_status\"],\n",
    "                   parse_dates=[\"starttime\",\"endtime\"])\n",
    "vent = vent[vent[\"ventilation_status\"]==\"InvasiveVent\"]\n",
    "\n",
    "# mark BG rows that fall inside an invasive-vent interval\n",
    "vent_grp = vent.groupby(\"stay_id\")\n",
    "def is_vent_row(row):\n",
    "    v = vent_grp.get_group(row.stay_id) if row.stay_id in vent_grp.groups else None\n",
    "    return 0 if v is None else int(((v.starttime<=row.charttime)&(row.charttime<=v.endtime)).any())\n",
    "\n",
    "bg[\"isvent\"] = bg.apply(is_vent_row, axis=1)\n",
    "\n",
    "pafi = (\n",
    "    bg.groupby(\"stay_id\")\n",
    "      .agg(\n",
    "        pao2fio2_novent_min = (\"PaO2FiO2\",\n",
    "                               lambda s: s[bg.loc[s.index,\"isvent\"]==0].min()),\n",
    "        pao2fio2_vent_min   = (\"PaO2FiO2\",\n",
    "                               lambda s: s[bg.loc[s.index,\"isvent\"]==1].min()),\n",
    "      )\n",
    ")\n",
    "\n",
    "# 4 ─ first-day vitals / labs / urine / GCS -----------------------\n",
    "vital = pd.read_csv(DRV / \"first_day_vitalsign.csv\",\n",
    "                    usecols=[\"stay_id\"]).set_index(\"stay_id\")\n",
    "lab   = pd.read_csv(DRV / \"first_day_lab.csv\",\n",
    "                    usecols=[\"stay_id\",\"creatinine_max\",\n",
    "                             \"bilirubin_total_max\",\"platelets_min\"]\n",
    "                   ).set_index(\"stay_id\")\n",
    "uo    = pd.read_csv(DRV / \"first_day_urine_output.csv\",\n",
    "                    usecols=[\"stay_id\",\"urineoutput\"]\n",
    "                   ).set_index(\"stay_id\")\n",
    "gcs   = pd.read_csv(DRV / \"first_day_gcs.csv\",\n",
    "                    usecols=[\"stay_id\",\"gcs_min\"]\n",
    "                   ).set_index(\"stay_id\")\n",
    "\n",
    "# 5 ─ gather all components --------------------------------------\n",
    "comp = (\n",
    "    icu.join([vaso_max, pafi, vital, lab, uo, gcs], how=\"left\")\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "for drug in [\"norepinephrine\",\"epinephrine\",\"dopamine\",\"dobutamine\"]:\n",
    "    comp[f\"rate_{drug}\"] = comp[f\"rate_{drug}\"].astype(float)\n",
    "\n",
    "# 6 ─ component score functions ----------------------------------\n",
    "def resp(r):\n",
    "    if pd.isna(r.pao2fio2_vent_min) and pd.isna(r.pao2fio2_novent_min): return np.nan\n",
    "    if r.pao2fio2_vent_min   < 100: return 4\n",
    "    if r.pao2fio2_vent_min   < 200: return 3\n",
    "    if r.pao2fio2_novent_min < 300: return 2\n",
    "    if r.pao2fio2_novent_min < 400: return 1\n",
    "    return 0\n",
    "\n",
    "def coag(p):\n",
    "    if pd.isna(p): return np.nan\n",
    "    if p<20: return 4\n",
    "    if p<50: return 3\n",
    "    if p<100: return 2\n",
    "    if p<150: return 1\n",
    "    return 0\n",
    "\n",
    "def liver(b):\n",
    "    if pd.isna(b): return np.nan\n",
    "    if b>=12: return 4\n",
    "    if b>=6:  return 3\n",
    "    if b>=2:  return 2\n",
    "    if b>=1.2:return 1\n",
    "    return 0\n",
    "\n",
    "def cardio(row):\n",
    "    dopa,dobu,epi,norepi = (row.rate_dopamine,row.rate_dobutamine,\n",
    "                                row.rate_epinephrine,row.rate_norepinephrine\n",
    "                                )\n",
    "    if (dopa>15) or (epi>0.1) or (norepi>0.1): return 4\n",
    "    if (dopa>5) or (0<=epi<=0.1) or (0<=norepi<=0.1): return 3\n",
    "    if (dopa>0) or (dobu>0): return 2\n",
    "    if all(pd.isna(x) for x in [dopa,dobu,epi,norepi]): return np.nan\n",
    "    return 0\n",
    "\n",
    "def cns(g):\n",
    "    if pd.isna(g): return np.nan\n",
    "    if g<6: return 4\n",
    "    if 6<=g<=9: return 3\n",
    "    if 10<=g<=12: return 2\n",
    "    if 13<=g<=14: return 1\n",
    "    return 0\n",
    "\n",
    "def renal(row):\n",
    "    cr,uo = row.creatinine_max,row.urineoutput\n",
    "    if (cr>=5) or (uo<200): return 4\n",
    "    if (3.5<=cr<5) or (uo<500): return 3\n",
    "    if 2<=cr<3.5: return 2\n",
    "    if 1.2<=cr<2: return 1\n",
    "    if pd.isna(cr) and pd.isna(uo): return np.nan\n",
    "    return 0\n",
    "\n",
    "# 7 ─ compute scores ---------------------------------------------\n",
    "comp[\"respiration\"]   = comp.apply(resp,  axis=1)\n",
    "comp[\"coagulation\"]   = comp.platelets_min.map(coag)\n",
    "comp[\"liver\"]         = comp.bilirubin_total_max.map(liver)\n",
    "comp[\"cardiovascular\"]= comp.apply(cardio, axis=1)\n",
    "comp[\"cns\"]           = comp.gcs_min.map(cns)\n",
    "comp[\"renal\"]         = comp.apply(renal,  axis=1)\n",
    "\n",
    "cols = [\"respiration\",\"coagulation\",\"liver\",\"cardiovascular\",\"cns\",\"renal\"]\n",
    "comp[\"sofa\"] = comp[cols].fillna(0).sum(axis=1)\n",
    "\n",
    "# 8 ─ final output -----------------------------------------------\n",
    "out = comp[[\"subject_id\",\"hadm_id\",\"stay_id\",\"sofa\"]+cols]\n",
    "out.to_csv(\"first_day_sofa.csv\", index=False)\n",
    "print(f\"Done – {len(out):,} rows → first_day_sofa.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766bd940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   subject_id   hadm_id   stay_id  heartrate_min  heartrate_max  \\\n",
      "0    12466550  23998182  30000153           83.0          128.0   \n",
      "1    13180007  27543152  30000213           66.0           91.0   \n",
      "2    12207593  22795209  30000646           69.0          102.0   \n",
      "3    12980335  23552849  30001148           64.0           80.0   \n",
      "4    12168737  29283664  30001336           53.0           73.0   \n",
      "\n",
      "   heartrate_mean  sysbp_min  sysbp_max  sysbp_mean  diasbp_min  ...  \\\n",
      "0      106.840000      108.0      171.0  136.088235        55.0  ...   \n",
      "1       81.680000      116.0      168.0  134.920000        47.0  ...   \n",
      "2       86.423729       71.0      151.0   92.703704        34.0  ...   \n",
      "3       75.520000       92.0      125.0  107.900000        48.0  ...   \n",
      "4       63.040000      100.0      120.0  110.625000        47.0  ...   \n",
      "\n",
      "   resprate_mean  tempc_min  tempc_max  tempc_mean  spo2_min  spo2_max  \\\n",
      "0      14.923077  37.222222  38.222222   37.500000      92.0     100.0   \n",
      "1      19.437500  36.333333  37.555556   37.006944      96.0     100.0   \n",
      "2      25.627119  36.555556  39.111111   37.375000      78.0     100.0   \n",
      "3      14.517241  35.333333  38.166667   36.388889      92.0     100.0   \n",
      "4      26.038462  36.722222  36.944444   36.857143      93.0      99.0   \n",
      "\n",
      "   spo2_mean  glucose_min  glucose_max  glucose_mean  \n",
      "0  96.640000        144.0        192.0    167.857143  \n",
      "1  98.960000         97.0        275.0    168.900000  \n",
      "2  95.610169        102.0        144.0    118.000000  \n",
      "3  98.360000         86.0        149.0    119.500000  \n",
      "4  96.461538         94.0        166.0    128.142857  \n",
      "\n",
      "[5 rows x 27 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"./data/icu/first_day_vitalsign.csv\", nrows=5)\n",
    "\n",
    "print(df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
